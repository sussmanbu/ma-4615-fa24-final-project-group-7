---
title: Data
author: "Group 7"
description: We describe the sources of our data and the cleaning process.
toc: true
draft: false
---

![](images/data-import-cheatsheet-thumbs.png)
***Before we begin, all of the cleaning and combining for our datasets is done with the code in [this load and clean file](\scripts\load_and_clean_data.R), so please head there for a more detailed breakdown of how all of this was done.***

**First Data Set:**  
The Effect of Prior Police Contact on Victimization Reporting: Results From the Police-Public Contact and National Crime Victimization Surveys, United States, 2002-2011 (ICPSR 36370) 
[Dataset Link](https://www.icpsr.umich.edu/web/NACJD/studies/36370/)

The dataset is collected by the Bureau of Justice Statistics (BJS), which is the primary statistical agency of the US's Department of Justice. It was first established in 1979 in order to better understand statistics about crimes in the United States. The data collected in these studies provides information on how each individuals' prior interactions with law enforcement, both directly and through the experiences of others, influence their decisions to report crimes. By linking data from the National Crime Victimization Survey (NCVS) with the Police-Public Contact Survey (PPCS), the researchers could explore the relationship between police encounter experiences and crime reporting behavior. The combined dataset used in this study contains data about survey participants in 2002, 2008, and 2011.


The dataset comprises person-level, household-level, and police encounter data, emphasizing key variables to study the relationship between police contact and victimization reporting. 
At the person level, variables include PER_ID (Unique person identifier), age, race, contact (if the respondent had police contact in the past 12 months), and number of face-to-face police contacts within the past year.
At the household level, HH_ID identifies households, while hhpov (poverty status) and HH_size (number of household members) contextualize socioeconomic status. Police encounters are detailed with REASON (reason for contact), arrested (if the respondent was arrested), and cuffed (if handcuffed), which capture the nature of police interactions. 
Victimization variables like d_HH_P23 (household crime victimization), time2vic_inc_P23HH (time between police interview and crime), and notify_inc_P23HH (police notified of household crime) provide insights into subsequent incidents. 
Additional demographic variables such as education, mar_stat (marital status), and work_lw (employment status) add socioeconomic context.

In terms of cleaning the data, we originally started by planning to remove rows with over 90,000 na values, but quickly realized that some of these highlighted rare experiences with police that are information that would be crucial to our analysis. 

In noticing that, we decided to more forwards by removing all columns we felt contained information which were not generally relevant to the analysis we wish to conduct. This was done by reading through the documentation for the dataset and filtering out columns we felt were unhelpful to our analysis. Below are a list of a few columns we removed and why:

PSTRATA: PSEUDO-STRATUM CODE - We didn't understand what this datapoint was representing, or how to use it.

SECUCODE - STANDARD ERROR COMPUTATION UNIT CODE: Again another code, which is vague in description.

num_fu_HHint, num_fu_perint - Number of follow-up HH interviews post-PPCS, Number of follow-up PERSON interviews post-PPCS : The number of follow-ups for each person or household isn't relevant to us.

Columns ending with "_sub": Data wasn't originally collected, and carried back later on. 

time2vic_inc_P23PER: We don't want to investigate months from PPCS interview to victimization

This is all done in our [cleaning script](\scripts\load_and_clean_data.R). In this script we start by removing certain columns, and then outlining the remaining columns as numerical or categorical, in order to check for inconsistent data values and providing default values to missing data. For most cases, we consider missing data to be okay.

**Second Data Set:**
Firearm Suicide Proxy for Household Gun Ownership, 1949-2020
[Dataset Link](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/QVYDUD)

This dataset was collected by Megan Kang and can be found on the Harvard Dataverse collection. The proxy measures household gun ownership trends at a state level. Notably it is not reliant on self reported data, which can be prone to certain biases. This dataset ranges a long period of time from 1949-2020 and represents the longest-ranging database of state-level gun ownership. 

Particularly this dataset includes information across districts, states, and years, and contains comprehensive variables that allow us to study the complex relationship between geography and household gun ownership and misuse. At the district-level we have variables that show the total population of the district and homicide/suicide rates caused by firearms and non-firearms.

Our goal with this dataset was to combine it with our original dataset related to police sentiment. Minor initial cleaning was performed, removing the following columns/variables:

*fss* - a metric irrelevant to our analysis.
*districts* - our original dataset used regions, which we could determine using the state level data, concatanating them into relevant regional variables.
*suicide related metrics* - we felt that homicide information was more relevant to the way police may choose to approach situations.
All year values besides the years which were present in our original dataset were removed, due to their lack of relevancy to our analysis. 

**Combination**
In combining the datasets we chose to combine information such that we would display information based on the region and year combinations. This is because the original data was presented across 4 different regions: Northeast, Midwest, South, and West as well as 3 years: 2002, 2008, and 2011.

Our final combined dataset includes the following variables
```{r}
suppressPackageStartupMessages(library(tidyverse))
variable_table <- tibble(
  Variable_Name = c("region", "year", "total_count", "WHITE_NH_PROP", "B_NH_PROP", "HISPANIC_PROP", "OTHER_MULTI_NH_PROP", "CONTACT_FREQ", "OC_FRISK_PROP", "TC_FRISK_PROP", "VSRCH_PROP", "ARREST_PROP", "CUFFED_PROP", "PROPER_PROP", "IMPROPER_PROP", "AVG_CONT", "population", "fa_homicides", "nfa_homicides", "homicides", "fa_homicide_rate", "nfa_homicide_rate", "homicide_rate"),
  Variable_Description = c("Region the data is from", "Year the data is from", "Total number of responses in the police sentiment dataset", "White non-hispanic proportion of responses for police sentiment", "Black non-hispanic proportion of responses for police sentiment", "Hispanic proportion of responses for police sentiment survey", "Other/multiracial/Non-hispanic proportion of responses for police sentiment survey", "Proportion of responders who had contact with law enforcement", "Proportion of stop and frisks for non-traffic stops", "Proportion of stop and frisks for traffic stops", "Proportion of vehicle searchs", "Proportions of contacts that ended in arrest", "Proportion of contacts that included cuffing", "Proportion of contacts deemed handled proper", "Proportion of contacts deemed unproperly handled", "Average number of contacts in the last year", "Total region population from US census", "Number of Firearm homicides", "Number of non-firearm homicides", "number of homidides", "firearm homicides per 100,000", "non-firearm homicides per 100,000", "homicides per 100,000")
)

print(variable_table, n = Inf)
```


The first step of doing so was to find the information for each region x year combination that we wanted to keep from the first dataset. This corresponds to the variables ranging from 3-16 in the table above.

For these variables there are a few main categories:

*total_count* - a metric taken directly from the original data 
*racial demographics* - which were taken by finding the proportion of certain races against the entire population
*event proportions* - which were taken by finding the average number of occurrences across the total number of police contacts

Next we had to transform the firearm dataset to follow the same region x year format. Since our firearm dataset came broken down by states we would have to use these states to reconstruct the original corresponding regions. To do this designation we used the Census Designated Regions of the United States. This is a potential point of error in our analysis as we could not find certain information about the regional breakdown in the first dataset. Then we simply grouped by region x year and recalculated the simple metrics in the original dataset but across the new breakdown. 

Finally we used full_join to combine our two transformed data sets to create a new combined dataset that shared the same grouping breakdowns. 


