[
  {
    "objectID": "scripts/other_dataset_cleaning.html",
    "href": "scripts/other_dataset_cleaning.html",
    "title": "Remove unnecessary columns and rename them for clarity",
    "section": "",
    "text": "suppressPackageStartupMessages(library(tidyverse))\nsuppressPackageStartupMessages(library(sf))\n\nWarning: package 'sf' was built under R version 4.4.2\n\nsuppressPackageStartupMessages(library(dplyr))\nsuppressPackageStartupMessages(library(viridis))\n\nWarning: package 'viridis' was built under R version 4.4.2\n\nsuppressPackageStartupMessages(library(maps))\n\n\nfiltered &lt;- read_csv(\"dataset/Police_Sentiment_Survey.csv\")\n\nRows: 690 Columns: 82\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (8): level_0, level_1, level_2, level_0_type, level_1_type, level_2_typ...\ndbl (70): org_level, safety, safety_race_african_american, safety_race_asian...\nlgl  (4): level_3, level_4, level_3_type, level_4_type\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nRemove unnecessary columns and rename them for clarity\n\nfiltered &lt;- filtered %&gt;%\n  rename_with(tolower) %&gt;%\n  mutate_all(~ ifelse(. == \"NA\" | . == \"\", NA, .))  # Replace blank/empty strings with NA\n\n\n\nRemove duplicate rows\n\nfiltered &lt;- filtered %&gt;% distinct()\n\n\n\nDrop columns with more than 50% missing data\n\nfiltered &lt;- filtered %&gt;% select_if(~ mean(!is.na(.)) &gt; 0.5)\n\n\nsummary_stats &lt;- filtered %&gt;%\n  select_if(is.numeric) %&gt;%\n  summary()\n\n\n\nDistribution of safety scores\n\nggplot(filtered, aes(x = safety)) +\n  geom_histogram(binwidth = 2, fill = \"blue\", color = \"black\", alpha = 0.7) +\n  labs(title = \"Distribution of Safety Scores\", x = \"Safety\", y = \"Frequency\")\n\n\n\n\n\n\n\n\n\n\nDistribution of ‘trust’ scores\n\nggplot(filtered, aes(x = trust)) +\n  geom_histogram(binwidth = 2, fill = \"green\", color = \"black\", alpha = 0.7) +\n  labs(title = \"Distribution of Trust Scores\", x = \"Trust\", y = \"Frequency\")\n\n\n\n\n\n\n\n\n\n# Average safety and trust scores by race\naverage_scores_by_race &lt;- filtered %&gt;%\n  summarise(\n    African_American_Safety = mean(safety_race_african_american, na.rm = TRUE),\n    Asian_American_Safety = mean(safety_race_asian_american, na.rm = TRUE),\n    Other_Race_Safety = mean(safety_race_other, na.rm = TRUE),\n    Hispanic_Safety = mean(safety_race_hispanic, na.rm = TRUE),\n    White_Safety = mean(safety_race_white, na.rm = TRUE)\n  )\nprint(average_scores_by_race)\n\n# A tibble: 1 × 5\n  African_American_Saf…¹ Asian_American_Safety Other_Race_Safety Hispanic_Safety\n                   &lt;dbl&gt;                 &lt;dbl&gt;             &lt;dbl&gt;           &lt;dbl&gt;\n1                   70.2                  68.8              65.6            69.0\n# ℹ abbreviated name: ¹​African_American_Safety\n# ℹ 1 more variable: White_Safety &lt;dbl&gt;"
  },
  {
    "objectID": "posts/2024-11-4-EDA/more EDA.html",
    "href": "posts/2024-11-4-EDA/more EDA.html",
    "title": "EDA",
    "section": "",
    "text": "This week, we continued exploring the dataset with a focus on relationships among different variables such as reasons for traffic stops, arrests, and the types of police contacts experienced under different demographic groups.\nFor example, below is a plot being generated. We studied the disparities in experiences of improper police contact among different racial groups. We made a bar plot to illustrate the proportion within each racial group who reported experiencing improper police contact and the result reveals notable differences across racial groups.This insight lays the groundwork for deeper analysis to understand what factors contribute to these differences, potentially using demographic and situational variables in further modeling.\n\nsuppressPackageStartupMessages(library(tidyverse))\nsuppressPackageStartupMessages(library(tidymodels))\ndata &lt;- readRDS(\"dataset/police_interaction.rds\") \ndata |&gt;\n  filter(!is.na(PROPER)) |&gt;\n  group_by(RACE) |&gt;\n  summarize(\n    improper_proportion = sum(PROPER == 0) / n()\n  ) |&gt;\n  ggplot(aes(x = RACE, y = improper_proportion, fill = RACE)) +\n  geom_bar(stat = \"identity\")\n\n\n\n\n\n\n\n\nThis is the second plot that we would like to include here: We made a heatmap that visualizes the correlations between various binary variables in the dataset, where the color intensity represents the strength and direction of the correlation (with red indicating positive correlations and blue indicating negative correlations). There are notable positive correlations such as HHPOV and WORK_LW and notable negative correlations such as WORK_LW and any high-income indicator. This heatmap helps identify groups of binary variables that may be related or unrelated, which can be useful in modeling and to understand relationships among the data.\n\nlibrary(tidyverse)\nlibrary(reshape2)\n\n\nAttaching package: 'reshape2'\n\n\nThe following object is masked from 'package:tidyr':\n\n    smiths\n\n# Categorizing variables based on their types\n#removed \"CONTACT\" and \"ERROR\" due to \"Warning in cor(numeric_data, use = \"complete.obs\") : the standard deviation is zero\" error\nbinary_columns &lt;- c(\"MALE\", \"WORK_LW\", \"HHPOV\", \"PUB_HOUSE\",   \"PUB_HOUSE_SUB\", \"REGION\", \"INPERSON\", \"VICAR_CITIZEN\", \"VICAR_PRO_AUTO\", \"VICAR_PRO_PERS\", \"VICAR_OTH_CONT\", \"VICAR_IMPROPER\", \"D_HH_P23\", \"PROPER\")\n\nordinal_columns &lt;- c(\"C4_RACE\", \"MAR_STAT\", \"FREQ_DRV\", \"TENURE\",  \"MSA_STATUS\")\n\ncontinuous_columns &lt;- c(\"AGE\", \"EDUCATION\", \"EDUCATION_SUB\", \"NUM_MOVES\", \"NUM_CONT\", \"HH_SIZE\", \"PPCS_YEAR\", \"N_HH_P1\", \"N_PERS_P1\", \"NUM_CITIZEN_HH\", \"NUM_PRO_AUTO_HH\", \"NUM_PRO_PERS_HH\", \"NUM_OTH_CONT_HH\", \"NUM_IMPROPER_HH\")\n\nfiltered &lt;- data\n# Selecting data by type\nbinary_data &lt;- filtered %&gt;%\n  select(all_of(binary_columns)) %&gt;%\n  select(where(is.numeric))\n\nordinal_data &lt;- filtered %&gt;%\n  select(all_of(ordinal_columns)) %&gt;%\n  select(where(is.numeric))\n\ncontinuous_data &lt;- filtered %&gt;%\n  select(all_of(continuous_columns)) %&gt;%\n  select(where(is.numeric))\n\n# Calculating correlation matrices with appropriate methods\ncor_binary &lt;- cor(binary_data, use = \"pairwise.complete.obs\", method = \"pearson\")\ncor_ordinal &lt;- cor(ordinal_data, use = \"pairwise.complete.obs\", method = \"spearman\")\ncor_continuous &lt;- cor(continuous_data, use = \"pairwise.complete.obs\", method = \"pearson\")\n\n# Creating a heatmap function\nplot_heatmap &lt;- function(cor_matrix, title) {\n  cor_melted &lt;- melt(cor_matrix)\n  ggplot(cor_melted, aes(Var1, Var2, fill = value)) +\n    geom_tile(color = \"white\") +\n    scale_fill_gradient2(low = \"blue\", high = \"red\", mid = \"white\",\n                         midpoint = 0, limit = c(-1, 1), name = \"Correlation\") +\n    labs(title = title, x = \"Variables\", y = \"Variables\") +\n    theme_minimal() +\n    theme(axis.text.x = element_text(angle = 90, hjust = 1),\n          axis.text.y = element_text(angle = 0, vjust = 1))\n}\n\n# Plotting heatmaps for each correlation matrix\nif (ncol(binary_data) &gt; 1) {\n  print(plot_heatmap(cor_binary, \"Correlation Heatmap of Binary Variables\"))\n}\n\n\n\n\n\n\n\nif (ncol(ordinal_data) &gt; 1) {\n  print(plot_heatmap(cor_ordinal, \"Correlation Heatmap of Ordinal Variables\"))\n}\n\n\n\n\n\n\n\nif (ncol(continuous_data) &gt; 1) {\n  print(plot_heatmap(cor_continuous, \"Correlation Heatmap of Continuous Variables\"))\n}\n\n\n\n\n\n\n\n\nFor modeling, we used a linear model to try to predict whether someone would be arrested based on their race, years of education, if their household is living in poverty, and their gender. We separated the data into a train and test set with an 80% split. In the training set, the F-statistic is far over 1 and the p-values show over 95% confidence in all the response variables being significant.\n\nsplit &lt;- initial_split(filtered, prop = .8) #good way to keep yourself honest. splits it by prop % being in training, 1-prop being test\ntraining&lt;- training(split)\ntesting &lt;- testing(split)\npredict_filtered &lt;- training |&gt;\n  filter(!is.na(ARRESTED) & !is.na(RACE) & !is.na(EDUCATION) & !is.na(HHPOV) & !is.na(MALE))\n\nmod1 &lt;- lm(ARRESTED ~ RACE + EDUCATION + HHPOV + MALE, predict_filtered)\nsummary(mod1)\n\n\nCall:\nlm(formula = ARRESTED ~ RACE + EDUCATION + HHPOV + MALE, data = predict_filtered)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.14796 -0.03851 -0.02355 -0.00673  1.01148 \n\nCoefficients:\n                                         Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)                             0.1005132  0.0091200  11.021  &lt; 2e-16\nRACEHispanic                           -0.0332987  0.0065175  -5.109 3.28e-07\nRACEOther or multiracial, Non-Hispanic -0.0300471  0.0084248  -3.566 0.000363\nRACEWhite, Non-Hispanic                -0.0300465  0.0050001  -6.009 1.92e-09\nEDUCATION                              -0.0045524  0.0005609  -8.116 5.24e-16\nHHPOV                                   0.0215277  0.0045161   4.767 1.89e-06\nMALE                                    0.0259233  0.0027728   9.349  &lt; 2e-16\n                                          \n(Intercept)                            ***\nRACEHispanic                           ***\nRACEOther or multiracial, Non-Hispanic ***\nRACEWhite, Non-Hispanic                ***\nEDUCATION                              ***\nHHPOV                                  ***\nMALE                                   ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1549 on 12586 degrees of freedom\nMultiple R-squared:  0.01913,   Adjusted R-squared:  0.01867 \nF-statistic: 40.92 on 6 and 12586 DF,  p-value: &lt; 2.2e-16\n\nplot(mod1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn the test set, the F-statistic is much lower and the p-values and t-stats rise for each variable, with the p-value for each race variable but the intercept being over 0.1. This finding implies that there isn’t a significant relationship on average between these race categories and being arrested, although, for “Other or multiracial, Non-Hispanic”, that could be skewed by the much smaller sample size of the category compared to the other races in the category. Futhermore, the difference in performance between the testing and training data implies an overfitting that is being done by the model, that is different from actually learning the relationship.\n\npredict_filtered_test &lt;- testing |&gt;\n  filter(!is.na(ARRESTED) & !is.na(RACE) & !is.na(EDUCATION) & !is.na(HHPOV) & !is.na(MALE))\n\nmod2 &lt;- lm(ARRESTED ~ RACE + EDUCATION + HHPOV + MALE, predict_filtered_test)\nsummary(mod2)\n\n\nCall:\nlm(formula = ARRESTED ~ RACE + EDUCATION + HHPOV + MALE, data = predict_filtered_test)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.13360 -0.04190 -0.02285 -0.00936  1.01456 \n\nCoefficients:\n                                        Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)                             0.082088   0.018601   4.413 1.05e-05\nRACEHispanic                            0.004917   0.012921   0.381 0.703573\nRACEOther or multiracial, Non-Hispanic -0.020445   0.017103  -1.195 0.232012\nRACEWhite, Non-Hispanic                -0.006045   0.010063  -0.601 0.548032\nEDUCATION                              -0.004763   0.001149  -4.144 3.51e-05\nHHPOV                                   0.033108   0.009843   3.364 0.000779\nMALE                                    0.023010   0.005689   4.045 5.36e-05\n                                          \n(Intercept)                            ***\nRACEHispanic                              \nRACEOther or multiracial, Non-Hispanic    \nRACEWhite, Non-Hispanic                   \nEDUCATION                              ***\nHHPOV                                  ***\nMALE                                   ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.158 on 3102 degrees of freedom\nMultiple R-squared:  0.01831,   Adjusted R-squared:  0.01641 \nF-statistic: 9.644 on 6 and 3102 DF,  p-value: 1.537e-10\n\nplot(mod2)"
  },
  {
    "objectID": "posts/2024-11-11-Other-datasets/Other datasets.html",
    "href": "posts/2024-11-11-Other-datasets/Other datasets.html",
    "title": "Other datasets",
    "section": "",
    "text": "Dataset: Police Sentiment Survey\nDescription The Tempe Police Sentiment Survey dataset captures detailed perceptions of safety and trust in the Tempe community, categorized by demographics such as race, age, gender, education, and income levels. Also, data is catagorized by different geographical areas within Tempe, including patrol zones and beats. The metrics include community members’ feelings of safety and levels of trust and respect towards the police. Responses are broken down further to reflect sentiments within specific demographic groups, covering general and respect-focused trust levels. Each record includes start and end dates for when the survey data was collected, providing a time-based context for analyzing sentiment changes over time. The weakness of the dataset is that the data does not align with our original dataset. Our original dataset’s date is from 2002-2011, and this dataset is more recent, with data afte 2020. We’re not sure if this works.\nCombination We haven’t started to combine the dataset yet, but we have some initial thoughts. For example, we can do some incident-based analysis. We can examine how specific types of police-public interactions (e.g., arrests, traffic stops) affect sentiment. We can match by incident identifiers and by combining incidents from both datasets, we can analyze whether particular types of police interactions correlate with higher or lower levels of trust and safety sentiments within specific demographics or locations. Also, this dataset includes many socioeconomic factors such as race, education level, income, and age, etc. We can also take these factors into account.\nDataset:Firearm Suicide Proxy for Household Gun Ownership\nDescription This dataset describes the number of homocides, the number of suicides, and the number of suicides done with firearms for whites & nonwhites within different states from 1949 to 2020. These numbers, particularly the FSS (the ratio of the number of suicides done with firearms/the total number of suicides) is used as a much less biased estimator of the total gun ownership in the US than traditional voluntary surveys, in which people underestimate the number of guns they own out of social desirability bias. Advantages of linking this particular dataset include the overlap of the demographics, state variables, and time as well as a probable connection between gun ownership and crime prevalence within communities. Luckily, this dataset has few missing values and the time periods analyzed closely correspond to those in our original dataset. Nevertheless, if used, we anticipate some problems. One problem arises from the fact that the demographic data is only split between whites and non-whites, which may prevent us from understanding the true role of demographics on police reporting data. Another problem arises from the fact that FSS is used as an estimate of gun ownership rates instead of definite numbers of guns per person/household. A separate dataset might need to be used to link FSS to gun ownership rates.\nCombination Although we haven’t started to combine the dataset, we plan on observing whether there exists a correlation between FSS (which correlates with the number of guns per capita) and crime reporting prevalence in various communities. We would expect a positive correlation, but confounding variables (demographics, safety, etc.) may make the correlation less obvious. We also plan on analyzing the differences between the FSS of whites and non-whites within different states and whether that correlates with the number of crimes reported by separate demographics. Finally, we plan on determining whether a correlation can be observed between homocide rate and general crime rate in different states.\nDataset: State-Level Household Gun Ownership Proxy Dataset\nDescription This dataset holds the count and population-adjusted rate of suicides, firearm suicides, homicides, and firearm homicides, among other figures. In general it was created by extending an existing proxy for U.S. household gun ownership rates. Most importantly this data was not compiled from self-reporting data, which can be heavily skewed based on the bias of those who choose to report. This extended proxy represents the most comprehensive report of state-level gun ownership rates and trends.\nCombination We wanted to check if this dataset would be compatible with our previous dataset. In terms of combining them into a its own study we would look to see how the gun-ownership of individuals may affect their view on their interactions with the police. In doing so we can see if those individuals in higher gun ownership states are more comfortable or apprehensive in their interactions with law enforcement. In the same affect we can see if the presence of a gun owner changes the way that law enforcement approach interactions with the public. In doing a more detailed analysis we can see if gun ownership improves the relationship of law enforcement and those they serve and protect.\nFurthermore we may also see how trends in gun ownership across time correlate to patterns in the original survey we studied. This can be used in comprehension with regional changes to better study any pattern that may exist. Having multiple means of doing so would reduce the likelihood of chance being the root cause of any patterns."
  },
  {
    "objectID": "posts/2024-10-28-Data-Cleaning/Data Cleaning.html",
    "href": "posts/2024-10-28-Data-Cleaning/Data Cleaning.html",
    "title": "Data Cleaning and Loading & Data for Equity",
    "section": "",
    "text": "This week, we finalized the data description and characterization before beginning the process of loading and cleaning the data. The data, which was collected from the Bureau of Justice Statistics (BJS), is a person-level and household-level dataset characterizing police encounter data, emphasizing key variables to study the relationship between police contact and victimization reporting. The raw data consists of 110 variables and 105273 unique observations.\nThe first step in the process of loading & cleaning the data was converting the tsv file we found online to a R dataset. We did so through the read_tsv tidyverse function, and the process went smoothly. There were no errors, unusual values, or oddly formatted data other than NA, or missing, values. After performing this initial data import, we decided to filter out specific variables that we didn’t intend on using in our analysis. These included variables such as PSTRATA and SECUCODE, which are more complex values used to calculate parameters like variance, and WEIGHT, which was the relative weight given to each observation due to dataset size differences across different years. We decided to remove variables like these because we realized that, in order to perform analysis of the data ourselves, it would be more accurate and meaningful to calculate these parameters from scratch. That way, we would decrease the risk of performing a biased analysis with data values we didn’t fully comprehend.\nWe also made the systematic decision to remove the variables corresponding to data about follow-up interviews, like NUM_FU_HHINT and NUM_FU_PERINT. We did so because we currently don’t intend to use that data in our analysis. Similarly, we decided to systematically remove the variable TIME2VIC_INC_P23PER because we don’t want to investigate data about the time between PPCS interview and victimization.\nFinally, we were forced to exclude certain variables from the dataset that contained values we didn’t understand. For example, all variables that ended with “_sub” were excluded because we were unsure what the description “missing carried back from later waves” from the data info sheet meant.\nAn important next step in the data cleaning process was exploring the data through barplots showing the distributions for specific variables and boxplots showing outliers. This would aid us in our analysis and further steps when working with the data. As an example, here is a function used to create boxplots:\ncreate_outlier_boxplot &lt;- function(data, col_name) { ggplot(data, aes_string(y = col_name)) + geom_boxplot(outlier.color = “red”,\noutlier.shape = 16,\noutlier.size = 2) +\nlabs(title = paste(“Boxplot of”, col_name, “with Outliers”), y = col_name) + theme_minimal() }\nData For Equity Principles:\nBeneficence: The dataset we are working on is about crimes and how interactions with police can influence how, why, or if people report them. It is important to us to not misrepresent a certain group of people as more prone to positive or negative interactions with police, as that can reflect a high degree of personal bias. We must represent the data objectively, discussing its implications and results within a wider scope and context. Nevertheless, there are always variables left out of scope that can influence the results.\nRespect for Persons: The analysis that we aim to do for this dataset is by no means definitive, and it leaves us with many open-ended questions and perhaps concerns. We do not want to bias the viewers of this analysis, and we give them the power to make their own decisions about this data. Holding an unbiased persective also allows usgiving respect to the groups showcased in the data, as we do not want to misrepresent their data."
  },
  {
    "objectID": "posts/2024-10-11-First-Blog-Post/firstBlog.html",
    "href": "posts/2024-10-11-First-Blog-Post/firstBlog.html",
    "title": "First Blog",
    "section": "",
    "text": "https://www.icpsr.umich.edu/web/NACJD/studies/36370/\nThis dataset looks into how prior experiences with the police can influence one’s decision to report a crime. The data is from 2002-2011. It includes 110 variables and 105273 rows with information on the prevalence, frequency, and nature of respondents’ encounters with the police in the prior year, as well as respondents’ personal and household victimization experiences that occurred after the administration of the PPCS, including whether the crime was reported to the police. Demographic variables include age, race, gender, education, and socioeconomic status.\nThe data groups race and ethnicity in the variable label C4_RACE into these categories:\n1. White, Non-Hispanic\n2. Black, Non-Hispanic\n3. Hispanic\n4. Other or multiracial, Non-Hispanic\nThe variables for the encounters with the police are numerical such as whether the respondent was arrested or handcuffed, incident frequency, etc. which makes it easy to plot by race, gender, or other categorical variables.\nhttps://catalog.data.gov/dataset/rates-and-trends-in-heart-disease-and-stroke-mortality-among-us-adults-35-by-county-a-2000-45659\nRates and Trends in Heart Disease and Stroke Mortality Among US Adults (35+) by County, Age Group, Race/Ethnicity, and Sex – 2000-2019\nThe dataset includes 21 columns and 5,770,224 rows. There is not much said about the data being collected in this dataset, it is said that “rates and trends were estimated using Bayesian spatiotemporal model and a smoothed over space and time demographic group”. Additionally, using the 2010 US population figures, the ages are standardized to 10-year groups. There is no issue with loading and cleaning the data, however the csv function used before in class may have a row limit lower than the number of rows in this dataset. This could for example be addressed by considering a number of rows under that row limit. The dataset allows for attempts to understand what kind of stroke or deceive is prevalent among certain racial groups, regions (counties, states), genders or ages, Perhaps, there could also be a difference in regions by year, which would be interesting to see given there is a lot of change in a specific region. Immediately, a challenge will be to decide whether to include the over 5 million rows in the analysis, which would require using some other csv reader or adjust the data for this one which could alter the results of the analysis. Another challenge are the overall categories within some variables in the dataset (“overall” in sex and gender variables), which may be of not great use when we would be trying to analyze for differences by either of the variables. To conclude, the dataset seems to be rich, however it does have some aspects that may be a challenge in the initial analysis stages.\nhttps://data.nysed.gov/downloads.php\nThe data describes students enrollment reported by total public school (aggregated data for all districts and charter schools), county (aggregated data for all districts and charter schools in the county), Needs-to-Resource-Capacity (N/RC) group, district, and public schools.  \n  The ENTITY_CD is the 12-digit Basic Educational Data System (BEDS) code that uniquely identifies the entity (school, district, etc.). The need/resource capacity index, a measure of a district’s ability to meet the needs of its students with local resources, is the ratio of the estimated poverty percentage1 (expressed in standard score form) to the Combined Wealth Ratio2 (expressed in standard score form).  \n  The dataset contains the different enrollments by grade, race/ethnicity, gender and other groups."
  },
  {
    "objectID": "posts/2023-10-15-getting-started/getting-started.html",
    "href": "posts/2023-10-15-getting-started/getting-started.html",
    "title": "Getting started",
    "section": "",
    "text": "Below, the items marked with [[OP]] should only be done by one person on the team.\n\nTo get started\n\n[[OP]] One person from the team should click the Github Classroom link on Teams.\n[[OP]] That person types in the group name for their group.\nThe rest of the team now clicks the Github Classroom link and selects their team from the dropdown list.\nFinally, each of you can clone the repository to your laptop like a normal assignment.\n\n\n\nSetting up the site\n\n[[OP]] Open the terminal and run quarto publish gh-pages.\n[[OP]] Select Yes to the prompt:  ? Publish site to https://sussmanbu.github.io/ma4615-fa23-final-project-TEAMNAME/ using gh-pages? (Y/n)\n[[OP]] Wait for the process to finish.\nOnce it is done, you can go to the URL it asked you about to see your site.\n\nNote: This is the process you will use every time you want to update your published site. Make sure to always follow the steps below for rendering, previewing, and committing your changes before doing these publish steps. Anyone can publish in the future.\n\n\nCustomize your site\n\n[[OP]] Open the _quarto.yml file and update the title to include your team name.\n[[OP]] Go to the about.qmd and remove the TF’s and professor’s names.\nadd your own along with a short introduction and a link to your Github user page.\n[[OP]] Render the site.\n[[OP]] Check and make sure you didn’t get any errors.\n[[OP]] Commit your changes and push.\n[[OP]] Repeat the steps under Setting up your site.\n\nOnce one person is done with this, each teammate in the group can, in turn, repeat steps 3-7. Before doing so, make sure to pull the changes from teammates before starting to make new changes. (We’ll talk soon about ways to organize your work and resolve conflicts.)\n\n\nStart your first post\n\nTo start your first post first, run remotes::install_github(\"sussmanbu/quartopost\") in your Console.\n[[OP]] Run quartopost::quartopost() (or click Addins-&gt;Create Quarto Post, or use C-Shift-P, type “Create Quarto” and press enter to run the command).\n\nNow you can start working on your post. You’ll want to render your post to see what it will look like on the site.\n\nEvery time you want to make a new post, you can repeat step 2 above.\nWhen you want to publish your progress, follow steps 4-7 from Customize your site.\n\nFinally, make sure to read through everything on this site which has the directions and rubric for the final project."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MA [46]15 Final Project",
    "section": "",
    "text": "Final Project due May 7, 2024 at 11:59pm.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmore EDA on Other Datasets\n\n\n\n\n\nMore EDA on other datasets \n\n\n\n\n\nNov 18, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nOther datasets\n\n\n\n\n\nBlog post describing other datasets we found and our initial thoughts to combine them with our origial dataset. \n\n\n\n\n\nNov 11, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nmore EDA\n\n\n\n\n\nMore EDA and Initial thoughts \n\n\n\n\n\nNov 6, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nEDA\n\n\n\n\n\nBlog post discussing further EDA and initial thoughts on statistical modeling. \n\n\n\n\n\nNov 4, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nData Cleaning and Loading & Data for Equity\n\n\n\n\n\nBlog post outlining specifics of the data and how we went about cleaning it, additionally discussing the data for equity principles, as well as related article. \n\n\n\n\n\nOct 28, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nData Background\n\n\n\n\n\nBlog post outlining background information related to our dataset. \n\n\n\n\n\nOct 15, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nFirst Blog\n\n\n\n\n\nFirst Blog Post \n\n\n\n\n\nOct 11, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nExamples\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 26, 2024\n\n\nDaniel Sussman\n\n\n\n\n\n\n\n\n\n\n\n\nGetting started\n\n\n\n\n\n\n\n\nDirections to set up your website and create your first post. \n\n\n\n\n\nFeb 23, 2024\n\n\nDaniel Sussman\n\n\n\n\n\n\n\n\n\n\n\n\nFirst Team Meeting\n\n\n\n\n\n\n\n\nThis post details the steps you’ll take for your first team meeting. \n\n\n\n\n\nFeb 21, 2024\n\n\nDaniel Sussman\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "big_picture.html",
    "href": "big_picture.html",
    "title": "Big Picture",
    "section": "",
    "text": "This comes from the file big_picture.Rmd.\nThink of this page as your 538/Upshot style article. This means that you should try to tell a story through the data and your analysis. Read articles from those sites and similar sites to get a feeling for what they are like. Try to write in the style of a news or popular article. Importantly, this page should be geared towards the general public. You shouldn’t assume the reader understands how to interpret a linear regression or a complicated plot. Focus on interpretation and visualizations."
  },
  {
    "objectID": "big_picture.html#rubric-on-this-page",
    "href": "big_picture.html#rubric-on-this-page",
    "title": "Big Picture",
    "section": "Rubric: On this page",
    "text": "Rubric: On this page\nYou will\n\nTitle\n\nYour big picture page should have a creative/click-bait-y title/headline that provides a hint about your thesis.\n\nClarity of Explanation\n\nYou should have a clear thesis/goal for this page. What are you trying to show? Make sure that you explain your analysis in detail but don’t go into top much mathematics or statistics. The audience for this page is the general public (to the extent possible). Your thesis should be a statement, not a question.\nEach figure should be very polished and also not too complicated. There should be a clear interpretation of the figure so the figure has a clear purpose. Even something like a histogram can be difficult to interpret for non-experts.\n\nCreativity\n\nDo your best to make things interesting. Think of a story. Think of how each part of your analysis supports the previous part or provides a different perspective.\n\nInteractive component\n\nQuality and ease of use of the interactive components. Is it clear what can be explored using your interactive components? Does it enhance and reinforce your conclusions?\n\nThis page should be self-contained.\n\nNote: This page should have no code visible, i.e. use #| echo: FALSE."
  },
  {
    "objectID": "big_picture.html#rubric-other-components",
    "href": "big_picture.html#rubric-other-components",
    "title": "Big Picture",
    "section": "Rubric: Other components",
    "text": "Rubric: Other components\n\nVideo Recording\nMake a video recording (probably using Zoom) demonstrating your interactive components. You should provide a quick explanation of your data and demonstrate some of the conclusions from your EDA. This video should be no longer than 4 minutes. Include a link to your video (and password if needed) in your README.md file on your Github repository. You are not required to provide a link on the website. This can be presented by any subset of the team members.\n\n\nRest of the Site\nFinally, here are important things to keep in mind for the rest of the site.\nThe main title of your page is informative. Each post has an author/description/informative title. All lab required posts are present. Each page (including the home page) has a nice featured image associated with it. Your about page is up to date and clean. You have removed the generic posts from the initial site template."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This comes from the file about.qmd.\nThis is a website for the final project for MA[46]15 Data Science with R by Team Group 7. The members of this team are below."
  },
  {
    "objectID": "about.html#ian-ding",
    "href": "about.html#ian-ding",
    "title": "About",
    "section": "Ian Ding",
    "text": "Ian Ding\nIan is a junior studying Math and Computer Science. Github"
  },
  {
    "objectID": "about.html#arjun-patel",
    "href": "about.html#arjun-patel",
    "title": "About",
    "section": "Arjun Patel",
    "text": "Arjun Patel\nArjun Patel is a junior majoring in Medical Sciences and Applied Math. Github"
  },
  {
    "objectID": "about.html#zexian-xu",
    "href": "about.html#zexian-xu",
    "title": "About",
    "section": "Zexian Xu",
    "text": "Zexian Xu\nZexian Xu is a senior majoring in Statistics and Computer Science. Github"
  },
  {
    "objectID": "about.html#rukevwe-omusi",
    "href": "about.html#rukevwe-omusi",
    "title": "About",
    "section": "Rukevwe Omusi",
    "text": "Rukevwe Omusi\nRukevwe Omusi is a senior majoring in Data Science with a Film minor. Github"
  },
  {
    "objectID": "about.html#sviatoslav-shevchenko",
    "href": "about.html#sviatoslav-shevchenko",
    "title": "About",
    "section": "Sviatoslav Shevchenko",
    "text": "Sviatoslav Shevchenko\nSviat is a junior pursuing a dual degree in Data Science and Economics. Github\n\n\nAbout this Template.\nThis is based off of the standard Quarto website template from RStudio (2023.09.0 Build 463)."
  },
  {
    "objectID": "analysis.html",
    "href": "analysis.html",
    "title": "Analysis",
    "section": "",
    "text": "Loading our Police Interaction dataset and all libraries.\n\nsuppressPackageStartupMessages(library(tidymodels))\nsuppressPackageStartupMessages(library(reshape2)) #For models\nsuppressPackageStartupMessages(library(ggplot2)) #For plots\nsuppressPackageStartupMessages(library(shiny))\nsuppressPackageStartupMessages(library(shinylive)) #For interactive plots\nsuppressPackageStartupMessages(library(readr)) # For reading .rds files\nsuppressPackageStartupMessages(library(dplyr))\nsuppressPackageStartupMessages(library(tidyverse)) #For data wrangling, manipulation, etc.\nsuppressPackageStartupMessages(library(viridis)) #For visual themes\n\nWarning: package 'viridis' was built under R version 4.4.2\n\nsuppressPackageStartupMessages(library(sf)) \n\nWarning: package 'sf' was built under R version 4.4.2\n\nsuppressPackageStartupMessages(library(maps)) #For maps\nsuppressPackageStartupMessages(library(DT))\n\nWarning: package 'DT' was built under R version 4.4.2\n\ndata &lt;- read_rds(here::here(\"dataset/police_interaction.rds\"))\n\nOne initial step in conducting EDA is by looking at the “PROPER” variable, which outlines whether or not the police behaved properly during the interaction, and seeing how the proportion of improper actions differs between individuals of different races.\n\ndata |&gt;\n  filter(!is.na(PROPER)) |&gt;\n  group_by(RACE) |&gt;\n  summarize(\n    improper_proportion = sum(PROPER == 0) / n()\n  ) |&gt;\n  ggplot(aes(x = RACE, y = improper_proportion, fill = RACE)) +\n  geom_bar(stat = \"identity\") + \n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\nThis second plot is a heatmap that visualizes the correlations between various continuous variables in the dataset, where the color intensity represents the strength and direction of the correlation (with red indicating positive correlations and blue indicating negative correlations). There are notable positive correlations such as HHPOV and WORK_LW and notable negative correlations such as WORK_LW and any high-income indicator.\n\n# Categorizing variables based on their types\n#removed \"CONTACT\" and \"ERROR\" due to \"Warning in cor(numeric_data, use = \"complete.obs\") : the standard deviation is zero\" error\nbinary_columns &lt;- c(\"MALE\", \"WORK_LW\", \"HHPOV\", \"PUB_HOUSE\",   \"PUB_HOUSE_SUB\", \"REGION\", \"INPERSON\", \"VICAR_CITIZEN\", \"VICAR_PRO_AUTO\", \"VICAR_PRO_PERS\", \"VICAR_OTH_CONT\", \"VICAR_IMPROPER\", \"D_HH_P23\", \"PROPER\")\n\nordinal_columns &lt;- c(\"C4_RACE\", \"MAR_STAT\", \"FREQ_DRV\", \"TENURE\",  \"MSA_STATUS\")\n\ncontinuous_columns &lt;- c(\"AGE\", \"EDUCATION\", \"EDUCATION_SUB\", \"NUM_MOVES\", \"NUM_CONT\", \"HH_SIZE\", \"PPCS_YEAR\", \"N_HH_P1\", \"N_PERS_P1\", \"NUM_CITIZEN_HH\", \"NUM_PRO_AUTO_HH\", \"NUM_PRO_PERS_HH\", \"NUM_OTH_CONT_HH\", \"NUM_IMPROPER_HH\")\n\nfiltered &lt;- data\n# Selecting data by type\nbinary_data &lt;- filtered %&gt;%\n  select(all_of(binary_columns)) %&gt;%\n  select(where(is.numeric))\n\nordinal_data &lt;- filtered %&gt;%\n  select(all_of(ordinal_columns)) %&gt;%\n  select(where(is.numeric))\n\ncontinuous_data &lt;- filtered %&gt;%\n  select(all_of(continuous_columns)) %&gt;%\n  select(where(is.numeric))\n\n# Calculating correlation matrices with appropriate methods\ncor_binary &lt;- cor(binary_data, use = \"pairwise.complete.obs\", method = \"pearson\")\ncor_ordinal &lt;- cor(ordinal_data, use = \"pairwise.complete.obs\", method = \"spearman\")\ncor_continuous &lt;- cor(continuous_data, use = \"pairwise.complete.obs\", method = \"pearson\")\n\n# Creating a heatmap function\nplot_heatmap &lt;- function(cor_matrix, title) {\n  cor_melted &lt;- melt(cor_matrix)\n  ggplot(cor_melted, aes(Var1, Var2, fill = value)) +\n    geom_tile(color = \"white\") +\n    scale_fill_gradient2(low = \"blue\", high = \"red\", mid = \"white\",\n                         midpoint = 0, limit = c(-1, 1), name = \"Correlation\") +\n    labs(title = title, x = \"Variables\", y = \"Variables\") +\n    theme_minimal() +\n    theme(axis.text.x = element_text(angle = 90, hjust = 1),\n          axis.text.y = element_text(angle = 0, vjust = 1))\n}\n\n# Plotting heatmaps for each correlation matrix\nif (ncol(continuous_data) &gt; 1) {\n  print(plot_heatmap(cor_continuous, \"Correlation Heatmap of Continuous Variables\"))\n}\n\n\n\n\n\n\n\n\nFor modeling, we used a linear model with an 80% split train and test set to try to predict whether someone would be arrested based on their race, years of education, if their household is living in poverty, and their gender. In the training set, the F-statistic is far over 1 and the p-values show over 95% confidence in all the response variables being significant.\n\nsplit &lt;- initial_split(filtered, prop = .8) #good way to keep yourself honest. splits it by prop % being in training, 1-prop being test\ntraining&lt;- training(split)\ntesting &lt;- testing(split)\npredict_filtered &lt;- training |&gt;\n  filter(!is.na(ARRESTED) & !is.na(RACE) & !is.na(EDUCATION) & !is.na(HHPOV) & !is.na(MALE))\n\nmod1 &lt;- lm(ARRESTED ~ RACE + EDUCATION + HHPOV + MALE, predict_filtered)\n\nIn the test set, the F-statistic is much lower and the p-values and t-stats rise for each variable, with the p-value for each race variable but the intercept being over 0.1. This finding implies that there isn’t a significant relationship on average between these race categories and being arrested, although, for “Other or multiracial, Non-Hispanic”, that could be skewed by the much smaller sample size of the category compared to the other races in the category.\n\npredict_filtered_test &lt;- testing |&gt;\n  filter(!is.na(ARRESTED) & !is.na(RACE) & !is.na(EDUCATION) & !is.na(HHPOV) & !is.na(MALE))\n\nmod2 &lt;- lm(ARRESTED ~ RACE + EDUCATION + HHPOV + MALE, predict_filtered_test)\nplot(mod2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe added another dataset that describes the number of homicides, the number of suicides, and the number of suicides done with firearms for whites & nonwhites within different states from 1949 to 2020.\nDataset:Firearm Suicide Proxy for Household Gun Ownership\nLoading our Firearm dataset.\n\nfirearm_data &lt;- read_rds(\"dataset/firearm_sh_ds.rds\")\n\nThis map represents the rate of homicides by firearm by state.\n\nfirearm_data &lt;- firearm_data |&gt;\n  mutate(firearm_homicide_rate = as.numeric(firearm_homicide_rate),\n         state = tolower(state)) |&gt;\n  filter(!is.na(firearm_homicide_rate))\n\nus_states &lt;- st_as_sf(maps::map(\"state\", plot = FALSE, fill = TRUE)) %&gt;%\n  mutate(region = tolower(ID)) %&gt;% # Convert state names to lowercase\n  select(region, geom) # Retain only state name and geometry columns\n\n\nmap_data &lt;- us_states |&gt;\n  rename(state = region) |&gt;\n  left_join(firearm_data, by = \"state\")\n\nmap_data$log_firearm_homicide_rate &lt;- log(map_data$firearm_homicide_rate + 1)\n \nggplot(map_data) +\n  geom_sf(aes(fill = log_firearm_homicide_rate), color = \"black\", size = 0.2) +\n  scale_fill_viridis_c(option = \"plasma\", name = \"Homicide Rate\") +\n  theme_minimal() +\n  labs(\n    title = \"Firearm Homicide Rate by State\",\n    fill = \"Rate\"\n  ) +\n  facet_wrap(~year)\n\n\n\n\n\n\n\n\nWe combined the two datasets based on locations. The first dataset was divided into four regions: Northeast, Midwest, South, and West while the second dataset was divided into different states.\nWe grouped data from both datasets by region and year using the regional definitions from the Census Bureau Designated Regions. For each region and year the data was summarized into categories that include proportional data for certain types of incidents related to police contact, racial distributions, homicide rate, etc.\nThe combination of the data is done using the code in “/scripts/load_and_clean_data.R”. Below is a table, which displays each new column and its description. Followed by the new combined data itself.\n\nvariable_table &lt;- tibble(\n  Variable_Name = c(\"region\", \"year\", \"total_count\", \"WHITE_NH_PROP\", \"B_NH_PROP\", \"HISPANIC_PROP\", \"OTHER_MULTI_NH_PROP\", \"CONTACT_FREQ\", \"OC_FRISK_PROP\", \"TC_FRISK_PROP\", \"VSRCH_PROP\", \"ARREST_PROP\", \"CUFFED_PROP\", \"PROPER_PROP\", \"IMPROPER_PROP\", \"AVG_CONT\", \"population\", \"fa_homicides\", \"nfa_homicides\", \"homicides\", \"fa_homicide_rate\", \"nfa_homicide_rate\", \"homicide_rate\"),\n  Variable_Description = c(\"Region the data is from\", \"Year the data is from\", \"Total number of responses in the police sentiment dataset\", \"White non-hispanic proportion of responses for police sentiment\", \"Black non-hispanic proportion of responses for police sentiment\", \"Hispanic proportion of responses for police sentiment survey\", \"Other/multiracial/Non-hispanic proportion of responses for police sentiment survey\", \"Proportion of responders who had contact with law enforcement\", \"Proportion of stop and frisks for non-traffic stops\", \"Proportion of stop and frisks for traffic stops\", \"Proportion of vehicle searchs\", \"Proportions of contacts that ended in arrest\", \"Proportion of contacts that included cuffing\", \"Proportion of contacts deemed handled proper\", \"Proportion of contacts deemed unproperly handled\", \"Average number of contacts in the last year\", \"Total region population from US census\", \"Number of Firearm homicides\", \"Number of non-firearm homicides\", \"number of homidides\", \"firearm homicides per 100,000\", \"non-firearm homicides per 100,000\", \"homicides per 100,000\")\n)\n\nprint(variable_table, n = Inf)\n\n# A tibble: 23 × 2\n   Variable_Name       Variable_Description                                     \n   &lt;chr&gt;               &lt;chr&gt;                                                    \n 1 region              Region the data is from                                  \n 2 year                Year the data is from                                    \n 3 total_count         Total number of responses in the police sentiment dataset\n 4 WHITE_NH_PROP       White non-hispanic proportion of responses for police se…\n 5 B_NH_PROP           Black non-hispanic proportion of responses for police se…\n 6 HISPANIC_PROP       Hispanic proportion of responses for police sentiment su…\n 7 OTHER_MULTI_NH_PROP Other/multiracial/Non-hispanic proportion of responses f…\n 8 CONTACT_FREQ        Proportion of responders who had contact with law enforc…\n 9 OC_FRISK_PROP       Proportion of stop and frisks for non-traffic stops      \n10 TC_FRISK_PROP       Proportion of stop and frisks for traffic stops          \n11 VSRCH_PROP          Proportion of vehicle searchs                            \n12 ARREST_PROP         Proportions of contacts that ended in arrest             \n13 CUFFED_PROP         Proportion of contacts that included cuffing             \n14 PROPER_PROP         Proportion of contacts deemed handled proper             \n15 IMPROPER_PROP       Proportion of contacts deemed unproperly handled         \n16 AVG_CONT            Average number of contacts in the last year              \n17 population          Total region population from US census                   \n18 fa_homicides        Number of Firearm homicides                              \n19 nfa_homicides       Number of non-firearm homicides                          \n20 homicides           number of homidides                                      \n21 fa_homicide_rate    firearm homicides per 100,000                            \n22 nfa_homicide_rate   non-firearm homicides per 100,000                        \n23 homicide_rate       homicides per 100,000                                    \n\n\nThis chart normalizes racial proportions by multiplying by 100 to plot alongside homicide rates on the same scale, then multiplies firearm homicide rate by 10 to get a better understanding of what the trend looks like.\n\ndata &lt;- read_rds(here::here(\"dataset\", \"combined_regional_data.rds\"))\nggplot(data, aes(x = year)) +\n  geom_line(aes(y = fa_homicide_rate * 10, color = \"Firearm Homicide Rate * 10\"), size = 1) +\n  geom_line(aes(y = WHITE_NH_PROP * 100, color = \"White NH Prop\"), linetype = \"dashed\", size = 0.8) +\n  geom_line(aes(y = B_NH_PROP * 100, color = \"Black NH Prop\"), linetype = \"dashed\", size = 0.8) +\n  facet_wrap(~ region) +\n  labs(\n    title = \"Trends in Firearm Homicide Rate and Racial Proportions Over Time\",\n    x = \"Year\",\n    y = \"Rate (%)\",\n    color = \"Variable\"\n  ) +\n  theme_minimal()\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\nThis interactive scatter plot shows the Firearm Homicide Rate relative to racial proportions of the 4 regions. Most of the firearm homicides took place in the south, with similar firearm homicide rate in the midwest and northeast, and the lowest rate of firearm homicides in the west. However, this dataset lacks data on the race of the homicide victims.\n\n# Define server logic ----\nserver &lt;- function(input, output) {\n  # Load the data\n  # Replace this URL with the actual URL of your .rds file on GitHub Pages\n  data &lt;- read_rds(here::here(\"dataset\", \"combined_regional_data.rds\"))\n    # read_rds(\"https://sussmanbu.github.io/ma-4615-fa24-final-project-group-7/dataset/combined_regional_data.rds\")\n    # Have to configure to get this to link to the dataset\n  \n    # Render the interactive plot\n  output$scatterPlot &lt;- renderPlot({\n    ggplot(data, aes(x = fa_homicide_rate, y = .data[[input$selected_race]])) +\n      geom_point(aes(color = as.factor(year)), size = 2, alpha = 0.7) + # Use color to differentiate years\n      geom_text(aes(label = year), vjust = -1, size = 3, alpha = 0.8) + # Add year annotations above points\n      facet_wrap(~ region) + # Optional: Facet by region\n      labs(\n        title = paste(\"Firearm Homicide Rate vs\", input$selected_race),\n        x = \"Firearm Homicide Rate\",\n        y = \"Proportion\",\n        color = \"Year\"\n      ) +\n      theme_minimal() +\n      theme(\n        legend.position = \"bottom\",       # Place legend at the bottom\n        legend.box = \"horizontal\",        # Align legend items horizontally\n        legend.text = element_text(size = 10), # Adjust legend text size\n        legend.title = element_text(size = 12), # Adjust legend title size\n        legend.key.width = unit(1, \"cm\"), # Add space between legend items\n        legend.spacing.x = unit(0.5, \"cm\") # Increase horizontal spacing\n      ) +\n      guides(\n        color = guide_legend(nrow = 1, byrow = TRUE) # Force a single-row legend\n      )\n  })\n}\n\n\n\n\n# Define UI for the application ----\nui &lt;- fluidPage(\n  # Application title\n  titlePanel(\"Interactive Scatter Plot: Firearm Homicide Rate and Racial Proportions\"),\n  \n  # Sidebar layout with input and output\n  sidebarLayout(\n    # Sidebar panel for inputs\n    sidebarPanel(\n      # Dropdown menu for selecting racial group\n      selectInput(\n        inputId = \"selected_race\",\n        label = \"Select a Racial Group:\",\n        choices = c(\n          \"White NH\" = \"WHITE_NH_PROP\",\n          \"Black NH\" = \"B_NH_PROP\",\n          \"Hispanic\" = \"HISPANIC_PROP\",\n          \"Other/Multi NH\" = \"OTHER_MULTI_NH_PROP\"\n        ),\n        selected = \"WHITE_NH_PROP\"\n      )\n    ),\n    \n    # Main panel for displaying outputs\n    mainPanel(\n      plotOutput(outputId = \"scatterPlot\")\n    )\n  )\n)\n\n# Run the application ----\nshinyApp(ui = ui, server = server)\n\nShiny applications not supported in static R Markdown documents\n\n\n\n# Define the server logic\nserver &lt;- function(input, output) {\n  # Simulated example dataset (replace with actual dataset)\n  data &lt;- data.frame(\n    year = rep(2000:2020, each = 4),\n    region = rep(c(\"North\", \"South\", \"East\", \"West\"), times = 21),\n    fa_homicide_rate = runif(84, 0, 10),\n    WHITE_NH_PROP = runif(84, 0, 1),\n    B_NH_PROP = runif(84, 0, 1),\n    HISPANIC_PROP = runif(84, 0, 1),\n    OTHER_MULTI_NH_PROP = runif(84, 0, 1)\n  )\n  \n  # Reactive filtered data\n  filtered_data &lt;- reactive({\n    data %&gt;%\n      filter(year &gt;= 2000 & year &lt;= 2020) %&gt;%\n      filter(region %in% c(\"North\", \"South\", \"East\", \"West\"))\n  })\n  \n  # Render the summary table\n  output$summaryTable &lt;- DT::renderDataTable({\n    filtered_data() %&gt;%\n      summarise(\n        `Mean Homicide Rate` = mean(fa_homicide_rate, na.rm = TRUE),\n        `Median Homicide Rate` = median(fa_homicide_rate, na.rm = TRUE),\n        `Std Dev Homicide Rate` = sd(fa_homicide_rate, na.rm = TRUE),\n        `Mean Proportion` = mean(.data[[input$selected_race]], na.rm = TRUE),\n        `Median Proportion` = median(.data[[input$selected_race]], na.rm = TRUE),\n        `Std Dev Proportion` = sd(.data[[input$selected_race]], na.rm = TRUE)\n      )\n  })\n}\n\n# Define the UI\nui &lt;- fluidPage(\n  titlePanel(\"Shiny App with Summary Table\"),\n  sidebarLayout(\n    sidebarPanel(\n      selectInput(\n        inputId = \"selected_race\",\n        label = \"Select a Racial Group:\",\n        choices = c(\n          \"White NH\" = \"WHITE_NH_PROP\",\n          \"Black NH\" = \"B_NH_PROP\",\n          \"Hispanic\" = \"HISPANIC_PROP\",\n          \"Other/Multi NH\" = \"OTHER_MULTI_NH_PROP\"\n        ),\n        selected = \"WHITE_NH_PROP\"\n      )\n    ),\n    mainPanel(\n      tabsetPanel(\n        tabPanel(\"Summary Table\", DT::dataTableOutput(outputId = \"summaryTable\"))\n      )\n    )\n  )\n)\n\n# Run the application\nshinyApp(ui = ui, server = server)\n\nShiny applications not supported in static R Markdown documents"
  },
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "Data",
    "section": "",
    "text": "Before we begin, all of the cleaning and combining for our datasets is done with the code in this load and clean file, so please head there for a more detailed breakdown of how all of this was done.\nFirst Data Set:\nThe Effect of Prior Police Contact on Victimization Reporting: Results From the Police-Public Contact and National Crime Victimization Surveys, United States, 2002-2011 (ICPSR 36370) Dataset Link\nThe dataset is collected by the Bureau of Justice Statistics (BJS), which is the primary statistical agency of the US’s Department of Justice. It was first established in 1979 in order to better understand statistics about crimes in the United States. The data collected in these studies provides information on how each individuals’ prior interactions with law enforcement, both directly and through the experiences of others, influence their decisions to report crimes. By linking data from the National Crime Victimization Survey (NCVS) with the Police-Public Contact Survey (PPCS), the researchers could explore the relationship between police encounter experiences and crime reporting behavior. The combined dataset used in this study contains data about survey participants in 2002, 2008, and 2011.\nThe dataset comprises person-level, household-level, and police encounter data, emphasizing key variables to study the relationship between police contact and victimization reporting. At the person level, variables include PER_ID(Unique person identifier), age, race, contact(if the respondent had police contact in the past 12 months), and number of face-to-face police contacts within the past year. At the household level, HH_ID identifies households, while hhpov (poverty status) and HH_size (number of household members) contextualize socioeconomic status. Police encounters are detailed with REASON (reason for contact), arrested (if the respondent was arrested), and cuffed (if handcuffed), which capture the nature of police interactions. Victimization variables like d_HH_P23 (household crime victimization), time2vic_inc_P23HH (time between police interview and crime), and notify_inc_P23HH (police notified of household crime) provide insights into subsequent incidents. Additional demographic variables such as education, mar_stat (marital status), and work_lw (employment status) add socioeconomic context.\nIn terms of cleaning the data, we originally started by planning to remove rows with over 90,000 na values, but quickly realized that some of these highlighted rare experiences with police that are information that would be crucial to our analysis.\nIn noticing that, we decided to more forwards by removing all columns we felt contained information which were not generally relevant to the analysis we wish to conduct. This was done by reading through the documentation for the dataset and filtering out columns we felt were unhelpful to our analysis. Below are a list of a few columns we removed and why:\nPSTRATA: PSEUDO-STRATUM CODE - We didn’t understand what this datapoint was representing, or how to use it.\nSECUCODE - STANDARD ERROR COMPUTATION UNIT CODE: Again another code, which is vague in description.\nnum_fu_HHint, num_fu_perint - Number of follow-up HH interviews post-PPCS, Number of follow-up PERSON interviews post-PPCS : The number of follow-ups for each person or household isn’t relevant to us.\nColumns ending with “_sub”: Data wasn’t originally collected, and carried back later on.\ntime2vic_inc_P23PER: We don’t want to investigate months from PPCS interview to victimization\nThis is all done in our cleaning script. In this script we start by removing certain columns, and then outlining the remaining columns as numerical or categorical, in order to check for inconsistent data values and providing default values to missing data. For most cases, we consider missing data to be okay.\nSecond Data Set: Firearm Suicide Proxy for Household Gun Ownership, 1949-2020 Dataset Link\nThis dataset was collected by Megan Kang and can be found on the Harvard Dataverse collection. The proxy measures household gun ownership trends at a state level. Notably it is not reliant on self reported data, which can be prone to certain biases. This dataset ranges a long period of time from 1949-2020 and represents the longest-ranging database of state-level gun ownership.\nParticularly this dataset includes information across districts, states, and years, and contains comprehensive variables that allow us to study the complex relationship between geography and household gun ownership and misuse. At the district-level we have variables that show the total population of the district and homicide/suicide rates caused by firearms and non-firearms.\nOur goal with this dataset was to combine it with our original dataset related to police sentiment. In doing so we did very minor initial cleaning by removing the following columns:\nfss - a metric irrelevant to our analysis districts - our original dataset uses regions, so we can construct those regions using the state level descriptions suicide related metrics - we felt that homicide information was more relevant to the way police may choose to approach situations\nAs well as removing all years besides the years which were present in our original dataset.\nCombination In combining the datasets we chose to combine information such that we would display information based on the region and year combinations. This is because the original data was presented across 4 different regions: Northeast, Midwest, South, and West as well as 3 years: 2002, 2008, and 2011.\nOur final combined dataset includes the following variables\n\nsuppressPackageStartupMessages(library(tidyverse))\nvariable_table &lt;- tibble(\n  Variable_Name = c(\"region\", \"year\", \"total_count\", \"WHITE_NH_PROP\", \"B_NH_PROP\", \"HISPANIC_PROP\", \"OTHER_MULTI_NH_PROP\", \"CONTACT_FREQ\", \"OC_FRISK_PROP\", \"TC_FRISK_PROP\", \"VSRCH_PROP\", \"ARREST_PROP\", \"CUFFED_PROP\", \"PROPER_PROP\", \"IMPROPER_PROP\", \"AVG_CONT\", \"population\", \"fa_homicides\", \"nfa_homicides\", \"homicides\", \"fa_homicide_rate\", \"nfa_homicide_rate\", \"homicide_rate\"),\n  Variable_Description = c(\"Region the data is from\", \"Year the data is from\", \"Total number of responses in the police sentiment dataset\", \"White non-hispanic proportion of responses for police sentiment\", \"Black non-hispanic proportion of responses for police sentiment\", \"Hispanic proportion of responses for police sentiment survey\", \"Other/multiracial/Non-hispanic proportion of responses for police sentiment survey\", \"Proportion of responders who had contact with law enforcement\", \"Proportion of stop and frisks for non-traffic stops\", \"Proportion of stop and frisks for traffic stops\", \"Proportion of vehicle searchs\", \"Proportions of contacts that ended in arrest\", \"Proportion of contacts that included cuffing\", \"Proportion of contacts deemed handled proper\", \"Proportion of contacts deemed unproperly handled\", \"Average number of contacts in the last year\", \"Total region population from US census\", \"Number of Firearm homicides\", \"Number of non-firearm homicides\", \"number of homidides\", \"firearm homicides per 100,000\", \"non-firearm homicides per 100,000\", \"homicides per 100,000\")\n)\n\nprint(variable_table, n = Inf)\n\n# A tibble: 23 × 2\n   Variable_Name       Variable_Description                                     \n   &lt;chr&gt;               &lt;chr&gt;                                                    \n 1 region              Region the data is from                                  \n 2 year                Year the data is from                                    \n 3 total_count         Total number of responses in the police sentiment dataset\n 4 WHITE_NH_PROP       White non-hispanic proportion of responses for police se…\n 5 B_NH_PROP           Black non-hispanic proportion of responses for police se…\n 6 HISPANIC_PROP       Hispanic proportion of responses for police sentiment su…\n 7 OTHER_MULTI_NH_PROP Other/multiracial/Non-hispanic proportion of responses f…\n 8 CONTACT_FREQ        Proportion of responders who had contact with law enforc…\n 9 OC_FRISK_PROP       Proportion of stop and frisks for non-traffic stops      \n10 TC_FRISK_PROP       Proportion of stop and frisks for traffic stops          \n11 VSRCH_PROP          Proportion of vehicle searchs                            \n12 ARREST_PROP         Proportions of contacts that ended in arrest             \n13 CUFFED_PROP         Proportion of contacts that included cuffing             \n14 PROPER_PROP         Proportion of contacts deemed handled proper             \n15 IMPROPER_PROP       Proportion of contacts deemed unproperly handled         \n16 AVG_CONT            Average number of contacts in the last year              \n17 population          Total region population from US census                   \n18 fa_homicides        Number of Firearm homicides                              \n19 nfa_homicides       Number of non-firearm homicides                          \n20 homicides           number of homidides                                      \n21 fa_homicide_rate    firearm homicides per 100,000                            \n22 nfa_homicide_rate   non-firearm homicides per 100,000                        \n23 homicide_rate       homicides per 100,000                                    \n\n\nThe first step of doing so was to find the information for each region x year combination that we wanted to keep from the first dataset. This corresponds to the variables ranging from 3-16 in the table above.\nFor these variables there are a few main categories:\ntotal_count - a metric taken directly from the original data racial demographics - which were taken by finding the proportion of certain races against the entire population event proportions - which were taken by finding the average number of occurrences across the total number of police contacts\nNext we had to transform the firearm dataset to follow the same region x year format. Since our firearm dataset came broken down by states we would have to use these states to reconstruct the original corresponding regions. To do this designation we used the Census Designated Regions of the United States. This is a potential point of error in our analysis as we could not find certain information about the regional breakdown in the first dataset. Then we simply grouped by region x year and recalculated the simple metrics in the original dataset but across the new breakdown.\nFinally we used full_join to combine our two transformed data sets to create a new combined dataset that shared the same grouping breakdowns."
  },
  {
    "objectID": "posts/2023-10-13-first-team-meeting/first-team-meeting.html",
    "href": "posts/2023-10-13-first-team-meeting/first-team-meeting.html",
    "title": "First Team Meeting",
    "section": "",
    "text": "These are the steps that you will take today to get started on your project. Today, you will just be brainstorming, and then next week, you’ll get started on the main aspects of the project.\n\nStart by introducing yourselves to each other. I also recommend creating a private channel on Microsoft Teams with all your team members. This will be a place that you can communicate and share ideas, code, problems, etc.\nDiscuss what aspects of the project each of you are more or less excited about. These include\n\nCollecting, cleaning, and munging data ,\nStatistical Modeling,\nVisualization,\nWriting about analyses, and\nManaging and reviewing team work.\n\nBased on this, discuss where you feel your strengths and weaknesses might be.\nNext, start brainstorming questions you hope to answer as part of this project. This question should in some way be addressing issues around racial disparities. The questions you come up with should be at the level of the question we started with when exploring the HMDA data. (“Are there differences in the ease of securing a loan based on the race of the applicant?”) You’ll revise your questions a lot over the course of the project. Come up with a few questions that your group might be interested in exploring.\nBased on these questions, start looking around for data that might help you analyze this. If you are looking at U.S. based data, data.gov is a good source and if you are looking internationally, I recommend checking out the World Bank. Also, try Googling for data. Include “data set” or “dataset” in your query. You might even include “CSV” or some other format. Using “data” by itself in your query often doesn’t work too well. Spend some time searching for data and try to come up with at least three possible data sets. (For your first blog post, you’ll write short proposals about each of them that I’ll give feedback on.)\nCome up with a team name. Next week, I’ll provide the Github Classroom assignment that will be where you work on your final project and you’ll have to have your team name finalized by then. Your project will be hosted online at the website with a URL like sussmanbu.github.io/ma4615-fa23-final-project-TEAMNAME.\n\nNext time, you’ll get your final project website set up and write your first blog post."
  },
  {
    "objectID": "posts/2023-12-20-examples/examples.html",
    "href": "posts/2023-12-20-examples/examples.html",
    "title": "Examples",
    "section": "",
    "text": "Here are some examples of changing the size of a figure.\n\nplot(1:10)\n\n\n\n\n\n\n\n\n\nplot(1:10)\n\n\n\n\n\n\n\n\nWe can also specify column: screen and out-width: 100% so that the figure will fill the screen. plot in the svg vector graphics file format.\n\nlibrary(ggplot2)\nggplot(pressure, aes(x = temperature, y = pressure)) + geom_point()"
  },
  {
    "objectID": "posts/2023-12-20-examples/examples.html#figure-sizing",
    "href": "posts/2023-12-20-examples/examples.html#figure-sizing",
    "title": "Examples",
    "section": "",
    "text": "Here are some examples of changing the size of a figure.\n\nplot(1:10)\n\n\n\n\n\n\n\n\n\nplot(1:10)\n\n\n\n\n\n\n\n\nWe can also specify column: screen and out-width: 100% so that the figure will fill the screen. plot in the svg vector graphics file format.\n\nlibrary(ggplot2)\nggplot(pressure, aes(x = temperature, y = pressure)) + geom_point()"
  },
  {
    "objectID": "posts/2024-10-15-Data-Background/Data Background.html",
    "href": "posts/2024-10-15-Data-Background/Data Background.html",
    "title": "Data Background",
    "section": "",
    "text": "Dataset Name: The Effect of Prior Police Contact on Victimization Reporting: Results From the Police-Public Contact and National Crime Victimization Surveys, United States, 2002-2011 (ICPSR 36370) Dataset Link\nThe data in this dataset is compiled from the National Crime Victimization Survey (NCVS) and the Police-Public Contact Survey (PPCS). Both are collected by the Bureau of Justice Statistics (BJS), which is the primary statistical agency of the US’s Department of Justice. It was first established in 1979 in order to better understand statistics about crimes in the United States. The data collected in these studies provides information on how each individuals’ prior interactions with law enforcement, both directly and through the experiences of others, influence their decisions to report crimes. By linking data from the National Crime Victimization Survey (NCVS) with the Police-Public Contact Survey (PPCS), the researchers could explore the relationship between police encounter experiences and crime reporting behavior. The combined dataset used in this study contains data about survey participants in 2002, 2008, and 2011.\nVariables common to the NCVS and PPCS were concatenated to create unique identifiers which could link the two datasets, but there was variation across different years in how much of the data that could be linked. For example, due to a switch in study design, only about 15% of participants who took the both surveys in 2011 could be included in the combined dataset. Another issue arises from the unequal weight of data of people within differing segments of the population. Because not all PPCS interviews could be matched, population estimates are not necessarily representative of the U.S. population, even though both of the original studies were designed to be representative. Additionally, since this dataset relies on responses to both of these surveys, it is possible that the sample is biased to include more individuals who experienced victimization at the hand of law enforcement, as they may be more likely to respond to the NCVS versus those who may not have.\nThe dataset provides detailed crime data in the U.S. and is used to inform policy-making decisions. Law enforcement agencies, policymakers, and researchers. These groups use this data to identify crime trends, allocate resources, and develop crime prevention strategies.\nBJS Website"
  },
  {
    "objectID": "posts/2024-11-06-more-eda/more-eda.html",
    "href": "posts/2024-11-06-more-eda/more-eda.html",
    "title": "more EDA",
    "section": "",
    "text": "This week, we continued exploring the dataset with a focus on relationships among different variables such as reasons for traffic stops, arrests, and the types of police contacts experienced under different demographical groups.\nFor example, below is a plot being generated. We studied the disparities in experiences of improper police contact among different racial groups. We made a bar plot to illustrate the proportion within each racial group who reported experiencing improper police contact and the result reveals notable differences across racial groups.This insight lays the groundwork for deeper analysis to understand what factors contribute to these differences, potentially using demographic and situational variables in further modeling.\n\nsuppressPackageStartupMessages(library(tidyverse))\nsuppressPackageStartupMessages(library(tidymodels))\ndata &lt;- readRDS(\"dataset/police_interaction.rds\") \ndata |&gt;\n  filter(!is.na(PROPER)) |&gt;\n  group_by(RACE) |&gt;\n  summarize(\n    improper_proportion = sum(PROPER == 0) / n()\n  ) |&gt;\n  ggplot(aes(x = RACE, y = improper_proportion, fill = RACE)) +\n  geom_bar(stat = \"identity\")\n\n\n\n\n\n\n\n\nThis is the second plot that we would like to include here: We made a heatmap that visualizes the correlations between various continuous variables in the dataset, where the color intensity represents the strength and direction of the correlation (with red indicating positive correlations and blue indicating negative correlations). There are notable positive correlations such as HHPOV and WORK_LW and notable negative correlations such as WORK_LW and any high-income indicator. This heatmap helps identify groups of continuous variables that may be related or unrelated, which can be useful in modeling and to understand relationships among the data.\n\nlibrary(tidyverse)\nlibrary(reshape2)\n\n\nAttaching package: 'reshape2'\n\n\nThe following object is masked from 'package:tidyr':\n\n    smiths\n\n# Categorizing variables based on their types\n#removed \"CONTACT\" and \"ERROR\" due to \"Warning in cor(numeric_data, use = \"complete.obs\") : the standard deviation is zero\" error\nbinary_columns &lt;- c(\"MALE\", \"WORK_LW\", \"HHPOV\", \"PUB_HOUSE\",   \"PUB_HOUSE_SUB\", \"REGION\", \"INPERSON\", \"VICAR_CITIZEN\", \"VICAR_PRO_AUTO\", \"VICAR_PRO_PERS\", \"VICAR_OTH_CONT\", \"VICAR_IMPROPER\", \"D_HH_P23\", \"PROPER\")\n\nordinal_columns &lt;- c(\"C4_RACE\", \"MAR_STAT\", \"FREQ_DRV\", \"TENURE\",  \"MSA_STATUS\")\n\ncontinuous_columns &lt;- c(\"AGE\", \"EDUCATION\", \"EDUCATION_SUB\", \"NUM_MOVES\", \"NUM_CONT\", \"HH_SIZE\", \"PPCS_YEAR\", \"N_HH_P1\", \"N_PERS_P1\", \"NUM_CITIZEN_HH\", \"NUM_PRO_AUTO_HH\", \"NUM_PRO_PERS_HH\", \"NUM_OTH_CONT_HH\", \"NUM_IMPROPER_HH\")\n\nfiltered &lt;- data\n# Selecting data by type\nbinary_data &lt;- filtered %&gt;%\n  select(all_of(binary_columns)) %&gt;%\n  select(where(is.numeric))\n\nordinal_data &lt;- filtered %&gt;%\n  select(all_of(ordinal_columns)) %&gt;%\n  select(where(is.numeric))\n\ncontinuous_data &lt;- filtered %&gt;%\n  select(all_of(continuous_columns)) %&gt;%\n  select(where(is.numeric))\n\n# Calculating correlation matrices with appropriate methods\ncor_binary &lt;- cor(binary_data, use = \"pairwise.complete.obs\", method = \"pearson\")\ncor_ordinal &lt;- cor(ordinal_data, use = \"pairwise.complete.obs\", method = \"spearman\")\ncor_continuous &lt;- cor(continuous_data, use = \"pairwise.complete.obs\", method = \"pearson\")\n\n# Creating a heatmap function\nplot_heatmap &lt;- function(cor_matrix, title) {\n  cor_melted &lt;- melt(cor_matrix)\n  ggplot(cor_melted, aes(Var1, Var2, fill = value)) +\n    geom_tile(color = \"white\") +\n    scale_fill_gradient2(low = \"blue\", high = \"red\", mid = \"white\",\n                         midpoint = 0, limit = c(-1, 1), name = \"Correlation\") +\n    labs(title = title, x = \"Variables\", y = \"Variables\") +\n    theme_minimal() +\n    theme(axis.text.x = element_text(angle = 90, hjust = 1),\n          axis.text.y = element_text(angle = 0, vjust = 1))\n}\n\n# Plotting heatmaps for each correlation matrix\nif (ncol(continuous_data) &gt; 1) {\n  print(plot_heatmap(cor_continuous, \"Correlation Heatmap of Continuous Variables\"))\n}\n\n\n\n\n\n\n\n\nFor modeling, we used a linear model to try to predict whether someone would be arrested based on their race, years of education, if their household is living in poverty, and their gender. We separated the data into a train and test set with an 80% split. In the training set, the F-statistic is far over 1 and the p-values show over 95% confidence in all the response variables being significant.\n\nsplit &lt;- initial_split(filtered, prop = .8) #good way to keep yourself honest. splits it by prop % being in training, 1-prop being test\ntraining&lt;- training(split)\ntesting &lt;- testing(split)\npredict_filtered &lt;- training |&gt;\n  filter(!is.na(ARRESTED) & !is.na(RACE) & !is.na(EDUCATION) & !is.na(HHPOV) & !is.na(MALE))\n\nmod1 &lt;- lm(ARRESTED ~ RACE + EDUCATION + HHPOV + MALE, predict_filtered)\n\nIn the test set, the F-statistic is much lower and the p-values and t-stats rise for each variable, with the p-value for each race variable but the intercept being over 0.1. This finding implies that there isn’t a significant relationship on average between these race categories and being arrested, although, for “Other or multiracial, Non-Hispanic”, that could be skewed by the much smaller sample size of the category compared to the other races in the category.\n\npredict_filtered_test &lt;- testing |&gt;\n  filter(!is.na(ARRESTED) & !is.na(RACE) & !is.na(EDUCATION) & !is.na(HHPOV) & !is.na(MALE))\n\nmod2 &lt;- lm(ARRESTED ~ RACE + EDUCATION + HHPOV + MALE, predict_filtered_test)\nplot(mod2)"
  },
  {
    "objectID": "posts/2024-11-15-EDA-On-Other-Dataset/More EDA.html",
    "href": "posts/2024-11-15-EDA-On-Other-Dataset/More EDA.html",
    "title": "more EDA on Other Datasets",
    "section": "",
    "text": "suppressPackageStartupMessages(library(tidyverse))\nsuppressPackageStartupMessages(library(sf))\n\nWarning: package 'sf' was built under R version 4.4.2\n\nsuppressPackageStartupMessages(library(dplyr))\nsuppressPackageStartupMessages(library(viridis))\n\nWarning: package 'viridis' was built under R version 4.4.2\n\nsuppressPackageStartupMessages(library(maps))\n\nThis week, we picked one dataset from the two other dataset we found last week. We’ve picked the firearm suicide dataset. This dataset describes the number of homicides, the number of suicides, and the number of suicides done with firearms for whites & nonwhites within different states from 1949 to 2020. We picked this dataset since it is easier to combine by locations.\nDataset:Firearm Suicide Proxy for Household Gun Ownership\nFirst of all, we did some EDA on the second dataset. This map represents the rate of homicides by firearm by state. We were having some issues working with the mapping functions, which we have to fix. As the map borders do not currently line up with the data coordinates which is causing the coloring to be misaligned with the borders, but we are still working through solving this issue.\n\nfirearm_data &lt;- read_rds(\"dataset/firearm_sh_ds.rds\")\n\n\nfirearm_data &lt;- firearm_data |&gt;\n  mutate(firearm_homicide_rate = as.numeric(firearm_homicide_rate),\n         state = tolower(state)) |&gt;\n  filter(!is.na(firearm_homicide_rate))\n\nus_states &lt;- map_data(\"state\") |&gt;\n  as_tibble() |&gt;\n  st_as_sf(coords = c(\"long\", \"lat\"), crs = 4326) |&gt;\n  group_by(region) |&gt;\n  summarise(geometry = st_union(geometry)) |&gt;\n  st_transform(crs = 5070)\n\nus_states &lt;- us_states |&gt;\n  st_cast(\"POLYGON\")\n\nmap_data &lt;- us_states |&gt;\n  rename(state = region) |&gt;\n  left_join(firearm_data, by = \"state\")\n\nmap_data$log_firearm_homicide_rate &lt;- log(map_data$firearm_homicide_rate + 1)\n\nggplot(map_data) +\n  geom_sf(aes(fill = log_firearm_homicide_rate), color = \"black\", size = 0.2) +\n  scale_fill_viridis_c(option = \"plasma\", name = \"Homicide Rate\") +\n  theme_minimal() +\n  labs(\n    title = \"Firearm Homicide Rate by State\",\n    fill = \"Rate\"\n  ) +\n  facet_wrap(~year)\n\n\n\n\n\n\n\n\nThen, we started to combine the two datasets. We decided to combine based on locations. The first dataset was divided into four regions: Northeast, Midwest, South, and West while the second dataset was divided into different states. We planned to categorize the the states in the second dataset into regions just like the first dataset.\nIn combining the dataset we decided to do so by grouping data from both datasets by region and year. Our original dataset used regions instead of states, so by converting from states to region we used the regional definitions used by the Census Bureau Designated Regions. For each region and year we summarized the data into categories that include proportional data for certain types of incidents related to police contact, racial distributions, homicide rate, etc.\nThe combination of the data is done using the code in “/scripts/load_and_clean_data.R”. Below is a table, which displays each new column and its description. Followed by the new combined data itself.\n\nvariable_table &lt;- tibble(\n  Variable_Name = c(\"region\", \"year\", \"total_count\", \"WHITE_NH_PROP\", \"B_NH_PROP\", \"HISPANIC_PROP\", \"OTHER_MULTI_NH_PROP\", \"CONTACT_FREQ\", \"OC_FRISK_PROP\", \"TC_FRISK_PROP\", \"VSRCH_PROP\", \"ARREST_PROP\", \"CUFFED_PROP\", \"PROPER_PROP\", \"IMPROPER_PROP\", \"AVG_CONT\", \"population\", \"fa_homicides\", \"nfa_homicides\", \"homicides\", \"fa_homicide_rate\", \"nfa_homicide_rate\", \"homicide_rate\"),\n  Variable_Description = c(\"Region the data is from\", \"Year the data is from\", \"Total number of responses in the police sentiment dataset\", \"White non-hispanic proportion of responses for police sentiment\", \"Black non-hispanic proportion of responses for police sentiment\", \"Hispanic proportion of responses for police sentiment survey\", \"Other/multiracial/Non-hispanic proportion of responses for police sentiment survey\", \"Proportion of responders who had contact with law enforcement\", \"Proportion of stop and frisks for non-traffic stops\", \"Proportion of stop and frisks for traffic stops\", \"Proportion of vehicle searchs\", \"Proportions of contacts that ended in arrest\", \"Proportion of contacts that included cuffing\", \"Proportion of contacts deemed handled proper\", \"Proportion of contacts deemed unproperly handled\", \"Average number of contacts in the last year\", \"Total region population from US census\", \"Number of Firearm homicides\", \"Number of non-firearm homicides\", \"number of homidides\", \"firearm homicides per 100,000\", \"non-firearm homicides per 100,000\", \"homicides per 100,000\")\n)\n\nprint(variable_table, n = Inf)\n\n# A tibble: 23 × 2\n   Variable_Name       Variable_Description                                     \n   &lt;chr&gt;               &lt;chr&gt;                                                    \n 1 region              Region the data is from                                  \n 2 year                Year the data is from                                    \n 3 total_count         Total number of responses in the police sentiment dataset\n 4 WHITE_NH_PROP       White non-hispanic proportion of responses for police se…\n 5 B_NH_PROP           Black non-hispanic proportion of responses for police se…\n 6 HISPANIC_PROP       Hispanic proportion of responses for police sentiment su…\n 7 OTHER_MULTI_NH_PROP Other/multiracial/Non-hispanic proportion of responses f…\n 8 CONTACT_FREQ        Proportion of responders who had contact with law enforc…\n 9 OC_FRISK_PROP       Proportion of stop and frisks for non-traffic stops      \n10 TC_FRISK_PROP       Proportion of stop and frisks for traffic stops          \n11 VSRCH_PROP          Proportion of vehicle searchs                            \n12 ARREST_PROP         Proportions of contacts that ended in arrest             \n13 CUFFED_PROP         Proportion of contacts that included cuffing             \n14 PROPER_PROP         Proportion of contacts deemed handled proper             \n15 IMPROPER_PROP       Proportion of contacts deemed unproperly handled         \n16 AVG_CONT            Average number of contacts in the last year              \n17 population          Total region population from US census                   \n18 fa_homicides        Number of Firearm homicides                              \n19 nfa_homicides       Number of non-firearm homicides                          \n20 homicides           number of homidides                                      \n21 fa_homicide_rate    firearm homicides per 100,000                            \n22 nfa_homicide_rate   non-firearm homicides per 100,000                        \n23 homicide_rate       homicides per 100,000"
  },
  {
    "objectID": "scripts/experimenting_plots.html",
    "href": "scripts/experimenting_plots.html",
    "title": "MA [46]15 Final Project - Group 7",
    "section": "",
    "text": "suppressPackageStartupMessages(library(tidyverse))\nsuppressPackageStartupMessages(library(sf))\n\nWarning: package 'sf' was built under R version 4.4.2\n\nsuppressPackageStartupMessages(library(dplyr))\nsuppressPackageStartupMessages(library(viridis))\n\nWarning: package 'viridis' was built under R version 4.4.2\n\nsuppressPackageStartupMessages(library(maps))\n\n\nfiltered &lt;- read_rds(\"dataset/police_interaction.rds\")\n\n\ncreate_barplot &lt;- function(data, col_name) {\n  ggplot(data, aes_string(x = col_name)) +\n    geom_bar(fill = \"skyblue\", color = \"black\") +\n    labs(title = paste(\"Barplot of\", col_name),\n         x = col_name,\n         y = \"Count\") +\n    theme_minimal() +\n    theme(axis.text.x = element_text(angle = 45, hjust = 1))\n}\nreason_key = c(\"Out of universe/missing\", \"Accident\", \"Traffic stop-driver\", \"Traffic stop-passenger\", \"Rsp reported crime\", \"Police assistance\", \"Police investigation\", \"Police suspected rsp\", \"Other reason\")\nfiltered |&gt;\n  group_by(RACE) |&gt;\n  mutate(\n    reason_text = reason_key[REASON + 1] \n  ) |&gt;\n  ggplot(aes(x = REASON, fill = reason_text)) +\n    geom_bar(show.legend = TRUE) +\n    theme_minimal() +\n  facet_wrap(~RACE) \n\n\n\n\n\n\n\nreason_key = c(\"Accident\", \"Traffic stop-driver\", \"Traffic stop-passenger\", \"Rsp reported crime\", \"Police assistance\", \"Police investigation\", \"Police suspected rsp\", \"Other reason\")\n# filtered_reason &lt;- filtered %&gt;%\n#  count(RACE , wt = reason_key, name = \"ny\")\n# filtered |&gt;\n#   group_by(RACE) |&gt;\n#   filter(REASON &gt; 0) |&gt;\n#   mutate(\n#     reason_text = reason_key[REASON + 1] \n#   ) |&gt;\n#   ggplot(aes(x = REASON, fill = reason_text)) +\n#     geom_bar(show.legend = TRUE) +\n#     theme_minimal() +\n#   facet_wrap(~RACE) \nfiltered |&gt;\n  filter(REASON &gt; 0) |&gt;\nggplot(aes(x = as.factor(REASON), fill = as.factor(RACE)))+\n  geom_bar(position = \"fill\") + \n  labs(title = \"Proportion of Reasons for Traffic Stop by Race\", x = \"Reason: \\n1 = Accident\\n2 = Traffic stop-driver\\n3 = Traffic stop-passenger\\n4 = Rsp reported crime\\n5 = Police assistance\\n6 = Police investigation\\n7 = Police suspected rsp\\n8 = Other reason\", y = \"Proportion\", fill = \"Race\") \n\n\n\n\n\n\n\n  #scale_fill_discrete(name = \"Race\")\n\n # ggplot(aes(x = REASON, y = after_stat(prop), fill = reason_text, group = reason_text)) +\n #    geom_bar(show.legend = TRUE, color = \"black\", position = \"dodge\") +\n #    theme_minimal() +\n#  theme(legend = element_text(\"Reason for traffic stop\"))\n\n\n  #theme(legend.title = element_text(hjust = 1))\n\nfiltered |&gt;\n  filter(REASON &gt; 0) |&gt;\n  ggplot(aes(x = RACE, fill = as.factor(REASON))) +\n  geom_bar(position = \"fill\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n  labs(title = \"Proportion of Reasons for Traffic Stop by Race\")\n\n\n\n\n\n\n\n\n\nfiltered |&gt;\n  filter(!is.na(ARRESTED)) |&gt;\n  ggplot(aes(x = as.factor(RACE), fill = as.factor(ARRESTED))) +\n    geom_bar(position = \"fill\") +\n    labs(title = \"Proportion of Arrests by Race\", x = \"Race\", y = \"Arrested (0 = No, 1 = Yes)\", fill = \"Arrested\")\n\n\n\n\n\n\n\nfiltered |&gt;\n  filter(!is.na(ARRESTED)) |&gt;\n  group_by(RACE) |&gt;\n  summarize(\n    arrest_proportion = sum(ARRESTED = 1) / n()\n  ) |&gt;\n  ggplot(aes(x = RACE, y = arrest_proportion, fill = RACE)) +\n  geom_bar(stat = \"identity\") + \n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\nggplot(filtered, aes(x = as.factor(RACE), fill = as.factor(CONTACT))) +\n  geom_bar(position = \"fill\") +\n  labs(title = \"Proportion of Race by Police Contact\", x = \"Race\", y = \"Police Contact\", fill = \"Police Contact\") +\n  scale_x_discrete(labels = c(\"0\" = \"No\", \"1\" = \"Yes\")) +\n  scale_y_continuous(labels = scales::percent)\n\n\n\n\n\n\n\n\n\nggplot(filtered, aes(x = as.factor(C_CONTCT), fill = as.factor(RACE))) +\n  geom_bar(position = \"fill\") +\n  labs(title = \"Proportion of Contact Type by Race\", x = \"Contact Type\", y = \"Proportion\", fill = \"Race\") +\n  scale_x_discrete(labels = c(\"0\" = \"No contact\", \n                              \"1\" = \"Citizen-initiated contact\", \n                              \"2\" = \"Vehicle stop\", \n                              \"3\" = \"Suspect-related contact\", \n                              \"4\" = \"Other police contact\")) +\n  scale_y_continuous(labels = scales::percent)\n\n\n\n\n\n\n\n\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.2.0 ──\n\n\n✔ broom        1.0.6     ✔ rsample      1.2.1\n✔ dials        1.3.0     ✔ tune         1.2.1\n✔ infer        1.0.7     ✔ workflows    1.1.4\n✔ modeldata    1.4.0     ✔ workflowsets 1.1.0\n✔ parsnip      1.2.1     ✔ yardstick    1.3.1\n✔ recipes      1.1.0     \n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ maps::map()       masks purrr::map()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n• Learn how to get started at https://www.tidymodels.org/start/\n\nsplit &lt;- initial_split(filtered, prop = .8) #good way to keep yourself honest. splits it by prop % being in training, 1-prop being test\ntraining&lt;- training(split)\ntesting &lt;- testing(split)\npredict_filtered &lt;- training |&gt;\n  filter(!is.na(ARRESTED) & !is.na(RACE) & !is.na(EDUCATION) & !is.na(HHPOV) & !is.na(MALE))\n\nmod1 &lt;- lm(ARRESTED ~ RACE + EDUCATION + HHPOV + MALE, predict_filtered)\nsummary(mod1)\n\n\nCall:\nlm(formula = ARRESTED ~ RACE + EDUCATION + HHPOV + MALE, data = predict_filtered)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.14067 -0.04080 -0.02443 -0.00681  1.00245 \n\nCoefficients:\n                                         Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)                             0.0909397  0.0091097   9.983  &lt; 2e-16\nRACEHispanic                           -0.0262168  0.0064783  -4.047 5.22e-05\nRACEOther or multiracial, Non-Hispanic -0.0279222  0.0084310  -3.312 0.000929\nRACEWhite, Non-Hispanic                -0.0268455  0.0050082  -5.360 8.45e-08\nEDUCATION                              -0.0040915  0.0005583  -7.329 2.46e-13\nHHPOV                                   0.0239242  0.0046157   5.183 2.21e-07\nMALE                                    0.0258033  0.0027945   9.234  &lt; 2e-16\n                                          \n(Intercept)                            ***\nRACEHispanic                           ***\nRACEOther or multiracial, Non-Hispanic ***\nRACEWhite, Non-Hispanic                ***\nEDUCATION                              ***\nHHPOV                                  ***\nMALE                                   ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1558 on 12541 degrees of freedom\nMultiple R-squared:  0.01772,   Adjusted R-squared:  0.01725 \nF-statistic:  37.7 on 6 and 12541 DF,  p-value: &lt; 2.2e-16\n\nplot(mod1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npredict_filtered_test &lt;- testing |&gt;\n  filter(!is.na(ARRESTED) & !is.na(RACE) & !is.na(EDUCATION) & !is.na(HHPOV) & !is.na(MALE))\n\nmod2 &lt;- lm(ARRESTED ~ RACE + EDUCATION + HHPOV + MALE, predict_filtered_test)\nsummary(mod2)\n\n\nCall:\nlm(formula = ARRESTED ~ RACE + EDUCATION + HHPOV + MALE, data = predict_filtered_test)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.12951 -0.04320 -0.02262 -0.00841  1.01822 \n\nCoefficients:\n                                        Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)                             0.122271   0.018750   6.521 8.09e-11\nRACEHispanic                           -0.021053   0.013259  -1.588   0.1124\nRACEOther or multiracial, Non-Hispanic -0.028629   0.017078  -1.676   0.0938\nRACEWhite, Non-Hispanic                -0.017992   0.010013  -1.797   0.0725\nEDUCATION                              -0.006805   0.001176  -5.788 7.81e-09\nHHPOV                                   0.022000   0.008992   2.447   0.0145\nMALE                                    0.023650   0.005523   4.282 1.90e-05\n                                          \n(Intercept)                            ***\nRACEHispanic                              \nRACEOther or multiracial, Non-Hispanic .  \nRACEWhite, Non-Hispanic                .  \nEDUCATION                              ***\nHHPOV                                  *  \nMALE                                   ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1547 on 3147 degrees of freedom\nMultiple R-squared:  0.02266,   Adjusted R-squared:  0.0208 \nF-statistic: 12.16 on 6 and 3147 DF,  p-value: 1.453e-13\n\nplot(mod2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# predict_filtered &lt;- filtered |&gt;\n#   filter(!is.na(ARRESTED) & !is.na(HHPOV) & !is.na(MALE))\n# \n# mod2 &lt;- lm(ARRESTED ~ HHPOV + MALE, predict_filtered)\n# summary(mod2)\n# plot(mod2)\n# predict_filtered &lt;- filtered |&gt;\n#   filter(!is.na(ARRESTED) & !is.na(RACE))\n# \n# mod2 &lt;- lm(ARRESTED ~ RACE, predict_filtered)\n# summary(mod2)\n# plot(mod2)\n# filtered |&gt;\n#   # ggplot(aes(x = C4_RACE, y = PROPER)) +\n#   # geom_quantile()\n#   \n#   ggplot(aes(x = C4_RACE, y = PROPER)) +\n#   geom_\n\n\nbroom::augment(mod1)\n\n# A tibble: 12,548 × 11\n   ARRESTED RACE   EDUCATION HHPOV  MALE .fitted   .resid    .hat .sigma .cooksd\n      &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n 1        0 Hispa…        14     0     0 0.00744 -0.00744 9.53e-4  0.156 3.11e-7\n 2        0 White…        14     0     1 0.0326  -0.0326  1.87e-4  0.156 1.17e-6\n 3        0 Black…        12     0     1 0.0676  -0.0676  1.08e-3  0.156 2.91e-5\n 4        0 White…        14     0     0 0.00681 -0.00681 1.93e-4  0.156 5.26e-8\n 5        0 White…        10     0     1 0.0490  -0.0490  3.97e-4  0.156 5.61e-6\n 6        0 Hispa…        14     0     0 0.00744 -0.00744 9.53e-4  0.156 3.11e-7\n 7        0 White…        16     1     0 0.0226  -0.0226  1.00e-3  0.156 3.01e-6\n 8        0 Hispa…        12     0     1 0.0414  -0.0414  9.17e-4  0.156 9.28e-6\n 9        0 White…        14     0     1 0.0326  -0.0326  1.87e-4  0.156 1.17e-6\n10        0 White…        16     0     1 0.0244  -0.0244  2.36e-4  0.156 8.29e-7\n# ℹ 12,538 more rows\n# ℹ 1 more variable: .std.resid &lt;dbl&gt;\n\nggplot(mod1, aes(x = .fitted, y=.resid)) +\n  geom_point(alpha = 0.3) +\n  geom_smooth()\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\n\n\n\nggplot(mod1, aes(sample = rstandard(mod1))) +\n  geom_qq() + \n  stat_qq_line()\n\n\n\n\n\n\n\n\n\nlibrary(tidyverse)\nlibrary(reshape2)\n\n\nAttaching package: 'reshape2'\n\n\nThe following object is masked from 'package:tidyr':\n\n    smiths\n\n# Categorizing variables based on their types\n#removed \"CONTACT\" and \"ERROR\" due to \"Warning in cor(numeric_data, use = \"complete.obs\") : the standard deviation is zero\" error\nbinary_columns &lt;- c(\"MALE\", \"WORK_LW\", \"HHPOV\", \"PUB_HOUSE\",   \"PUB_HOUSE_SUB\", \"REGION\", \"INPERSON\", \"VICAR_CITIZEN\", \"VICAR_PRO_AUTO\", \"VICAR_PRO_PERS\", \"VICAR_OTH_CONT\", \"VICAR_IMPROPER\", \"D_HH_P23\", \"PROPER\")\n\nordinal_columns &lt;- c(\"C4_RACE\", \"MAR_STAT\", \"FREQ_DRV\", \"TENURE\",  \"MSA_STATUS\")\n\ncontinuous_columns &lt;- c(\"AGE\", \"EDUCATION\", \"EDUCATION_SUB\", \"NUM_MOVES\", \"NUM_CONT\", \"HH_SIZE\", \"PPCS_YEAR\", \"N_HH_P1\", \"N_PERS_P1\", \"NUM_CITIZEN_HH\", \"NUM_PRO_AUTO_HH\", \"NUM_PRO_PERS_HH\", \"NUM_OTH_CONT_HH\", \"NUM_IMPROPER_HH\")\n\n# Selecting data by type\nbinary_data &lt;- filtered %&gt;%\n  select(all_of(binary_columns)) %&gt;%\n  select(where(is.numeric))\n\nordinal_data &lt;- filtered %&gt;%\n  select(all_of(ordinal_columns)) %&gt;%\n  select(where(is.numeric))\n\ncontinuous_data &lt;- filtered %&gt;%\n  select(all_of(continuous_columns)) %&gt;%\n  select(where(is.numeric))\n\n# Calculating correlation matrices with appropriate methods\ncor_binary &lt;- cor(binary_data, use = \"pairwise.complete.obs\", method = \"pearson\")\ncor_ordinal &lt;- cor(ordinal_data, use = \"pairwise.complete.obs\", method = \"spearman\")\ncor_continuous &lt;- cor(continuous_data, use = \"pairwise.complete.obs\", method = \"pearson\")\n\n# Creating a heatmap function\nplot_heatmap &lt;- function(cor_matrix, title) {\n  cor_melted &lt;- melt(cor_matrix)\n  ggplot(cor_melted, aes(Var1, Var2, fill = value)) +\n    geom_tile(color = \"white\") +\n    scale_fill_gradient2(low = \"blue\", high = \"red\", mid = \"white\",\n                         midpoint = 0, limit = c(-1, 1), name = \"Correlation\") +\n    labs(title = title, x = \"Variables\", y = \"Variables\") +\n    theme_minimal() +\n    theme(axis.text.x = element_text(angle = 90, hjust = 1),\n          axis.text.y = element_text(angle = 0, vjust = 1))\n}\n\n# Plotting heatmaps for each correlation matrix\nif (ncol(binary_data) &gt; 1) {\n  print(plot_heatmap(cor_binary, \"Correlation Heatmap of Binary Variables\"))\n}\n\n\n\n\n\n\n\nif (ncol(ordinal_data) &gt; 1) {\n  print(plot_heatmap(cor_ordinal, \"Correlation Heatmap of Ordinal Variables\"))\n}\n\n\n\n\n\n\n\nif (ncol(continuous_data) &gt; 1) {\n  print(plot_heatmap(cor_continuous, \"Correlation Heatmap of Continuous Variables\"))\n}\n\n\n\n\n\n\n\n\nPlot of the marriage data for different races\n\nmarriageData &lt;- filtered |&gt;\n  filter(!is.na(MAR_STAT)) |&gt;\n  group_by(RACE) |&gt;\n  mutate(totalPerRace = n()) |&gt;\n  ungroup() |&gt;\n  group_by(RACE, MAR_STAT) |&gt;\n  select(RACE, MAR_STAT, totalPerRace) |&gt;\n  mutate(numPerMarStatRace = n()) |&gt;\n  filter(row_number()==1) |&gt;\n  mutate(prop = numPerMarStatRace/totalPerRace) |&gt;\n  select(-totalPerRace, -numPerMarStatRace) |&gt;\n  mutate(MAR_STAT = as.factor(MAR_STAT))\n\nggplot(data = marriageData) +\n  geom_bar(mapping = aes(x = RACE, fill = MAR_STAT, y = prop),\n           stat = \"identity\",\n           position = \"dodge\")\n\n\n\n\n\n\n\n\n\nfirearm_data &lt;- read_rds(\"dataset/firearm_sh_ds.rds\")\n\nWorking on basic EDA for Second Dataset\n\nfirearm_data &lt;- firearm_data |&gt;\n  mutate(firearm_homicide_rate = as.numeric(firearm_homicide_rate),\n         state = tolower(state)) |&gt;\n  filter(!is.na(firearm_homicide_rate))\n\nus_states &lt;- map_data(\"state\") |&gt;\n  as_tibble() |&gt;\n  st_as_sf(coords = c(\"long\", \"lat\"), crs = 4326) |&gt;\n  group_by(region) |&gt;\n  summarise(geometry = st_union(geometry)) |&gt;\n  st_transform(crs = 5070)\n\nus_states &lt;- us_states |&gt;\n  st_cast(\"POLYGON\")\n\nmap_data &lt;- us_states |&gt;\n  rename(state = region) |&gt;\n  left_join(firearm_data, by = \"state\")\n\ncolnames(map_data)\n\n [1] \"state\"                    \"geometry\"                \n [3] \"year\"                     \"total_population\"        \n [5] \"homicide_rate\"            \"firearm_homicide_rate\"   \n [7] \"nonfirearm_homicide_rate\" \"firearm_homicides\"       \n [9] \"nonfirearm_homicides\"     \"total_homicides\"         \n\nmap_data$log_firearm_homicide_rate &lt;- log(map_data$firearm_homicide_rate + 1)\n\nggplot(map_data) +\n  geom_sf(aes(fill = log_firearm_homicide_rate), color = \"black\", size = 0.2) +\n  scale_fill_viridis_c(option = \"plasma\", name = \"Homicide Rate\") +\n  theme_minimal() +\n  labs(\n    title = \"Firearm Homicide Rate by State\",\n    fill = \"Rate\"\n  ) +\n  facet_wrap(~year)\n\n\n\n\n\n\n\n\n\n# Loading the combined data\nnew_data &lt;- read_rds(\"dataset/combined_regional_data.rds\")\n\nview(new_data)\n\n\n# Scatter plot for firearm homicide rate and racial proportions\nggplot(new_data, aes(x = fa_homicide_rate)) +\n  geom_point(aes(y = WHITE_NH_PROP, color = \"White NH\"), size = 2, alpha = 0.7) +\n  geom_point(aes(y = B_NH_PROP, color = \"Black NH\"), size = 2, alpha = 0.7) +\n  geom_point(aes(y = HISPANIC_PROP, color = \"Hispanic\"), size = 2, alpha = 0.7) +\n  geom_point(aes(y = OTHER_MULTI_NH_PROP, color = \"Other/Multi NH\"), size = 2, alpha = 0.7) +\n  facet_wrap(~ region) + # Optional: Facet by region\n  labs(\n    title = \"Firearm Homicide Rate by Racial Proportions\",\n    x = \"Firearm Homicide Rate\",\n    y = \"Proportion of Racial Group\",\n    color = \"Racial Group\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n# Loading libraries\nlibrary(shiny)\n\n\nAttaching package: 'shiny'\n\n\nThe following object is masked from 'package:infer':\n\n    observe\n\nlibrary(shinylive)\nlibrary(ggplot2)\nlibrary(readr) # For reading your .rds file\n\n\n# This chart normalizes racial proportions by multiplying by 100 to plot alongside homicide rates on the same scale, then multiplies firearm homicide rate by 10 to get a better understanding of what the trend looks like.\nggplot(new_data, aes(x = year)) +\n  geom_line(aes(y = fa_homicide_rate * 10, color = \"Firearm Homicide Rate * 10\"), size = 1) +\n  geom_line(aes(y = WHITE_NH_PROP * 100, color = \"White NH Prop\"), linetype = \"dashed\", size = 0.8) +\n  geom_line(aes(y = B_NH_PROP * 100, color = \"Black NH Prop\"), linetype = \"dashed\", size = 0.8) +\n  facet_wrap(~ region) +\n  labs(\n    title = \"Trends in Firearm Homicide Rate and Racial Proportions Over Time\",\n    x = \"Year\",\n    y = \"Rate (%)\",\n    color = \"Variable\"\n  ) +\n  theme_minimal()\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\n\n# Define server logic ----\nserver &lt;- function(input, output) {\n  # Load the data\n  # Replace this URL with the actual URL of your .rds file on GitHub Pages\n   data &lt;- read_rds(here::here(\"dataset\", \"combined_regional_data.rds\"))\n    # read_rds(\"https://sussmanbu.github.io/ma-4615-fa24-final-project-group-7/dataset/combined_regional_data.rds\")\n    # Have to configure to get this to link to the dataset\n  \n    # Render the interactive plot\n  output$scatterPlot &lt;- renderPlot({\n    ggplot(data, aes(x = fa_homicide_rate, y = .data[[input$selected_race]])) +\n      geom_point(aes(color = as.factor(year)), size = 2, alpha = 0.7) + # Use color to differentiate years\n      geom_text(aes(label = year), vjust = -1, size = 3, alpha = 0.8) + # Add year annotations above points\n      facet_wrap(~ region) + # Optional: Facet by region\n      labs(\n        title = paste(\"Firearm Homicide Rate vs\", input$selected_race),\n        x = \"Firearm Homicide Rate\",\n        y = \"Proportion\",\n        color = \"Year\"\n      ) +\n      theme_minimal() +\n      theme(\n        legend.position = \"bottom\",       # Place legend at the bottom\n        legend.box = \"horizontal\",        # Align legend items horizontally\n        legend.text = element_text(size = 10), # Adjust legend text size\n        legend.title = element_text(size = 12), # Adjust legend title size\n        legend.key.width = unit(1, \"cm\"), # Add space between legend items\n        legend.spacing.x = unit(0.5, \"cm\") # Increase horizontal spacing\n      ) +\n      guides(\n        color = guide_legend(nrow = 1, byrow = TRUE) # Force a single-row legend\n      )\n  })\n}\n\n# Define UI for the application ----\nui &lt;- fluidPage(\n  # Application title\n  titlePanel(\"Interactive Scatter Plot: Firearm Homicide Rate and Racial Proportions\"),\n  \n  # Sidebar layout with input and output\n  sidebarLayout(\n    # Sidebar panel for inputs\n    sidebarPanel(\n      # Dropdown menu for selecting racial group\n      selectInput(\n        inputId = \"selected_race\",\n        label = \"Select a Racial Group:\",\n        choices = c(\n          \"White NH\" = \"WHITE_NH_PROP\",\n          \"Black NH\" = \"B_NH_PROP\",\n          \"Hispanic\" = \"HISPANIC_PROP\",\n          \"Other/Multi NH\" = \"OTHER_MULTI_NH_PROP\"\n        ),\n        selected = \"WHITE_NH_PROP\"\n      )\n    ),\n    \n    # Main panel for displaying outputs\n    mainPanel(\n      plotOutput(outputId = \"scatterPlot\")\n    )\n  )\n)\n\n# Run the application ----\nshinyApp(ui = ui, server = server)\n\nShiny applications not supported in static R Markdown documents"
  }
]