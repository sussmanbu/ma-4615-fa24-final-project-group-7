[
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "Data",
    "section": "",
    "text": "The Dataset we picked: The Effect of Prior Police Contact on Victimization Reporting: Results From the Police-Public Contact and National Crime Victimization Surveys, United States, 2002-2011 (ICPSR 36370) Dataset Link\nThe dataset is collected by the Bureau of Justice Statistics (BJS), which is the primary statistical agency of the US’s Department of Justice. It was first established in 1979 in order to better understand statistics about crimes in the United States. The data collected in these studies provides information on how each individuals’ prior interactions with law enforcement, both directly and through the experiences of others, influence their decisions to report crimes. By linking data from the National Crime Victimization Survey (NCVS) with the Police-Public Contact Survey (PPCS), the researchers could explore the relationship between police encounter experiences and crime reporting behavior. The combined dataset used in this study contains data about survey participants in 2002, 2008, and 2011.\nThe dataset comprises person-level, household-level, and police encounter data, emphasizing key variables to study the relationship between police contact and victimization reporting. At the person level, variables include PER_ID(Unique person identifier), age, race, contact(if the respondent had police contact in the past 12 months), and number of face-to-face police contacts within the past year. At the household level, HH_ID identifies households, while hhpov (poverty status) and HH_size (number of household members) contextualize socioeconomic status. Police encounters are detailed with REASON (reason for contact), arrested (if the respondent was arrested), and cuffed (if handcuffed), which capture the nature of police interactions. Victimization variables like d_HH_P23 (household crime victimization), time2vic_inc_P23HH (time between police interview and crime), and notify_inc_P23HH (police notified of household crime) provide insights into subsequent incidents. Additional demographic variables such as education, mar_stat (marital status), and work_lw (employment status) add socioeconomic context.\nIn terms of cleaning the data, we originally started by planning to remove rows with over 90,000 na values, but quickly realized that some of these highlighted rare experiences with police that are information that would be crucial to our analysis.\nIn noticing that, we decided to more forwards by removing all columns we felt contained information which were not generally relevant to the analysis we wish to conduct. This was done by reading through the documentation for the dataset and filtering out columns we felt were unhelpful to our analysis. Below are a list of a few columns we removed and why:\nPSTRATA: PSEUDO-STRATUM CODE - We didn’t understand what this datapoint was representing, or how to use it.\nSECUCODE - STANDARD ERROR COMPUTATION UNIT CODE: Again another code, which is vague in description.\nnum_fu_HHint, num_fu_perint - Number of follow-up HH interviews post-PPCS, Number of follow-up PERSON interviews post-PPCS : The number of follow-ups for each person or household isn’t relevant to us.\nColumns ending with “_sub”: Data wasn’t originally collected, and carried back later on.\ntime2vic_inc_P23PER: We don’t want to investigate months from PPCS interview to victimization\nThis is all done in our cleaning script. In this script we start by removing certain columns, and then outlining the remaining columns as numerical or categorical, in order to check for inconsistent data values and providing default values to missing data. For most cases, we consider missing data to be okay, as na values may be interpreted as such in our analysis but we have considered adding certain default values like the average for numerical values such that it wouldn’t affect the average of the entire dataset.\nThis comes from the file data.qmd.\nYour first steps in this project will be to find data to work on.\nI recommend trying to find data that interests you and that you are knowledgeable about. A bad example would be if you have no interest in video games but your data set is about video games. I also recommend finding data that is related to current events, social justice, and other areas that have an impact.\nInitially, you will study one dataset but later you will need to combine that data with another dataset. For this reason, I recommend finding data that has some date and/or location components. These types of data are conducive to interesting visualizations and analysis and you can also combine this data with other data that also has a date or location variable. Data from the census, weather data, economic data, are all relatively easy to combine with other data with time/location components."
  },
  {
    "objectID": "data.html#what-makes-a-good-data-set",
    "href": "data.html#what-makes-a-good-data-set",
    "title": "Data",
    "section": "What makes a good data set?",
    "text": "What makes a good data set?\n\nData you are interested in and care about.\nData where there are a lot of potential questions that you can explore.\nA data set that isn’t completely cleaned already.\nMultiple sources for data that you can combine.\nSome type of time and/or location component."
  },
  {
    "objectID": "data.html#where-to-keep-data",
    "href": "data.html#where-to-keep-data",
    "title": "Data",
    "section": "Where to keep data?",
    "text": "Where to keep data?\nBelow 50mb: In dataset folder\nAbove 50mb: In dataset_ignore folder. This folder will be ignored by git so you’ll have to manually sync these files across your team.\n\nSharing your data\nFor small datasets (&lt;50mb), you can use the dataset folder that is tracked by github. Add the files just like you would any other file.\nIf you create a folder named data this will cause problems.\nFor larger datasets, you’ll need to create a new folder in the project root directory named dataset-ignore. This will be ignored by git (based off the .gitignore file in the project root directory) which will help you avoid issues with Github’s size limits. Your team will have to manually make sure the data files in dataset-ignore are synced across team members.\nYour load_and_clean_data.R file is how you will load and clean your data. Here is a an example of a very simple one.\n\n# source(\n  # \"scripts/load_and_clean_data.R\",\n  #echo = FALSE # Use echo=FALSE or omit it to avoid code output  \n#)\n\nYou should never use absolute paths (eg. /Users/danielsussman/path/to/project/ or C:\\MA415\\\\Final_Project\\).\nYou might consider using the here function from the here package to avoid path problems.\n\n\nLoad and clean data script\nThe idea behind this file is that someone coming to your website could largely replicate your analyses after running this script on the original data sets to clean them. This file might create a derivative data set that you then use for your subsequent analysis. Note that you don’t need to run this script from every post/page. Instead, you can load in the results of this script, which could be plain text files or .RData files. In your data page you’ll describe how these results were created. If you have a very large data set, you might save smaller data sets that you can use for exploration purposes. To link to this file, you can use [cleaning script](/scripts/load_and_clean_data.R) wich appears as cleaning script."
  },
  {
    "objectID": "data.html#rubric-on-this-page",
    "href": "data.html#rubric-on-this-page",
    "title": "Data",
    "section": "Rubric: On this page",
    "text": "Rubric: On this page\nYou will\n\nDescribe where/how to find data.\n\nYou must include a link to the original data source(s). Make sure to provide attribution to those who collected the data.\nWhy was the data collected/curated? Who put it together? (This is important, if you don’t know why it was collected then that might not be a good dataset to look at.\n\nDescribe the different data files used and what each variable means.\n\nIf you have many variables then only describe the most relevant ones and summarize the rest.\n\nDescribe any cleaning you had to do for your data.\n\nYou must include a link to your load_and_clean_data.R file.\nRrename variables and recode factors to make data more clear.\nAlso, describe any additional R packages you used outside of those covered in class.\nDescribe and show code for how you combined multiple data files and any cleaning that was necessary for that.\nSome repetition of what you do in your load_and_clean_data.R file is fine and encouraged if it helps explain what you did.\n\nOrganization, clarity, cleanliness of the page\n\nMake sure to remove excessive warnings, use clean easy-to-read code (without side scrolling), organize with sections, use bullets and other organization tools, etc.\nThis page should be self-contained."
  },
  {
    "objectID": "scripts/experimenting_plots.html",
    "href": "scripts/experimenting_plots.html",
    "title": "MA [46]15 Final Project - Group 7",
    "section": "",
    "text": "suppressPackageStartupMessages(library(tidyverse))\n\n\nfiltered &lt;- read_rds(\"dataset/police_interaction.rds\")\n\n\ncreate_barplot &lt;- function(data, col_name) {\n  ggplot(data, aes_string(x = col_name)) +\n    geom_bar(fill = \"skyblue\", color = \"black\") +\n    labs(title = paste(\"Barplot of\", col_name),\n         x = col_name,\n         y = \"Count\") +\n    theme_minimal() +\n    theme(axis.text.x = element_text(angle = 45, hjust = 1))\n}\nreason_key = c(\"Out of universe/missing\", \"Accident\", \"Traffic stop-driver\", \"Traffic stop-passenger\", \"Rsp reported crime\", \"Police assistance\", \"Police investigation\", \"Police suspected rsp\", \"Other reason\")\nfiltered |&gt;\n  group_by(RACE) |&gt;\n  mutate(\n    reason_text = reason_key[REASON + 1] \n  ) |&gt;\n  ggplot(aes(x = REASON, fill = reason_text)) +\n    geom_bar(show.legend = TRUE) +\n    theme_minimal() +\n  facet_wrap(~RACE) \n\n\n\n\n\n\n\nreason_key = c(\"Accident\", \"Traffic stop-driver\", \"Traffic stop-passenger\", \"Rsp reported crime\", \"Police assistance\", \"Police investigation\", \"Police suspected rsp\", \"Other reason\")\n# filtered_reason &lt;- filtered %&gt;%\n#  count(RACE , wt = reason_key, name = \"ny\")\n# filtered |&gt;\n#   group_by(RACE) |&gt;\n#   filter(REASON &gt; 0) |&gt;\n#   mutate(\n#     reason_text = reason_key[REASON + 1] \n#   ) |&gt;\n#   ggplot(aes(x = REASON, fill = reason_text)) +\n#     geom_bar(show.legend = TRUE) +\n#     theme_minimal() +\n#   facet_wrap(~RACE) \nfiltered |&gt;\n  filter(REASON &gt; 0) |&gt;\nggplot(aes(x = as.factor(REASON), fill = as.factor(RACE)))+\n  geom_bar(position = \"fill\") + \n  labs(title = \"Proportion of Reasons for Traffic Stop by Race\", x = \"Reason: \\n1 = Accident\\n2 = Traffic stop-driver\\n3 = Traffic stop-passenger\\n4 = Rsp reported crime\\n5 = Police assistance\\n6 = Police investigation\\n7 = Police suspected rsp\\n8 = Other reason\", y = \"Proportion\", fill = \"Race\") \n\n\n\n\n\n\n\n  #scale_fill_discrete(name = \"Race\")\n\n # ggplot(aes(x = REASON, y = after_stat(prop), fill = reason_text, group = reason_text)) +\n #    geom_bar(show.legend = TRUE, color = \"black\", position = \"dodge\") +\n #    theme_minimal() +\n#  theme(legend = element_text(\"Reason for traffic stop\"))\n\n\n  #theme(legend.title = element_text(hjust = 1))\n\nfiltered |&gt;\n  filter(REASON &gt; 0) |&gt;\n  ggplot(aes(x = RACE, fill = as.factor(REASON))) +\n  geom_bar(position = \"fill\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n  labs(title = \"Proportion of Reasons for Traffic Stop by Race\")\n\n\n\n\n\n\n\n\n\nfiltered |&gt;\n  filter(!is.na(ARRESTED)) |&gt;\n  ggplot(aes(x = as.factor(RACE), fill = as.factor(ARRESTED))) +\n    geom_bar(position = \"fill\") +\n    labs(title = \"Proportion of Arrests by Race\", x = \"Race\", y = \"Arrested (0 = No, 1 = Yes)\", fill = \"Arrested\")\n\n\n\n\n\n\n\nfiltered |&gt;\n  filter(!is.na(ARRESTED)) |&gt;\n  group_by(RACE) |&gt;\n  summarize(\n    arrest_proportion = sum(ARRESTED = 1) / n()\n  ) |&gt;\n  ggplot(aes(x = RACE, y = arrest_proportion, fill = RACE)) +\n  geom_bar(stat = \"identity\") + \n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\nggplot(filtered, aes(x = as.factor(RACE), fill = as.factor(CONTACT))) +\n  geom_bar(position = \"fill\") +\n  labs(title = \"Proportion of Race by Police Contact\", x = \"Race\", y = \"Police Contact\", fill = \"Police Contact\") +\n  scale_x_discrete(labels = c(\"0\" = \"No\", \"1\" = \"Yes\")) +\n  scale_y_continuous(labels = scales::percent)\n\n\n\n\n\n\n\n\n\nggplot(filtered, aes(x = as.factor(C_CONTCT), fill = as.factor(RACE))) +\n  geom_bar(position = \"fill\") +\n  labs(title = \"Proportion of Contact Type by Race\", x = \"Contact Type\", y = \"Proportion\", fill = \"Race\") +\n  scale_x_discrete(labels = c(\"0\" = \"No contact\", \n                              \"1\" = \"Citizen-initiated contact\", \n                              \"2\" = \"Vehicle stop\", \n                              \"3\" = \"Suspect-related contact\", \n                              \"4\" = \"Other police contact\")) +\n  scale_y_continuous(labels = scales::percent)\n\n\n\n\n\n\n\n\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.2.0 ──\n\n\n✔ broom        1.0.6     ✔ rsample      1.2.1\n✔ dials        1.3.0     ✔ tune         1.2.1\n✔ infer        1.0.7     ✔ workflows    1.1.4\n✔ modeldata    1.4.0     ✔ workflowsets 1.1.0\n✔ parsnip      1.2.1     ✔ yardstick    1.3.1\n✔ recipes      1.1.0     \n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n• Learn how to get started at https://www.tidymodels.org/start/\n\nsplit &lt;- initial_split(filtered, prop = .8) #good way to keep yourself honest. splits it by prop % being in training, 1-prop being test\ntraining&lt;- training(split)\ntesting &lt;- testing(split)\npredict_filtered &lt;- training |&gt;\n  filter(!is.na(ARRESTED) & !is.na(RACE) & !is.na(EDUCATION) & !is.na(HHPOV) & !is.na(MALE))\n\nmod1 &lt;- lm(ARRESTED ~ RACE + EDUCATION + HHPOV + MALE, predict_filtered)\nsummary(mod1)\n\n\nCall:\nlm(formula = ARRESTED ~ RACE + EDUCATION + HHPOV + MALE, data = predict_filtered)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.14612 -0.04018 -0.02358 -0.00786  1.01029 \n\nCoefficients:\n                                         Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)                             0.0981331  0.0092614  10.596  &lt; 2e-16\nRACEHispanic                           -0.0283200  0.0065927  -4.296 1.75e-05\nRACEOther or multiracial, Non-Hispanic -0.0272940  0.0085574  -3.190  0.00143\nRACEWhite, Non-Hispanic                -0.0267760  0.0050584  -5.293 1.22e-07\nEDUCATION                              -0.0045357  0.0005693  -7.967 1.77e-15\nHHPOV                                   0.0231955  0.0046349   5.004 5.68e-07\nMALE                                    0.0247919  0.0028146   8.808  &lt; 2e-16\n                                          \n(Intercept)                            ***\nRACEHispanic                           ***\nRACEOther or multiracial, Non-Hispanic ** \nRACEWhite, Non-Hispanic                ***\nEDUCATION                              ***\nHHPOV                                  ***\nMALE                                   ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1566 on 12491 degrees of freedom\nMultiple R-squared:  0.01782,   Adjusted R-squared:  0.01734 \nF-statistic: 37.76 on 6 and 12491 DF,  p-value: &lt; 2.2e-16\n\nplot(mod1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npredict_filtered_test &lt;- testing |&gt;\n  filter(!is.na(ARRESTED) & !is.na(RACE) & !is.na(EDUCATION) & !is.na(HHPOV) & !is.na(MALE))\n\nmod2 &lt;- lm(ARRESTED ~ RACE + EDUCATION + HHPOV + MALE, predict_filtered_test)\nsummary(mod2)\n\n\nCall:\nlm(formula = ARRESTED ~ RACE + EDUCATION + HHPOV + MALE, data = predict_filtered_test)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.11989 -0.03967 -0.02260 -0.00494  1.00467 \n\nCoefficients:\n                                        Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)                             0.091108   0.017514   5.202 2.10e-07\nRACEHispanic                           -0.013994   0.012358  -1.132  0.25755\nRACEOther or multiracial, Non-Hispanic -0.030829   0.016122  -1.912  0.05594\nRACEWhite, Non-Hispanic                -0.018860   0.009618  -1.961  0.04997\nEDUCATION                              -0.004808   0.001084  -4.435 9.53e-06\nHHPOV                                   0.025117   0.008847   2.839  0.00455\nMALE                                    0.027275   0.005371   5.078 4.03e-07\n                                          \n(Intercept)                            ***\nRACEHispanic                              \nRACEOther or multiracial, Non-Hispanic .  \nRACEWhite, Non-Hispanic                *  \nEDUCATION                              ***\nHHPOV                                  ** \nMALE                                   ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1517 on 3197 degrees of freedom\nMultiple R-squared:  0.02134,   Adjusted R-squared:  0.0195 \nF-statistic: 11.62 on 6 and 3197 DF,  p-value: 6.542e-13\n\nplot(mod2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# predict_filtered &lt;- filtered |&gt;\n#   filter(!is.na(ARRESTED) & !is.na(HHPOV) & !is.na(MALE))\n# \n# mod2 &lt;- lm(ARRESTED ~ HHPOV + MALE, predict_filtered)\n# summary(mod2)\n# plot(mod2)\n# predict_filtered &lt;- filtered |&gt;\n#   filter(!is.na(ARRESTED) & !is.na(RACE))\n# \n# mod2 &lt;- lm(ARRESTED ~ RACE, predict_filtered)\n# summary(mod2)\n# plot(mod2)\n# filtered |&gt;\n#   # ggplot(aes(x = C4_RACE, y = PROPER)) +\n#   # geom_quantile()\n#   \n#   ggplot(aes(x = C4_RACE, y = PROPER)) +\n#   geom_\n\n\nbroom::augment(mod1)\n\n# A tibble: 12,498 × 11\n   ARRESTED RACE   EDUCATION HHPOV  MALE .fitted   .resid    .hat .sigma .cooksd\n      &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n 1        0 White…        12     0     1 0.0417  -0.0417  2.43e-4  0.157 2.47e-6\n 2        1 Other…        11     0     1 0.0457   0.954   2.28e-3  0.156 1.22e-2\n 3        0 Black…         7     0     1 0.0912  -0.0912  1.61e-3  0.157 7.84e-5\n 4        0 White…        14     0     0 0.00786 -0.00786 1.93e-4  0.157 6.95e-8\n 5        0 White…        12     1     0 0.0401  -0.0401  8.60e-4  0.157 8.08e-6\n 6        0 White…        14     0     0 0.00786 -0.00786 1.93e-4  0.157 6.95e-8\n 7        0 Hispa…        11     0     0 0.0199  -0.0199  1.03e-3  0.157 2.38e-6\n 8        0 White…        16     0     1 0.0236  -0.0236  2.37e-4  0.157 7.68e-7\n 9        0 Hispa…        14     0     0 0.00631 -0.00631 9.88e-4  0.157 2.30e-7\n10        0 Hispa…        18     0     1 0.0130  -0.0130  1.30e-3  0.157 1.28e-6\n# ℹ 12,488 more rows\n# ℹ 1 more variable: .std.resid &lt;dbl&gt;\n\nggplot(mod1, aes(x = .fitted, y=.resid)) +\n  geom_point(alpha = 0.3) +\n  geom_smooth()\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\n\n\n\nggplot(mod1, aes(sample = rstandard(mod1))) +\n  geom_qq() + \n  stat_qq_line()\n\n\n\n\n\n\n\n\n\nlibrary(tidyverse)\nlibrary(reshape2)\n\n\nAttaching package: 'reshape2'\n\n\nThe following object is masked from 'package:tidyr':\n\n    smiths\n\n# Categorizing variables based on their types\n#removed \"CONTACT\" and \"ERROR\" due to \"Warning in cor(numeric_data, use = \"complete.obs\") : the standard deviation is zero\" error\nbinary_columns &lt;- c(\"MALE\", \"WORK_LW\", \"HHPOV\", \"PUB_HOUSE\",   \"PUB_HOUSE_SUB\", \"REGION\", \"INPERSON\", \"VICAR_CITIZEN\", \"VICAR_PRO_AUTO\", \"VICAR_PRO_PERS\", \"VICAR_OTH_CONT\", \"VICAR_IMPROPER\", \"D_HH_P23\", \"PROPER\")\n\nordinal_columns &lt;- c(\"C4_RACE\", \"MAR_STAT\", \"FREQ_DRV\", \"TENURE\",  \"MSA_STATUS\")\n\ncontinuous_columns &lt;- c(\"AGE\", \"EDUCATION\", \"EDUCATION_SUB\", \"NUM_MOVES\", \"NUM_CONT\", \"HH_SIZE\", \"PPCS_YEAR\", \"N_HH_P1\", \"N_PERS_P1\", \"NUM_CITIZEN_HH\", \"NUM_PRO_AUTO_HH\", \"NUM_PRO_PERS_HH\", \"NUM_OTH_CONT_HH\", \"NUM_IMPROPER_HH\")\n\n# Selecting data by type\nbinary_data &lt;- filtered %&gt;%\n  select(all_of(binary_columns)) %&gt;%\n  select(where(is.numeric))\n\nordinal_data &lt;- filtered %&gt;%\n  select(all_of(ordinal_columns)) %&gt;%\n  select(where(is.numeric))\n\ncontinuous_data &lt;- filtered %&gt;%\n  select(all_of(continuous_columns)) %&gt;%\n  select(where(is.numeric))\n\n# Calculating correlation matrices with appropriate methods\ncor_binary &lt;- cor(binary_data, use = \"pairwise.complete.obs\", method = \"pearson\")\ncor_ordinal &lt;- cor(ordinal_data, use = \"pairwise.complete.obs\", method = \"spearman\")\ncor_continuous &lt;- cor(continuous_data, use = \"pairwise.complete.obs\", method = \"pearson\")\n\n# Creating a heatmap function\nplot_heatmap &lt;- function(cor_matrix, title) {\n  cor_melted &lt;- melt(cor_matrix)\n  ggplot(cor_melted, aes(Var1, Var2, fill = value)) +\n    geom_tile(color = \"white\") +\n    scale_fill_gradient2(low = \"blue\", high = \"red\", mid = \"white\",\n                         midpoint = 0, limit = c(-1, 1), name = \"Correlation\") +\n    labs(title = title, x = \"Variables\", y = \"Variables\") +\n    theme_minimal() +\n    theme(axis.text.x = element_text(angle = 90, hjust = 1),\n          axis.text.y = element_text(angle = 0, vjust = 1))\n}\n\n# Plotting heatmaps for each correlation matrix\nif (ncol(binary_data) &gt; 1) {\n  print(plot_heatmap(cor_binary, \"Correlation Heatmap of Binary Variables\"))\n}\n\n\n\n\n\n\n\nif (ncol(ordinal_data) &gt; 1) {\n  print(plot_heatmap(cor_ordinal, \"Correlation Heatmap of Ordinal Variables\"))\n}\n\n\n\n\n\n\n\nif (ncol(continuous_data) &gt; 1) {\n  print(plot_heatmap(cor_continuous, \"Correlation Heatmap of Continuous Variables\"))\n}\n\n\n\n\n\n\n\n\nPlot of the marriage data for different races\n\nmarriageData &lt;- filtered |&gt;\n  filter(!is.na(MAR_STAT)) |&gt;\n  group_by(RACE) |&gt;\n  mutate(totalPerRace = n()) |&gt;\n  ungroup() |&gt;\n  group_by(RACE, MAR_STAT) |&gt;\n  select(RACE, MAR_STAT, totalPerRace) |&gt;\n  mutate(numPerMarStatRace = n()) |&gt;\n  filter(row_number()==1) |&gt;\n  mutate(prop = numPerMarStatRace/totalPerRace) |&gt;\n  select(-totalPerRace, -numPerMarStatRace) |&gt;\n  mutate(MAR_STAT = as.factor(MAR_STAT))\n\nggplot(data = marriageData) +\n  geom_bar(mapping = aes(x = RACE, fill = MAR_STAT, y = prop),\n           stat = \"identity\",\n           position = \"dodge\")"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This comes from the file about.qmd.\nThis is a website for the final project for MA[46]15 Data Science with R by Team Group 7. The members of this team are below."
  },
  {
    "objectID": "about.html#ian-ding",
    "href": "about.html#ian-ding",
    "title": "About",
    "section": "Ian Ding",
    "text": "Ian Ding\nIan is a junior studying Math and Computer Science. Github"
  },
  {
    "objectID": "about.html#arjun-patel",
    "href": "about.html#arjun-patel",
    "title": "About",
    "section": "Arjun Patel",
    "text": "Arjun Patel\nArjun Patel is a junior majoring in Medical Sciences and Applied Math. Github"
  },
  {
    "objectID": "about.html#zexian-xu",
    "href": "about.html#zexian-xu",
    "title": "About",
    "section": "Zexian Xu",
    "text": "Zexian Xu\nZexian Xu is a senior majoring in Statistics and Computer Science. Github"
  },
  {
    "objectID": "about.html#rukevwe-omusi",
    "href": "about.html#rukevwe-omusi",
    "title": "About",
    "section": "Rukevwe Omusi",
    "text": "Rukevwe Omusi\nRukevwe Omusi is a senior majoring in Data Science with a Film minor. Github"
  },
  {
    "objectID": "about.html#sviatoslav-shevchenko",
    "href": "about.html#sviatoslav-shevchenko",
    "title": "About",
    "section": "Sviatoslav Shevchenko",
    "text": "Sviatoslav Shevchenko\nSviat is a junior pursuing a dual degree in Data Science and Economics. Github\n\n\nAbout this Template.\nThis is based off of the standard Quarto website template from RStudio (2023.09.0 Build 463)."
  },
  {
    "objectID": "posts/2023-12-20-examples/examples.html",
    "href": "posts/2023-12-20-examples/examples.html",
    "title": "Examples",
    "section": "",
    "text": "Here are some examples of changing the size of a figure.\n\nplot(1:10)\n\n\n\n\n\n\n\n\n\nplot(1:10)\n\n\n\n\n\n\n\n\nWe can also specify column: screen and out-width: 100% so that the figure will fill the screen. plot in the svg vector graphics file format.\n\nlibrary(ggplot2)\nggplot(pressure, aes(x = temperature, y = pressure)) + geom_point()"
  },
  {
    "objectID": "posts/2023-12-20-examples/examples.html#figure-sizing",
    "href": "posts/2023-12-20-examples/examples.html#figure-sizing",
    "title": "Examples",
    "section": "",
    "text": "Here are some examples of changing the size of a figure.\n\nplot(1:10)\n\n\n\n\n\n\n\n\n\nplot(1:10)\n\n\n\n\n\n\n\n\nWe can also specify column: screen and out-width: 100% so that the figure will fill the screen. plot in the svg vector graphics file format.\n\nlibrary(ggplot2)\nggplot(pressure, aes(x = temperature, y = pressure)) + geom_point()"
  },
  {
    "objectID": "posts/2024-10-28-Data-Cleaning/Data Cleaning.html",
    "href": "posts/2024-10-28-Data-Cleaning/Data Cleaning.html",
    "title": "Data Cleaning and Loading & Data for Equity",
    "section": "",
    "text": "This week, we finalized the data description and characterization before beginning the process of loading and cleaning the data. The data, which was collected from the Bureau of Justice Statistics (BJS), is a person-level and household-level dataset characterizing police encounter data, emphasizing key variables to study the relationship between police contact and victimization reporting. The raw data consists of 110 variables and 105273 unique observations.\nThe first step in the process of loading & cleaning the data was converting the tsv file we found online to a R dataset. We did so through the read_tsv tidyverse function, and the process went smoothly. There were no errors, unusual values, or oddly formatted data other than NA, or missing, values. After performing this initial data import, we decided to filter out specific variables that we didn’t intend on using in our analysis. These included variables such as PSTRATA and SEUCODE, which are more complex values used to calculate parameters like variance, and WEIGHT, which was the relative weight given to each observation because of the differences in sizes of the dataset in different years. We decided to remove these variables because we figured it would be better if we could control.\nAfter performing this initial data import, we decided to filter out specific variables that we didn’t intend on using in our analysis. These included variables such as PSTRATA and SECUCODE, which are more complex values used to calculate parameters like variance, and WEIGHT, which was the relative weight given to each observation due to dataset size differences across different years. We decided to remove variables like these because we realized that, in order to perform analysis of the data ourselves, it would be more accurate and meaningful to calculate these parameters from scratch. That way, we would decrease the risk of performing a biased analysis with data values we didn’t fully comprehend.\nWe also made the systematic decision to remove the variables corresponding to data about follow-up interviews, like NUM_FU_HHINT and NUM_FU_PERINT. We did so because we currently don’t intend to use that data in our analysis. Similarly, we decided to systematically remove the variable TIME2VIC_INC_P23PER because we don’t want to investigate data about the time between PPCS interview and victimization.\nFinally, we were forced to exclude certain variables from the dataset that contained values we didn’t understand. For example, all variables that ended with “_sub” were excluded because we were unsure what the entries “missing carried back from later waves” corresponded to.\nAdditionally, an important step in the data cleaning process was to explore the data with the help of barplot showing distributions for specific variables and boxplots showing outliers. This would aid us in our analysis and further steps when working with the data. Below is a function used to create boxplots:\ncreate_outlier_boxplot &lt;- function(data, col_name) { ggplot(data, aes_string(y = col_name)) + geom_boxplot(outlier.color = “red”,\noutlier.shape = 16,\noutlier.size = 2) +\nlabs(title = paste(“Boxplot of”, col_name, “with Outliers”), y = col_name) + theme_minimal() }\nData For Equity Principles:\nBeneficence: The dataset we are working on is about crimes and how interactions with police can influence how, why, or if people report them. It is important to us to not misrepresent a certain group of people as more prone to positive or negative interactions with police, removing any personal bias in how we analyze the data. We must represent the data objectively, discussing its implications and results within a wider scope and context, signifying that there are always variables left out of scope that can influence the results nevertheless.\nRespect for Persons: The analysis that we aim to do for this dataset should by no means be definitive, but still leave open-ended questions and perhaps concerns. We do not want to create a bias for the viewers of this analysis and give them the power to make their own decisions about this data. It is again important that we bring this unbiased view as means of displaying accurate and unbiased results (giving respect to the groups showcased in the data), however also allowing for the viewers the right to consider and decide whether they agree with us or not, without forcing any bias on them."
  },
  {
    "objectID": "posts/2024-11-4-EDA/more EDA.html",
    "href": "posts/2024-11-4-EDA/more EDA.html",
    "title": "EDA",
    "section": "",
    "text": "This week, we continued exploring the dataset with a focus on relationships among different variables such as reasons for traffic stops, arrests, and the types of police contacts experienced under different demographic groups.\nFor example, below is a plot being generated. We studied the disparities in experiences of improper police contact among different racial groups. We made a bar plot to illustrate the proportion within each racial group who reported experiencing improper police contact and the result reveals notable differences across racial groups.This insight lays the groundwork for deeper analysis to understand what factors contribute to these differences, potentially using demographic and situational variables in further modeling.\n\nsuppressPackageStartupMessages(library(tidyverse))\nsuppressPackageStartupMessages(library(tidymodels))\ndata &lt;- readRDS(\"dataset/police_interaction.rds\") \ndata |&gt;\n  filter(!is.na(PROPER)) |&gt;\n  group_by(RACE) |&gt;\n  summarize(\n    improper_proportion = sum(PROPER == 0) / n()\n  ) |&gt;\n  ggplot(aes(x = RACE, y = improper_proportion, fill = RACE)) +\n  geom_bar(stat = \"identity\")\n\n\n\n\n\n\n\n\nThis is the second plot that we would like to include here: We made a heatmap that visualizes the correlations between various binary variables in the dataset, where the color intensity represents the strength and direction of the correlation (with red indicating positive correlations and blue indicating negative correlations). There are notable positive correlations such as HHPOV and WORK_LW and notable negative correlations such as WORK_LW and any high-income indicator. This heatmap helps identify groups of binary variables that may be related or unrelated, which can be useful in modeling and to understand relationships among the data.\n\nlibrary(tidyverse)\nlibrary(reshape2)\n\n\nAttaching package: 'reshape2'\n\n\nThe following object is masked from 'package:tidyr':\n\n    smiths\n\n# Categorizing variables based on their types\n#removed \"CONTACT\" and \"ERROR\" due to \"Warning in cor(numeric_data, use = \"complete.obs\") : the standard deviation is zero\" error\nbinary_columns &lt;- c(\"MALE\", \"WORK_LW\", \"HHPOV\", \"PUB_HOUSE\",   \"PUB_HOUSE_SUB\", \"REGION\", \"INPERSON\", \"VICAR_CITIZEN\", \"VICAR_PRO_AUTO\", \"VICAR_PRO_PERS\", \"VICAR_OTH_CONT\", \"VICAR_IMPROPER\", \"D_HH_P23\", \"PROPER\")\n\nordinal_columns &lt;- c(\"C4_RACE\", \"MAR_STAT\", \"FREQ_DRV\", \"TENURE\",  \"MSA_STATUS\")\n\ncontinuous_columns &lt;- c(\"AGE\", \"EDUCATION\", \"EDUCATION_SUB\", \"NUM_MOVES\", \"NUM_CONT\", \"HH_SIZE\", \"PPCS_YEAR\", \"N_HH_P1\", \"N_PERS_P1\", \"NUM_CITIZEN_HH\", \"NUM_PRO_AUTO_HH\", \"NUM_PRO_PERS_HH\", \"NUM_OTH_CONT_HH\", \"NUM_IMPROPER_HH\")\n\nfiltered &lt;- data\n# Selecting data by type\nbinary_data &lt;- filtered %&gt;%\n  select(all_of(binary_columns)) %&gt;%\n  select(where(is.numeric))\n\nordinal_data &lt;- filtered %&gt;%\n  select(all_of(ordinal_columns)) %&gt;%\n  select(where(is.numeric))\n\ncontinuous_data &lt;- filtered %&gt;%\n  select(all_of(continuous_columns)) %&gt;%\n  select(where(is.numeric))\n\n# Calculating correlation matrices with appropriate methods\ncor_binary &lt;- cor(binary_data, use = \"pairwise.complete.obs\", method = \"pearson\")\ncor_ordinal &lt;- cor(ordinal_data, use = \"pairwise.complete.obs\", method = \"spearman\")\ncor_continuous &lt;- cor(continuous_data, use = \"pairwise.complete.obs\", method = \"pearson\")\n\n# Creating a heatmap function\nplot_heatmap &lt;- function(cor_matrix, title) {\n  cor_melted &lt;- melt(cor_matrix)\n  ggplot(cor_melted, aes(Var1, Var2, fill = value)) +\n    geom_tile(color = \"white\") +\n    scale_fill_gradient2(low = \"blue\", high = \"red\", mid = \"white\",\n                         midpoint = 0, limit = c(-1, 1), name = \"Correlation\") +\n    labs(title = title, x = \"Variables\", y = \"Variables\") +\n    theme_minimal() +\n    theme(axis.text.x = element_text(angle = 90, hjust = 1),\n          axis.text.y = element_text(angle = 0, vjust = 1))\n}\n\n# Plotting heatmaps for each correlation matrix\nif (ncol(binary_data) &gt; 1) {\n  print(plot_heatmap(cor_binary, \"Correlation Heatmap of Binary Variables\"))\n}\n\n\n\n\n\n\n\nif (ncol(ordinal_data) &gt; 1) {\n  print(plot_heatmap(cor_ordinal, \"Correlation Heatmap of Ordinal Variables\"))\n}\n\n\n\n\n\n\n\nif (ncol(continuous_data) &gt; 1) {\n  print(plot_heatmap(cor_continuous, \"Correlation Heatmap of Continuous Variables\"))\n}\n\n\n\n\n\n\n\n\nFor modeling, we used a linear model to try to predict whether someone would be arrested based on their race, years of education, if their household is living in poverty, and their gender. We separated the data into a train and test set with an 80% split. In the training set, the F-statistic is far over 1 and the p-values show over 95% confidence in all the response variables being significant.\n\nsplit &lt;- initial_split(filtered, prop = .8) #good way to keep yourself honest. splits it by prop % being in training, 1-prop being test\ntraining&lt;- training(split)\ntesting &lt;- testing(split)\npredict_filtered &lt;- training |&gt;\n  filter(!is.na(ARRESTED) & !is.na(RACE) & !is.na(EDUCATION) & !is.na(HHPOV) & !is.na(MALE))\n\nmod1 &lt;- lm(ARRESTED ~ RACE + EDUCATION + HHPOV + MALE, predict_filtered)\nsummary(mod1)\n\n\nCall:\nlm(formula = ARRESTED ~ RACE + EDUCATION + HHPOV + MALE, data = predict_filtered)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.14796 -0.03851 -0.02355 -0.00673  1.01148 \n\nCoefficients:\n                                         Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)                             0.1005132  0.0091200  11.021  &lt; 2e-16\nRACEHispanic                           -0.0332987  0.0065175  -5.109 3.28e-07\nRACEOther or multiracial, Non-Hispanic -0.0300471  0.0084248  -3.566 0.000363\nRACEWhite, Non-Hispanic                -0.0300465  0.0050001  -6.009 1.92e-09\nEDUCATION                              -0.0045524  0.0005609  -8.116 5.24e-16\nHHPOV                                   0.0215277  0.0045161   4.767 1.89e-06\nMALE                                    0.0259233  0.0027728   9.349  &lt; 2e-16\n                                          \n(Intercept)                            ***\nRACEHispanic                           ***\nRACEOther or multiracial, Non-Hispanic ***\nRACEWhite, Non-Hispanic                ***\nEDUCATION                              ***\nHHPOV                                  ***\nMALE                                   ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1549 on 12586 degrees of freedom\nMultiple R-squared:  0.01913,   Adjusted R-squared:  0.01867 \nF-statistic: 40.92 on 6 and 12586 DF,  p-value: &lt; 2.2e-16\n\nplot(mod1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn the test set, the F-statistic is much lower and the p-values and t-stats rise for each variable, with the p-value for each race variable but the intercept being over 0.1. This finding implies that there isn’t a significant relationship on average between these race categories and being arrested, although, for “Other or multiracial, Non-Hispanic”, that could be skewed by the much smaller sample size of the category compared to the other races in the category. Futhermore, the difference in performance between the testing and training data implies an overfitting that is being done by the model, that is different from actually learning the relationship.\n\npredict_filtered_test &lt;- testing |&gt;\n  filter(!is.na(ARRESTED) & !is.na(RACE) & !is.na(EDUCATION) & !is.na(HHPOV) & !is.na(MALE))\n\nmod2 &lt;- lm(ARRESTED ~ RACE + EDUCATION + HHPOV + MALE, predict_filtered_test)\nsummary(mod2)\n\n\nCall:\nlm(formula = ARRESTED ~ RACE + EDUCATION + HHPOV + MALE, data = predict_filtered_test)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.13360 -0.04190 -0.02285 -0.00936  1.01456 \n\nCoefficients:\n                                        Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)                             0.082088   0.018601   4.413 1.05e-05\nRACEHispanic                            0.004917   0.012921   0.381 0.703573\nRACEOther or multiracial, Non-Hispanic -0.020445   0.017103  -1.195 0.232012\nRACEWhite, Non-Hispanic                -0.006045   0.010063  -0.601 0.548032\nEDUCATION                              -0.004763   0.001149  -4.144 3.51e-05\nHHPOV                                   0.033108   0.009843   3.364 0.000779\nMALE                                    0.023010   0.005689   4.045 5.36e-05\n                                          \n(Intercept)                            ***\nRACEHispanic                              \nRACEOther or multiracial, Non-Hispanic    \nRACEWhite, Non-Hispanic                   \nEDUCATION                              ***\nHHPOV                                  ***\nMALE                                   ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.158 on 3102 degrees of freedom\nMultiple R-squared:  0.01831,   Adjusted R-squared:  0.01641 \nF-statistic: 9.644 on 6 and 3102 DF,  p-value: 1.537e-10\n\nplot(mod2)"
  },
  {
    "objectID": "posts/2023-10-13-first-team-meeting/first-team-meeting.html",
    "href": "posts/2023-10-13-first-team-meeting/first-team-meeting.html",
    "title": "First Team Meeting",
    "section": "",
    "text": "These are the steps that you will take today to get started on your project. Today, you will just be brainstorming, and then next week, you’ll get started on the main aspects of the project.\n\nStart by introducing yourselves to each other. I also recommend creating a private channel on Microsoft Teams with all your team members. This will be a place that you can communicate and share ideas, code, problems, etc.\nDiscuss what aspects of the project each of you are more or less excited about. These include\n\nCollecting, cleaning, and munging data ,\nStatistical Modeling,\nVisualization,\nWriting about analyses, and\nManaging and reviewing team work.\n\nBased on this, discuss where you feel your strengths and weaknesses might be.\nNext, start brainstorming questions you hope to answer as part of this project. This question should in some way be addressing issues around racial disparities. The questions you come up with should be at the level of the question we started with when exploring the HMDA data. (“Are there differences in the ease of securing a loan based on the race of the applicant?”) You’ll revise your questions a lot over the course of the project. Come up with a few questions that your group might be interested in exploring.\nBased on these questions, start looking around for data that might help you analyze this. If you are looking at U.S. based data, data.gov is a good source and if you are looking internationally, I recommend checking out the World Bank. Also, try Googling for data. Include “data set” or “dataset” in your query. You might even include “CSV” or some other format. Using “data” by itself in your query often doesn’t work too well. Spend some time searching for data and try to come up with at least three possible data sets. (For your first blog post, you’ll write short proposals about each of them that I’ll give feedback on.)\nCome up with a team name. Next week, I’ll provide the Github Classroom assignment that will be where you work on your final project and you’ll have to have your team name finalized by then. Your project will be hosted online at the website with a URL like sussmanbu.github.io/ma4615-fa23-final-project-TEAMNAME.\n\nNext time, you’ll get your final project website set up and write your first blog post."
  },
  {
    "objectID": "posts/2023-10-15-getting-started/getting-started.html",
    "href": "posts/2023-10-15-getting-started/getting-started.html",
    "title": "Getting started",
    "section": "",
    "text": "Below, the items marked with [[OP]] should only be done by one person on the team.\n\nTo get started\n\n[[OP]] One person from the team should click the Github Classroom link on Teams.\n[[OP]] That person types in the group name for their group.\nThe rest of the team now clicks the Github Classroom link and selects their team from the dropdown list.\nFinally, each of you can clone the repository to your laptop like a normal assignment.\n\n\n\nSetting up the site\n\n[[OP]] Open the terminal and run quarto publish gh-pages.\n[[OP]] Select Yes to the prompt:  ? Publish site to https://sussmanbu.github.io/ma4615-fa23-final-project-TEAMNAME/ using gh-pages? (Y/n)\n[[OP]] Wait for the process to finish.\nOnce it is done, you can go to the URL it asked you about to see your site.\n\nNote: This is the process you will use every time you want to update your published site. Make sure to always follow the steps below for rendering, previewing, and committing your changes before doing these publish steps. Anyone can publish in the future.\n\n\nCustomize your site\n\n[[OP]] Open the _quarto.yml file and update the title to include your team name.\n[[OP]] Go to the about.qmd and remove the TF’s and professor’s names.\nadd your own along with a short introduction and a link to your Github user page.\n[[OP]] Render the site.\n[[OP]] Check and make sure you didn’t get any errors.\n[[OP]] Commit your changes and push.\n[[OP]] Repeat the steps under Setting up your site.\n\nOnce one person is done with this, each teammate in the group can, in turn, repeat steps 3-7. Before doing so, make sure to pull the changes from teammates before starting to make new changes. (We’ll talk soon about ways to organize your work and resolve conflicts.)\n\n\nStart your first post\n\nTo start your first post first, run remotes::install_github(\"sussmanbu/quartopost\") in your Console.\n[[OP]] Run quartopost::quartopost() (or click Addins-&gt;Create Quarto Post, or use C-Shift-P, type “Create Quarto” and press enter to run the command).\n\nNow you can start working on your post. You’ll want to render your post to see what it will look like on the site.\n\nEvery time you want to make a new post, you can repeat step 2 above.\nWhen you want to publish your progress, follow steps 4-7 from Customize your site.\n\nFinally, make sure to read through everything on this site which has the directions and rubric for the final project."
  },
  {
    "objectID": "posts/2024-10-11-First-Blog-Post/firstBlog.html",
    "href": "posts/2024-10-11-First-Blog-Post/firstBlog.html",
    "title": "First Blog",
    "section": "",
    "text": "https://www.icpsr.umich.edu/web/NACJD/studies/36370/\nThis dataset looks into how prior experiences with the police can influence one’s decision to report a crime. The data is from 2002-2011. It includes 110 variables and 105273 rows with information on the prevalence, frequency, and nature of respondents’ encounters with the police in the prior year, as well as respondents’ personal and household victimization experiences that occurred after the administration of the PPCS, including whether the crime was reported to the police. Demographic variables include age, race, gender, education, and socioeconomic status.\nThe data groups race and ethnicity in the variable label C4_RACE into these categories:\n1. White, Non-Hispanic\n2. Black, Non-Hispanic\n3. Hispanic\n4. Other or multiracial, Non-Hispanic\nThe variables for the encounters with the police are numerical such as whether the respondent was arrested or handcuffed, incident frequency, etc. which makes it easy to plot by race, gender, or other categorical variables.\nhttps://catalog.data.gov/dataset/rates-and-trends-in-heart-disease-and-stroke-mortality-among-us-adults-35-by-county-a-2000-45659\nRates and Trends in Heart Disease and Stroke Mortality Among US Adults (35+) by County, Age Group, Race/Ethnicity, and Sex – 2000-2019\nThe dataset includes 21 columns and 5,770,224 rows. There is not much said about the data being collected in this dataset, it is said that “rates and trends were estimated using Bayesian spatiotemporal model and a smoothed over space and time demographic group”. Additionally, using the 2010 US population figures, the ages are standardized to 10-year groups. There is no issue with loading and cleaning the data, however the csv function used before in class may have a row limit lower than the number of rows in this dataset. This could for example be addressed by considering a number of rows under that row limit. The dataset allows for attempts to understand what kind of stroke or deceive is prevalent among certain racial groups, regions (counties, states), genders or ages, Perhaps, there could also be a difference in regions by year, which would be interesting to see given there is a lot of change in a specific region. Immediately, a challenge will be to decide whether to include the over 5 million rows in the analysis, which would require using some other csv reader or adjust the data for this one which could alter the results of the analysis. Another challenge are the overall categories within some variables in the dataset (“overall” in sex and gender variables), which may be of not great use when we would be trying to analyze for differences by either of the variables. To conclude, the dataset seems to be rich, however it does have some aspects that may be a challenge in the initial analysis stages.\nhttps://data.nysed.gov/downloads.php\nThe data describes students enrollment reported by total public school (aggregated data for all districts and charter schools), county (aggregated data for all districts and charter schools in the county), Needs-to-Resource-Capacity (N/RC) group, district, and public schools.  \n  The ENTITY_CD is the 12-digit Basic Educational Data System (BEDS) code that uniquely identifies the entity (school, district, etc.). The need/resource capacity index, a measure of a district’s ability to meet the needs of its students with local resources, is the ratio of the estimated poverty percentage1 (expressed in standard score form) to the Combined Wealth Ratio2 (expressed in standard score form).  \n  The dataset contains the different enrollments by grade, race/ethnicity, gender and other groups."
  },
  {
    "objectID": "posts/2024-10-15-Data-Background/Data Background.html",
    "href": "posts/2024-10-15-Data-Background/Data Background.html",
    "title": "Data Background",
    "section": "",
    "text": "Dataset Name: The Effect of Prior Police Contact on Victimization Reporting: Results From the Police-Public Contact and National Crime Victimization Surveys, United States, 2002-2011 (ICPSR 36370) Dataset Link\nThe data in this dataset is compiled from the National Crime Victimization Survey (NCVS) and the Police-Public Contact Survey (PPCS). Both are collected by the Bureau of Justice Statistics (BJS), which is the primary statistical agency of the US’s Department of Justice. It was first established in 1979 in order to better understand statistics about crimes in the United States. The data collected in these studies provides information on how each individuals’ prior interactions with law enforcement, both directly and through the experiences of others, influence their decisions to report crimes. By linking data from the National Crime Victimization Survey (NCVS) with the Police-Public Contact Survey (PPCS), the researchers could explore the relationship between police encounter experiences and crime reporting behavior. The combined dataset used in this study contains data about survey participants in 2002, 2008, and 2011.\nVariables common to the NCVS and PPCS were concatenated to create unique identifiers which could link the two datasets, but there was variation across different years in how much of the data that could be linked. For example, due to a switch in study design, only about 15% of participants who took the both surveys in 2011 could be included in the combined dataset. Another issue arises from the unequal weight of data of people within differing segments of the population. Because not all PPCS interviews could be matched, population estimates are not necessarily representative of the U.S. population, even though both of the original studies were designed to be representative. Additionally, since this dataset relies on responses to both of these surveys, it is possible that the sample is biased to include more individuals who experienced victimization at the hand of law enforcement, as they may be more likely to respond to the NCVS versus those who may not have.\nThe dataset provides detailed crime data in the U.S. and is used to inform policy-making decisions. Law enforcement agencies, policymakers, and researchers. These groups use this data to identify crime trends, allocate resources, and develop crime prevention strategies.\nBJS Website"
  },
  {
    "objectID": "posts/2024-11-06-more-eda/more-eda.html",
    "href": "posts/2024-11-06-more-eda/more-eda.html",
    "title": "more EDA",
    "section": "",
    "text": "This week, we continued exploring the dataset with a focus on relationships among different variables such as reasons for traffic stops, arrests, and the types of police contacts experienced under different demographical groups.\nFor example, below is a plot being generated. We studied the disparities in experiences of improper police contact among different racial groups. We made a bar plot to illustrate the proportion within each racial group who reported experiencing improper police contact and the result reveals notable differences across racial groups.This insight lays the groundwork for deeper analysis to understand what factors contribute to these differences, potentially using demographic and situational variables in further modeling.\n\nsuppressPackageStartupMessages(library(tidyverse))\nsuppressPackageStartupMessages(library(tidymodels))\ndata &lt;- readRDS(\"dataset/police_interaction.rds\") \ndata |&gt;\n  filter(!is.na(PROPER)) |&gt;\n  group_by(RACE) |&gt;\n  summarize(\n    improper_proportion = sum(PROPER == 0) / n()\n  ) |&gt;\n  ggplot(aes(x = RACE, y = improper_proportion, fill = RACE)) +\n  geom_bar(stat = \"identity\")\n\n\n\n\n\n\n\n\nThis is the second plot that we would like to include here: We made a heatmap that visualizes the correlations between various continuous variables in the dataset, where the color intensity represents the strength and direction of the correlation (with red indicating positive correlations and blue indicating negative correlations). There are notable positive correlations such as HHPOV and WORK_LW and notable negative correlations such as WORK_LW and any high-income indicator. This heatmap helps identify groups of continuous variables that may be related or unrelated, which can be useful in modeling and to understand relationships among the data.\n\nlibrary(tidyverse)\nlibrary(reshape2)\n\n\nAttaching package: 'reshape2'\n\n\nThe following object is masked from 'package:tidyr':\n\n    smiths\n\n# Categorizing variables based on their types\n#removed \"CONTACT\" and \"ERROR\" due to \"Warning in cor(numeric_data, use = \"complete.obs\") : the standard deviation is zero\" error\nbinary_columns &lt;- c(\"MALE\", \"WORK_LW\", \"HHPOV\", \"PUB_HOUSE\",   \"PUB_HOUSE_SUB\", \"REGION\", \"INPERSON\", \"VICAR_CITIZEN\", \"VICAR_PRO_AUTO\", \"VICAR_PRO_PERS\", \"VICAR_OTH_CONT\", \"VICAR_IMPROPER\", \"D_HH_P23\", \"PROPER\")\n\nordinal_columns &lt;- c(\"C4_RACE\", \"MAR_STAT\", \"FREQ_DRV\", \"TENURE\",  \"MSA_STATUS\")\n\ncontinuous_columns &lt;- c(\"AGE\", \"EDUCATION\", \"EDUCATION_SUB\", \"NUM_MOVES\", \"NUM_CONT\", \"HH_SIZE\", \"PPCS_YEAR\", \"N_HH_P1\", \"N_PERS_P1\", \"NUM_CITIZEN_HH\", \"NUM_PRO_AUTO_HH\", \"NUM_PRO_PERS_HH\", \"NUM_OTH_CONT_HH\", \"NUM_IMPROPER_HH\")\n\nfiltered &lt;- data\n# Selecting data by type\nbinary_data &lt;- filtered %&gt;%\n  select(all_of(binary_columns)) %&gt;%\n  select(where(is.numeric))\n\nordinal_data &lt;- filtered %&gt;%\n  select(all_of(ordinal_columns)) %&gt;%\n  select(where(is.numeric))\n\ncontinuous_data &lt;- filtered %&gt;%\n  select(all_of(continuous_columns)) %&gt;%\n  select(where(is.numeric))\n\n# Calculating correlation matrices with appropriate methods\ncor_binary &lt;- cor(binary_data, use = \"pairwise.complete.obs\", method = \"pearson\")\ncor_ordinal &lt;- cor(ordinal_data, use = \"pairwise.complete.obs\", method = \"spearman\")\ncor_continuous &lt;- cor(continuous_data, use = \"pairwise.complete.obs\", method = \"pearson\")\n\n# Creating a heatmap function\nplot_heatmap &lt;- function(cor_matrix, title) {\n  cor_melted &lt;- melt(cor_matrix)\n  ggplot(cor_melted, aes(Var1, Var2, fill = value)) +\n    geom_tile(color = \"white\") +\n    scale_fill_gradient2(low = \"blue\", high = \"red\", mid = \"white\",\n                         midpoint = 0, limit = c(-1, 1), name = \"Correlation\") +\n    labs(title = title, x = \"Variables\", y = \"Variables\") +\n    theme_minimal() +\n    theme(axis.text.x = element_text(angle = 90, hjust = 1),\n          axis.text.y = element_text(angle = 0, vjust = 1))\n}\n\n# Plotting heatmaps for each correlation matrix\nif (ncol(continuous_data) &gt; 1) {\n  print(plot_heatmap(cor_continuous, \"Correlation Heatmap of Continuous Variables\"))\n}\n\n\n\n\n\n\n\n\nFor modeling, we used a linear model to try to predict whether someone would be arrested based on their race, years of education, if their household is living in poverty, and their gender. We separated the data into a train and test set with an 80% split. In the training set, the F-statistic is far over 1 and the p-values show over 95% confidence in all the response variables being significant.\n\nsplit &lt;- initial_split(filtered, prop = .8) #good way to keep yourself honest. splits it by prop % being in training, 1-prop being test\ntraining&lt;- training(split)\ntesting &lt;- testing(split)\npredict_filtered &lt;- training |&gt;\n  filter(!is.na(ARRESTED) & !is.na(RACE) & !is.na(EDUCATION) & !is.na(HHPOV) & !is.na(MALE))\n\nmod1 &lt;- lm(ARRESTED ~ RACE + EDUCATION + HHPOV + MALE, predict_filtered)\n\nIn the test set, the F-statistic is much lower and the p-values and t-stats rise for each variable, with the p-value for each race variable but the intercept being over 0.1. This finding implies that there isn’t a significant relationship on average between these race categories and being arrested, although, for “Other or multiracial, Non-Hispanic”, that could be skewed by the much smaller sample size of the category compared to the other races in the category.\n\npredict_filtered_test &lt;- testing |&gt;\n  filter(!is.na(ARRESTED) & !is.na(RACE) & !is.na(EDUCATION) & !is.na(HHPOV) & !is.na(MALE))\n\nmod2 &lt;- lm(ARRESTED ~ RACE + EDUCATION + HHPOV + MALE, predict_filtered_test)\nplot(mod2)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MA [46]15 Final Project",
    "section": "",
    "text": "Final Project due May 7, 2024 at 11:59pm.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmore EDA\n\n\n\n\n\nMore EDA and Initial thoughts \n\n\n\n\n\nNov 6, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nEDA\n\n\n\n\n\nBlog post discussing further EDA and initial thoughts on statistical modeling. \n\n\n\n\n\nNov 4, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nData Cleaning and Loading & Data for Equity\n\n\n\n\n\nBlog post outlining specifics of the data and how we went about cleaning it, additionally discussing the data for equity principles, as well as related article. \n\n\n\n\n\nOct 28, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nData Background\n\n\n\n\n\nBlog post outlining background information related to our dataset. \n\n\n\n\n\nOct 15, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nFirst Blog\n\n\n\n\n\nFirst Blog Post \n\n\n\n\n\nOct 11, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nExamples\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 26, 2024\n\n\nDaniel Sussman\n\n\n\n\n\n\n\n\n\n\n\n\nGetting started\n\n\n\n\n\n\n\n\nDirections to set up your website and create your first post. \n\n\n\n\n\nFeb 23, 2024\n\n\nDaniel Sussman\n\n\n\n\n\n\n\n\n\n\n\n\nFirst Team Meeting\n\n\n\n\n\n\n\n\nThis post details the steps you’ll take for your first team meeting. \n\n\n\n\n\nFeb 21, 2024\n\n\nDaniel Sussman\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "analysis.html",
    "href": "analysis.html",
    "title": "Analysis",
    "section": "",
    "text": "Loading our Police Interaction dataset.\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\ndata &lt;- read_rds(here::here(\"dataset/police_interaction.rds\"))\nOne initial step in conducting EDA is by looking at the “PROPER” variable, which outlines whether or not the police behaved properly during the interaction, and seeing how the proportion of improper actions differs between individuals of different races.\ndata |&gt;\n  filter(!is.na(PROPER)) |&gt;\n  group_by(RACE) |&gt;\n  summarize(\n    improper_proportion = sum(PROPER == 0) / n()\n  ) |&gt;\n  ggplot(aes(x = RACE, y = improper_proportion, fill = RACE)) +\n  geom_bar(stat = \"identity\") + \n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\nThis comes from the file analysis.qmd.\nWe describe here our detailed data analysis. This page will provide an overview of what questions you addressed, illustrations of relevant aspects of the data with tables and figures, and a statistical model that attempts to answer part of the question. You’ll also reflect on next steps and further analysis.\nThe audience for this page is someone like your class mates, so you can expect that they have some level of statistical and quantitative sophistication and understand ideas like linear and logistic regression, coefficients, confidence intervals, overfitting, etc.\nWhile the exact number of figures and tables will vary and depend on your analysis, you should target around 5 to 6. An overly long analysis could lead to losing points. If you want you can link back to your blog posts or create separate pages with more details.\nThe style of this paper should aim to be that of an academic paper. I don’t expect this to be of publication quality but you should keep that aim in mind. Avoid using “we” too frequently, for example “We also found that …”. Describe your methodology and your findings but don’t describe your whole process."
  },
  {
    "objectID": "analysis.html#note-on-attribution",
    "href": "analysis.html#note-on-attribution",
    "title": "Analysis",
    "section": "Note on Attribution",
    "text": "Note on Attribution\nIn general, you should try to provide links to relevant resources, especially those that helped you. You don’t have to link to every StackOverflow post you used but if there are explainers on aspects of the data or specific models that you found helpful, try to link to those. Also, try to link to other sources that might support (or refute) your analysis. These can just be regular hyperlinks. You don’t need a formal citation.\nIf you are directly quoting from a source, please make that clear. You can show quotes using &gt; like this\n&gt; To be or not to be.\n\nTo be or not to be."
  },
  {
    "objectID": "analysis.html#rubric-on-this-page",
    "href": "analysis.html#rubric-on-this-page",
    "title": "Analysis",
    "section": "Rubric: On this page",
    "text": "Rubric: On this page\nYou will\n\nIntroduce what motivates your Data Analysis (DA)\n\nWhich variables and relationships are you most interested in?\nWhat questions are you interested in answering?\nProvide context for the rest of the page. This will include figures/tables that illustrate aspects of the data of your question.\n\nModeling and Inference\n\nThe page will include some kind of formal statistical model. This could be a linear regression, logistic regression, or another modeling framework.\nExplain the ideas and techniques you used to choose the predictors for your model. (Think about including interaction terms and other transformations of your variables.)\nDescribe the results of your modelling and make sure to give a sense of the uncertainty in your estimates and conclusions.\n\nExplain the flaws and limitations of your analysis\n\nAre there some assumptions that you needed to make that might not hold? Is there other data that would help to answer your questions?\n\nClarity Figures\n\nAre your figures/tables/results easy to read, informative, without problems like overplotting, hard-to-read labels, etc?\nEach figure should provide a key insight. Too many figures or other data summaries can detract from this. (While not a hard limit, around 5 total figures is probably a good target.)\nDefault lm output and plots are typically not acceptable.\n\nClarity of Explanations\n\nHow well do you explain each figure/result?\nDo you provide interpretations that suggest further analysis or explanations for observed phenomenon?\n\nOrganization and cleanliness.\n\nMake sure to remove excessive warnings, hide most or all code, organize with sections or multiple pages, use bullets, etc.\nThis page should be self-contained, i.e. provide a description of the relevant data."
  },
  {
    "objectID": "big_picture.html",
    "href": "big_picture.html",
    "title": "Big Picture",
    "section": "",
    "text": "This comes from the file big_picture.Rmd.\nThink of this page as your 538/Upshot style article. This means that you should try to tell a story through the data and your analysis. Read articles from those sites and similar sites to get a feeling for what they are like. Try to write in the style of a news or popular article. Importantly, this page should be geared towards the general public. You shouldn’t assume the reader understands how to interpret a linear regression or a complicated plot. Focus on interpretation and visualizations."
  },
  {
    "objectID": "big_picture.html#rubric-on-this-page",
    "href": "big_picture.html#rubric-on-this-page",
    "title": "Big Picture",
    "section": "Rubric: On this page",
    "text": "Rubric: On this page\nYou will\n\nTitle\n\nYour big picture page should have a creative/click-bait-y title/headline that provides a hint about your thesis.\n\nClarity of Explanation\n\nYou should have a clear thesis/goal for this page. What are you trying to show? Make sure that you explain your analysis in detail but don’t go into top much mathematics or statistics. The audience for this page is the general public (to the extent possible). Your thesis should be a statement, not a question.\nEach figure should be very polished and also not too complicated. There should be a clear interpretation of the figure so the figure has a clear purpose. Even something like a histogram can be difficult to interpret for non-experts.\n\nCreativity\n\nDo your best to make things interesting. Think of a story. Think of how each part of your analysis supports the previous part or provides a different perspective.\n\nInteractive component\n\nQuality and ease of use of the interactive components. Is it clear what can be explored using your interactive components? Does it enhance and reinforce your conclusions?\n\nThis page should be self-contained.\n\nNote: This page should have no code visible, i.e. use #| echo: FALSE."
  },
  {
    "objectID": "big_picture.html#rubric-other-components",
    "href": "big_picture.html#rubric-other-components",
    "title": "Big Picture",
    "section": "Rubric: Other components",
    "text": "Rubric: Other components\n\nVideo Recording\nMake a video recording (probably using Zoom) demonstrating your interactive components. You should provide a quick explanation of your data and demonstrate some of the conclusions from your EDA. This video should be no longer than 4 minutes. Include a link to your video (and password if needed) in your README.md file on your Github repository. You are not required to provide a link on the website. This can be presented by any subset of the team members.\n\n\nRest of the Site\nFinally, here are important things to keep in mind for the rest of the site.\nThe main title of your page is informative. Each post has an author/description/informative title. All lab required posts are present. Each page (including the home page) has a nice featured image associated with it. Your about page is up to date and clean. You have removed the generic posts from the initial site template."
  }
]