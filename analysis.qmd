---
title: Analysis
description: Here we provide a detailed analysis using more sophisticated statistics techniques.
toc: true
draft: false
---

![](https://upload.wikimedia.org/wikipedia/commons/7/77/Pebbleswithquarzite.jpg)
Loading our Police Interaction dataset and all libraries.

```{r}
suppressPackageStartupMessages(library(tidymodels))
suppressPackageStartupMessages(library(reshape2)) #For models
suppressPackageStartupMessages(library(ggplot2)) #For plots
suppressPackageStartupMessages(library(shiny))
suppressPackageStartupMessages(library(shinylive)) #For interactive plots
suppressPackageStartupMessages(library(readr)) # For reading .rds files
suppressPackageStartupMessages(library(dplyr))
suppressPackageStartupMessages(library(tidyverse)) #For data wrangling, manipulation, etc.
suppressPackageStartupMessages(library(viridis)) #For visual themes
suppressPackageStartupMessages(library(sf)) 
suppressPackageStartupMessages(library(maps)) #For maps
suppressPackageStartupMessages(library(DT))

data <- read_rds(here::here("dataset/police_interaction.rds"))
```

One initial step in conducting EDA is by looking at the "PROPER" variable, which outlines whether or not the police behaved properly during the interaction, and seeing how the proportion of improper actions differs between individuals of different races. 

```{r}
data |>
  filter(!is.na(PROPER)) |>
  group_by(RACE) |>
  summarize(
    improper_proportion = sum(PROPER == 0) / n()
  ) |>
  ggplot(aes(x = RACE, y = improper_proportion, fill = RACE)) +
  geom_bar(stat = "identity") + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```
This second plot is a heatmap that visualizes the correlations between various continuous variables in the dataset, where the color intensity represents the strength and direction of the correlation (with red indicating positive correlations and blue indicating negative correlations). There are notable positive correlations such as HHPOV and WORK_LW and notable negative correlations such as WORK_LW and any high-income indicator.

``` {r}
# Categorizing variables based on their types
#removed "CONTACT" and "ERROR" due to "Warning in cor(numeric_data, use = "complete.obs") : the standard deviation is zero" error
binary_columns <- c("MALE", "WORK_LW", "HHPOV", "PUB_HOUSE",   "PUB_HOUSE_SUB", "REGION", "INPERSON", "VICAR_CITIZEN", "VICAR_PRO_AUTO", "VICAR_PRO_PERS", "VICAR_OTH_CONT", "VICAR_IMPROPER", "D_HH_P23", "PROPER")

ordinal_columns <- c("C4_RACE", "MAR_STAT", "FREQ_DRV", "TENURE",  "MSA_STATUS")

continuous_columns <- c("AGE", "EDUCATION", "EDUCATION_SUB", "NUM_MOVES", "NUM_CONT", "HH_SIZE", "PPCS_YEAR", "N_HH_P1", "N_PERS_P1", "NUM_CITIZEN_HH", "NUM_PRO_AUTO_HH", "NUM_PRO_PERS_HH", "NUM_OTH_CONT_HH", "NUM_IMPROPER_HH")

filtered <- data
# Selecting data by type
binary_data <- filtered %>%
  select(all_of(binary_columns)) %>%
  select(where(is.numeric))

ordinal_data <- filtered %>%
  select(all_of(ordinal_columns)) %>%
  select(where(is.numeric))

continuous_data <- filtered %>%
  select(all_of(continuous_columns)) %>%
  select(where(is.numeric))

# Calculating correlation matrices with appropriate methods
cor_binary <- cor(binary_data, use = "pairwise.complete.obs", method = "pearson")
cor_ordinal <- cor(ordinal_data, use = "pairwise.complete.obs", method = "spearman")
cor_continuous <- cor(continuous_data, use = "pairwise.complete.obs", method = "pearson")

# Creating a heatmap function
plot_heatmap <- function(cor_matrix, title) {
  cor_melted <- melt(cor_matrix)
  ggplot(cor_melted, aes(Var1, Var2, fill = value)) +
    geom_tile(color = "white") +
    scale_fill_gradient2(low = "blue", high = "red", mid = "white",
                         midpoint = 0, limit = c(-1, 1), name = "Correlation") +
    labs(title = title, x = "Variables", y = "Variables") +
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 90, hjust = 1),
          axis.text.y = element_text(angle = 0, vjust = 1))
}

# Plotting heatmaps for each correlation matrix
if (ncol(continuous_data) > 1) {
  print(plot_heatmap(cor_continuous, "Correlation Heatmap of Continuous Variables"))
}

```

For modeling, we used a linear model with an 80% split train and test set to try to predict whether someone would be arrested based on their race, years of education, if their household is living in poverty, and their gender. 
In the training set, the F-statistic is far over 1 and the p-values show over 95% confidence in all the response variables being significant.

```{r Training Summary:}
split <- initial_split(filtered, prop = .8) #good way to keep yourself honest. splits it by prop % being in training, 1-prop being test
training<- training(split)
testing <- testing(split)
predict_filtered <- training |>
  filter(!is.na(ARRESTED) & !is.na(RACE) & !is.na(EDUCATION) & !is.na(HHPOV) & !is.na(MALE))

mod1 <- lm(ARRESTED ~ RACE + EDUCATION + HHPOV + MALE, predict_filtered)
```

In the test set, the F-statistic is much lower and the p-values and t-stats rise for each variable, with the p-value for each race variable but the intercept being over 0.1. This finding implies that there isn't a significant relationship on average between these race categories and being arrested, although, for "Other or multiracial, Non-Hispanic", that could be skewed by the much smaller sample size of the category compared to the other races in the category.

```{r Testing Summary:}

predict_filtered_test <- testing |>
  filter(!is.na(ARRESTED) & !is.na(RACE) & !is.na(EDUCATION) & !is.na(HHPOV) & !is.na(MALE))

mod2 <- lm(ARRESTED ~ RACE + EDUCATION + HHPOV + MALE, predict_filtered_test)
plot(mod2)
```

We added another dataset that describes the number of homicides, the number of suicides, and the number of suicides done with firearms for whites & nonwhites within different states from 1949 to 2020.

Dataset:[Firearm Suicide Proxy for Household Gun Ownership](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/QVYDUD)

Loading our Firearm dataset.
```{r}
firearm_data <- read_rds("dataset/firearm_sh_ds.rds")
```

This map represents the rate of homicides by firearm by state.

```{r}
firearm_data <- firearm_data |>
  mutate(firearm_homicide_rate = as.numeric(firearm_homicide_rate),
         state = tolower(state)) |>
  filter(!is.na(firearm_homicide_rate))

us_states <- st_as_sf(maps::map("state", plot = FALSE, fill = TRUE)) %>%
  mutate(region = tolower(ID)) %>% # Convert state names to lowercase
  select(region, geom) # Retain only state name and geometry columns


map_data <- us_states |>
  rename(state = region) |>
  left_join(firearm_data, by = "state")

map_data$log_firearm_homicide_rate <- log(map_data$firearm_homicide_rate + 1)
 
ggplot(map_data) +
  geom_sf(aes(fill = log_firearm_homicide_rate), color = "black", size = 0.2) +
  scale_fill_viridis_c(option = "plasma", name = "Homicide Rate") +
  theme_minimal() +
  labs(
    title = "Firearm Homicide Rate by State",
    fill = "Rate"
  ) +
  facet_wrap(~year)

```
We combined the two datasets based on locations. The first dataset was divided into four regions: Northeast, Midwest, South, and West while the second dataset was divided into different states. 

We grouped data from both datasets by region and year using the regional definitions from the [Census Bureau Designated Regions](https://en.wikipedia.org/wiki/List_of_regions_of_the_United_States).
For each region and year the data was summarized into categories that include proportional data for certain types of incidents related to police contact, racial distributions, homicide rate, etc.

The combination of the data is done using the code in "/scripts/load_and_clean_data.R". Below is a table, which displays each new column and its description. Followed by the new combined data itself.

```{r}
variable_table <- tibble(
  Variable_Name = c("region", "year", "total_count", "WHITE_NH_PROP", "B_NH_PROP", "HISPANIC_PROP", "OTHER_MULTI_NH_PROP", "CONTACT_FREQ", "OC_FRISK_PROP", "TC_FRISK_PROP", "VSRCH_PROP", "ARREST_PROP", "CUFFED_PROP", "PROPER_PROP", "IMPROPER_PROP", "AVG_CONT", "population", "fa_homicides", "nfa_homicides", "homicides", "fa_homicide_rate", "nfa_homicide_rate", "homicide_rate"),
  Variable_Description = c("Region the data is from", "Year the data is from", "Total number of responses in the police sentiment dataset", "White non-hispanic proportion of responses for police sentiment", "Black non-hispanic proportion of responses for police sentiment", "Hispanic proportion of responses for police sentiment survey", "Other/multiracial/Non-hispanic proportion of responses for police sentiment survey", "Proportion of responders who had contact with law enforcement", "Proportion of stop and frisks for non-traffic stops", "Proportion of stop and frisks for traffic stops", "Proportion of vehicle searchs", "Proportions of contacts that ended in arrest", "Proportion of contacts that included cuffing", "Proportion of contacts deemed handled proper", "Proportion of contacts deemed unproperly handled", "Average number of contacts in the last year", "Total region population from US census", "Number of Firearm homicides", "Number of non-firearm homicides", "number of homidides", "firearm homicides per 100,000", "non-firearm homicides per 100,000", "homicides per 100,000")
)

print(variable_table, n = Inf)
```
This chart normalizes racial proportions by multiplying by 100 to plot alongside homicide rates on the same scale, then multiplies firearm homicide rate by 10 to get a better understanding of what the trend looks like.
```{r}
data <- read_rds(here::here("dataset", "combined_regional_data.rds"))
ggplot(data, aes(x = year)) +
  geom_line(aes(y = fa_homicide_rate * 10, color = "Firearm Homicide Rate * 10"), size = 1) +
  geom_line(aes(y = WHITE_NH_PROP * 100, color = "White NH Prop"), linetype = "dashed", size = 0.8) +
  geom_line(aes(y = B_NH_PROP * 100, color = "Black NH Prop"), linetype = "dashed", size = 0.8) +
  facet_wrap(~ region) +
  labs(
    title = "Trends in Firearm Homicide Rate and Racial Proportions Over Time",
    x = "Year",
    y = "Rate (%)",
    color = "Variable"
  ) +
  theme_minimal()

```


This interactive scatter plot shows the Firearm Homicide Rate relative to racial proportions of the 4 regions.

```{r}
# Define server logic ----
server <- function(input, output) {
  # Load the data
  # Replace this URL with the actual URL of your .rds file on GitHub Pages
  data <- read_rds(here::here("dataset", "combined_regional_data.rds"))
    # read_rds("https://sussmanbu.github.io/ma-4615-fa24-final-project-group-7/dataset/combined_regional_data.rds")
    # Have to configure to get this to link to the dataset
  
    # Render the interactive plot
  output$scatterPlot <- renderPlot({
    ggplot(data, aes(x = fa_homicide_rate, y = .data[[input$selected_race]])) +
      geom_point(aes(color = as.factor(year)), size = 2, alpha = 0.7) + # Use color to differentiate years
      geom_text(aes(label = year), vjust = -1, size = 3, alpha = 0.8) + # Add year annotations above points
      facet_wrap(~ region) + # Optional: Facet by region
      labs(
        title = paste("Firearm Homicide Rate vs", input$selected_race),
        x = "Firearm Homicide Rate",
        y = "Proportion",
        color = "Year"
      ) +
      theme_minimal() +
      theme(
        legend.position = "bottom",       # Place legend at the bottom
        legend.box = "horizontal",        # Align legend items horizontally
        legend.text = element_text(size = 10), # Adjust legend text size
        legend.title = element_text(size = 12), # Adjust legend title size
        legend.key.width = unit(1, "cm"), # Add space between legend items
        legend.spacing.x = unit(0.5, "cm") # Increase horizontal spacing
      ) +
      guides(
        color = guide_legend(nrow = 1, byrow = TRUE) # Force a single-row legend
      )
  })
}




# Define UI for the application ----
ui <- fluidPage(
  # Application title
  titlePanel("Interactive Scatter Plot: Firearm Homicide Rate and Racial Proportions"),
  
  # Sidebar layout with input and output
  sidebarLayout(
    # Sidebar panel for inputs
    sidebarPanel(
      # Dropdown menu for selecting racial group
      selectInput(
        inputId = "selected_race",
        label = "Select a Racial Group:",
        choices = c(
          "White NH" = "WHITE_NH_PROP",
          "Black NH" = "B_NH_PROP",
          "Hispanic" = "HISPANIC_PROP",
          "Other/Multi NH" = "OTHER_MULTI_NH_PROP"
        ),
        selected = "WHITE_NH_PROP"
      )
    ),
    
    # Main panel for displaying outputs
    mainPanel(
      plotOutput(outputId = "scatterPlot")
    )
  )
)

# Run the application ----
shinyApp(ui = ui, server = server)
```

```{r}
# Define the server logic
server <- function(input, output) {
  # Simulated example dataset (replace with actual dataset)
  data <- data.frame(
    year = rep(2000:2020, each = 4),
    region = rep(c("North", "South", "East", "West"), times = 21),
    fa_homicide_rate = runif(84, 0, 10),
    WHITE_NH_PROP = runif(84, 0, 1),
    B_NH_PROP = runif(84, 0, 1),
    HISPANIC_PROP = runif(84, 0, 1),
    OTHER_MULTI_NH_PROP = runif(84, 0, 1)
  )
  
  # Reactive filtered data
  filtered_data <- reactive({
    data %>%
      filter(year >= 2000 & year <= 2020) %>%
      filter(region %in% c("North", "South", "East", "West"))
  })
  
  # Render the summary table
  output$summaryTable <- DT::renderDataTable({
    filtered_data() %>%
      summarise(
        `Mean Homicide Rate` = mean(fa_homicide_rate, na.rm = TRUE),
        `Median Homicide Rate` = median(fa_homicide_rate, na.rm = TRUE),
        `Std Dev Homicide Rate` = sd(fa_homicide_rate, na.rm = TRUE),
        `Mean Proportion` = mean(.data[[input$selected_race]], na.rm = TRUE),
        `Median Proportion` = median(.data[[input$selected_race]], na.rm = TRUE),
        `Std Dev Proportion` = sd(.data[[input$selected_race]], na.rm = TRUE)
      )
  })
}

# Define the UI
ui <- fluidPage(
  titlePanel("Shiny App with Summary Table"),
  sidebarLayout(
    sidebarPanel(
      selectInput(
        inputId = "selected_race",
        label = "Select a Racial Group:",
        choices = c(
          "White NH" = "WHITE_NH_PROP",
          "Black NH" = "B_NH_PROP",
          "Hispanic" = "HISPANIC_PROP",
          "Other/Multi NH" = "OTHER_MULTI_NH_PROP"
        ),
        selected = "WHITE_NH_PROP"
      )
    ),
    mainPanel(
      tabsetPanel(
        tabPanel("Summary Table", DT::dataTableOutput(outputId = "summaryTable"))
      )
    )
  )
)

# Run the application
shinyApp(ui = ui, server = server)
```

<!-- This comes from the file `analysis.qmd`. -->

<!-- We describe here our detailed data analysis. This page will provide an overview of what questions you addressed, illustrations of relevant aspects of the data with tables and figures, and a statistical model that attempts to answer part of the question. You'll also reflect on next steps and further analysis. -->

<!-- The audience for this page is someone like your class mates, so you can expect that they have some level of statistical and quantitative sophistication and understand ideas like linear and logistic regression, coefficients, confidence intervals, overfitting, etc.  -->

<!-- While the exact number of figures and tables will vary and depend on your analysis, you should target around 5 to 6. An overly long analysis could lead to losing points. -->
<!-- If you want you can link back to your blog posts or create separate pages with more details. -->

<!-- The style of this paper should aim to be that of an academic paper.  -->
<!-- I don't expect this to be of publication quality but you should keep that aim in mind. -->
<!-- Avoid using "we" too frequently, for example "We also found that ...". Describe your methodology and your findings but don't describe your whole process. -->

<!-- ### Example of loading data -->

<!-- The code below shows an example of loading the loan refusal data set (which you should delete at some point). -->

<!-- ## Note on Attribution -->

<!-- In general, you should try to provide links to relevant resources, especially those that helped you. You don't have to link to every StackOverflow post you used but if there are explainers on aspects of the data or specific models that you found helpful, try to link to those. Also, try to link to other sources that might support (or refute) your analysis. These can just be regular hyperlinks. You don't need a formal citation. -->

<!-- If you are directly quoting from a source, please make that clear. You can show quotes using `>` like this -->

<!-- ```          -->
<!-- > To be or not to be. -->
<!-- ``` -->

<!-- > To be or not to be. -->

<!-- ------------------------------------------------------------------------ -->

<!-- ## Rubric: On this page -->

<!-- You will -->

<!-- -   Introduce what motivates your Data Analysis (DA) -->
<!--     -   Which variables and relationships are you most interested in? -->
<!--     -   What questions are you interested in answering? -->
<!--     -   Provide context for the rest of the page. This will include figures/tables that illustrate aspects of the data of your question. -->
<!-- -   Modeling and Inference -->
<!--     -   The page will include some kind of formal statistical model. This could be a linear regression, logistic regression, or another modeling framework. -->
<!--     -   Explain the ideas and techniques you used to choose the predictors for your model. (Think about including interaction terms and other transformations of your variables.) -->
<!--     -   Describe the results of your modelling and make sure to give a sense of the uncertainty in your estimates and conclusions. -->
<!-- -   Explain the flaws and limitations of your analysis -->
<!--     -   Are there some assumptions that you needed to make that might not hold? Is there other data that would help to answer your questions? -->
<!-- -   Clarity Figures -->
<!--     -   Are your figures/tables/results easy to read, informative, without problems like overplotting, hard-to-read labels, etc? -->
<!--     -   Each figure should provide a key insight. Too many figures or other data summaries can detract from this. (While not a hard limit, around 5 total figures is probably a good target.) -->
<!--     -   Default `lm` output and plots are typically not acceptable. -->
<!-- -   Clarity of Explanations -->
<!--     -   How well do you explain each figure/result? -->
<!--     -   Do you provide interpretations that suggest further analysis or explanations for observed phenomenon? -->
<!-- -   Organization and cleanliness. -->
<!--     -   Make sure to remove excessive warnings, hide most or all code, organize with sections or multiple pages, use bullets, etc. -->
<!--     -   This page should be self-contained, i.e. provide a description of the relevant data. -->
