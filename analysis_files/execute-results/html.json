{
  "hash": "4a25533d0fdd87d60470b16dbbac3faf",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Analysis\ndescription: Here we provide a detailed analysis using more sophisticated statistics techniques.\ntoc: true\ndraft: false\n\n---\n\n\n![](https://upload.wikimedia.org/wikipedia/commons/7/77/Pebbleswithquarzite.jpg)\nLoading our Police Interaction dataset and all libraries.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsuppressPackageStartupMessages(library(tidymodels))\nsuppressPackageStartupMessages(library(reshape2)) #For models\nsuppressPackageStartupMessages(library(ggplot2)) #For plots\nsuppressPackageStartupMessages(library(shiny))\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: package 'shiny' was built under R version 4.4.2\n```\n\n\n:::\n\n```{.r .cell-code}\nsuppressPackageStartupMessages(library(shinylive)) #For interactive plots\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: package 'shinylive' was built under R version 4.4.2\n```\n\n\n:::\n\n```{.r .cell-code}\nsuppressPackageStartupMessages(library(readr)) # For reading .rds files\nsuppressPackageStartupMessages(library(dplyr))\nsuppressPackageStartupMessages(library(tidyverse)) #For data wrangling, manipulation, etc.\nsuppressPackageStartupMessages(library(viridis)) #For visual themes\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: package 'viridis' was built under R version 4.4.2\n```\n\n\n:::\n\n```{.r .cell-code}\nsuppressPackageStartupMessages(library(sf)) \nsuppressPackageStartupMessages(library(maps)) #For maps\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: package 'maps' was built under R version 4.4.2\n```\n\n\n:::\n\n```{.r .cell-code}\nsuppressPackageStartupMessages(library(DT))\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: package 'DT' was built under R version 4.4.2\n```\n\n\n:::\n\n```{.r .cell-code}\ndata <- read_rds(here::here(\"dataset/police_interaction.rds\"))\n```\n:::\n\n\nOne initial step in conducting EDA is by looking at the \"PROPER\" variable, which outlines whether or not the police behaved properly during the interaction, and seeing how the proportion of improper actions differs between individuals of different races. \n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata |>\n  filter(!is.na(PROPER)) |>\n  group_by(RACE) |>\n  summarize(\n    improper_proportion = sum(PROPER == 0) / n()\n  ) |>\n  ggplot(aes(x = RACE, y = improper_proportion, fill = RACE)) +\n  geom_bar(stat = \"identity\") + \n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n```\n\n::: {.cell-output-display}\n![](analysis_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\nThis second plot is a heatmap that visualizes the correlations between various continuous variables in the dataset, where the color intensity represents the strength and direction of the correlation (with red indicating positive correlations and blue indicating negative correlations). There are notable positive correlations such as HHPOV and WORK_LW and notable negative correlations such as WORK_LW and any high-income indicator.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Categorizing variables based on their types\n#removed \"CONTACT\" and \"ERROR\" due to \"Warning in cor(numeric_data, use = \"complete.obs\") : the standard deviation is zero\" error\nbinary_columns <- c(\"MALE\", \"WORK_LW\", \"HHPOV\", \"PUB_HOUSE\",   \"PUB_HOUSE_SUB\", \"REGION\", \"INPERSON\", \"VICAR_CITIZEN\", \"VICAR_PRO_AUTO\", \"VICAR_PRO_PERS\", \"VICAR_OTH_CONT\", \"VICAR_IMPROPER\", \"D_HH_P23\", \"PROPER\")\n\nordinal_columns <- c(\"C4_RACE\", \"MAR_STAT\", \"FREQ_DRV\", \"TENURE\",  \"MSA_STATUS\")\n\ncontinuous_columns <- c(\"AGE\", \"EDUCATION\", \"EDUCATION_SUB\", \"NUM_MOVES\", \"NUM_CONT\", \"HH_SIZE\", \"PPCS_YEAR\", \"N_HH_P1\", \"N_PERS_P1\", \"NUM_CITIZEN_HH\", \"NUM_PRO_AUTO_HH\", \"NUM_PRO_PERS_HH\", \"NUM_OTH_CONT_HH\", \"NUM_IMPROPER_HH\")\n\nfiltered <- data\n# Selecting data by type\nbinary_data <- filtered %>%\n  select(all_of(binary_columns)) %>%\n  select(where(is.numeric))\n\nordinal_data <- filtered %>%\n  select(all_of(ordinal_columns)) %>%\n  select(where(is.numeric))\n\ncontinuous_data <- filtered %>%\n  select(all_of(continuous_columns)) %>%\n  select(where(is.numeric))\n\n# Calculating correlation matrices with appropriate methods\ncor_binary <- cor(binary_data, use = \"pairwise.complete.obs\", method = \"pearson\")\ncor_ordinal <- cor(ordinal_data, use = \"pairwise.complete.obs\", method = \"spearman\")\ncor_continuous <- cor(continuous_data, use = \"pairwise.complete.obs\", method = \"pearson\")\n\n# Creating a heatmap function\nplot_heatmap <- function(cor_matrix, title) {\n  cor_melted <- melt(cor_matrix)\n  ggplot(cor_melted, aes(Var1, Var2, fill = value)) +\n    geom_tile(color = \"white\") +\n    scale_fill_gradient2(low = \"blue\", high = \"red\", mid = \"white\",\n                         midpoint = 0, limit = c(-1, 1), name = \"Correlation\") +\n    labs(title = title, x = \"Variables\", y = \"Variables\") +\n    theme_minimal() +\n    theme(axis.text.x = element_text(angle = 90, hjust = 1),\n          axis.text.y = element_text(angle = 0, vjust = 1))\n}\n\n# Plotting heatmaps for each correlation matrix\nif (ncol(continuous_data) > 1) {\n  print(plot_heatmap(cor_continuous, \"Correlation Heatmap of Continuous Variables\"))\n}\n```\n\n::: {.cell-output-display}\n![](analysis_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\nFor modeling, we used a linear model with an 80% split train and test set to try to predict whether someone would be arrested based on their race, years of education, if their household is living in poverty, and their gender. \nIn the training set, the F-statistic is far over 1 and the p-values show over 95% confidence in all the response variables being significant.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsplit <- initial_split(filtered, prop = .8) #good way to keep yourself honest. splits it by prop % being in training, 1-prop being test\ntraining<- training(split)\ntesting <- testing(split)\npredict_filtered <- training |>\n  filter(!is.na(ARRESTED) & !is.na(RACE) & !is.na(EDUCATION) & !is.na(HHPOV) & !is.na(MALE))\n\nmod1 <- lm(ARRESTED ~ RACE + EDUCATION + HHPOV + MALE, predict_filtered)\n```\n:::\n\n\nIn the test set, the F-statistic is much lower and the p-values and t-stats rise for each variable, with the p-value for each race variable but the intercept being over 0.1. This finding implies that there isn't a significant relationship on average between these race categories and being arrested, although, for \"Other or multiracial, Non-Hispanic\", that could be skewed by the much smaller sample size of the category compared to the other races in the category.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredict_filtered_test <- testing |>\n  filter(!is.na(ARRESTED) & !is.na(RACE) & !is.na(EDUCATION) & !is.na(HHPOV) & !is.na(MALE))\n\nmod2 <- lm(ARRESTED ~ RACE + EDUCATION + HHPOV + MALE, predict_filtered_test)\nplot(mod2)\n```\n\n::: {.cell-output-display}\n![](analysis_files/figure-html/Testing Summary--1.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](analysis_files/figure-html/Testing Summary--2.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](analysis_files/figure-html/Testing Summary--3.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](analysis_files/figure-html/Testing Summary--4.png){width=672}\n:::\n:::\n\n\nWe added another dataset that describes the number of homicides, the number of suicides, and the number of suicides done with firearms for whites & nonwhites within different states from 1949 to 2020.\n\nDataset:[Firearm Suicide Proxy for Household Gun Ownership](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/QVYDUD)\n\nLoading our Firearm dataset.\n\n::: {.cell}\n\n```{.r .cell-code}\nfirearm_data <- read_rds(\"dataset/firearm_sh_ds.rds\")\n```\n:::\n\n\nThis map represents the rate of homicides by firearm by state.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfirearm_data <- firearm_data |>\n  mutate(firearm_homicide_rate = as.numeric(firearm_homicide_rate),\n         state = tolower(state)) |>\n  filter(!is.na(firearm_homicide_rate))\n\nus_states <- map_data(\"state\") |>\n  as_tibble() |>\n  st_as_sf(coords = c(\"long\", \"lat\"), crs = 4326) |>\n  group_by(region) |>\n  summarise(geometry = st_union(geometry)) |>\n  st_transform(crs = 5070)\n\nus_states <- us_states |>\n  st_cast(\"POLYGON\")\n\nmap_data <- us_states |>\n  rename(state = region) |>\n  left_join(firearm_data, by = \"state\")\n\nmap_data$log_firearm_homicide_rate <- log(map_data$firearm_homicide_rate + 1)\n\nggplot(map_data) +\n  geom_sf(aes(fill = log_firearm_homicide_rate), color = \"black\", size = 0.2) +\n  scale_fill_viridis_c(option = \"plasma\", name = \"Homicide Rate\") +\n  theme_minimal() +\n  labs(\n    title = \"Firearm Homicide Rate by State\",\n    fill = \"Rate\"\n  ) +\n  facet_wrap(~year)\n```\n\n::: {.cell-output-display}\n![](analysis_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\nWe combined the two datasets based on locations. The first dataset was divided into four regions: Northeast, Midwest, South, and West while the second dataset was divided into different states. \n\nWe grouped data from both datasets by region and year using the regional definitions from the [Census Bureau Designated Regions](https://en.wikipedia.org/wiki/List_of_regions_of_the_United_States).\nFor each region and year the data was summarized into categories that include proportional data for certain types of incidents related to police contact, racial distributions, homicide rate, etc.\n\nThe combination of the data is done using the code in \"/scripts/load_and_clean_data.R\". Below is a table, which displays each new column and its description. Followed by the new combined data itself.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvariable_table <- tibble(\n  Variable_Name = c(\"region\", \"year\", \"total_count\", \"WHITE_NH_PROP\", \"B_NH_PROP\", \"HISPANIC_PROP\", \"OTHER_MULTI_NH_PROP\", \"CONTACT_FREQ\", \"OC_FRISK_PROP\", \"TC_FRISK_PROP\", \"VSRCH_PROP\", \"ARREST_PROP\", \"CUFFED_PROP\", \"PROPER_PROP\", \"IMPROPER_PROP\", \"AVG_CONT\", \"population\", \"fa_homicides\", \"nfa_homicides\", \"homicides\", \"fa_homicide_rate\", \"nfa_homicide_rate\", \"homicide_rate\"),\n  Variable_Description = c(\"Region the data is from\", \"Year the data is from\", \"Total number of responses in the police sentiment dataset\", \"White non-hispanic proportion of responses for police sentiment\", \"Black non-hispanic proportion of responses for police sentiment\", \"Hispanic proportion of responses for police sentiment survey\", \"Other/multiracial/Non-hispanic proportion of responses for police sentiment survey\", \"Proportion of responders who had contact with law enforcement\", \"Proportion of stop and frisks for non-traffic stops\", \"Proportion of stop and frisks for traffic stops\", \"Proportion of vehicle searchs\", \"Proportions of contacts that ended in arrest\", \"Proportion of contacts that included cuffing\", \"Proportion of contacts deemed handled proper\", \"Proportion of contacts deemed unproperly handled\", \"Average number of contacts in the last year\", \"Total region population from US census\", \"Number of Firearm homicides\", \"Number of non-firearm homicides\", \"number of homidides\", \"firearm homicides per 100,000\", \"non-firearm homicides per 100,000\", \"homicides per 100,000\")\n)\n\nprint(variable_table, n = Inf)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 23 × 2\n   Variable_Name       Variable_Description                                     \n   <chr>               <chr>                                                    \n 1 region              Region the data is from                                  \n 2 year                Year the data is from                                    \n 3 total_count         Total number of responses in the police sentiment dataset\n 4 WHITE_NH_PROP       White non-hispanic proportion of responses for police se…\n 5 B_NH_PROP           Black non-hispanic proportion of responses for police se…\n 6 HISPANIC_PROP       Hispanic proportion of responses for police sentiment su…\n 7 OTHER_MULTI_NH_PROP Other/multiracial/Non-hispanic proportion of responses f…\n 8 CONTACT_FREQ        Proportion of responders who had contact with law enforc…\n 9 OC_FRISK_PROP       Proportion of stop and frisks for non-traffic stops      \n10 TC_FRISK_PROP       Proportion of stop and frisks for traffic stops          \n11 VSRCH_PROP          Proportion of vehicle searchs                            \n12 ARREST_PROP         Proportions of contacts that ended in arrest             \n13 CUFFED_PROP         Proportion of contacts that included cuffing             \n14 PROPER_PROP         Proportion of contacts deemed handled proper             \n15 IMPROPER_PROP       Proportion of contacts deemed unproperly handled         \n16 AVG_CONT            Average number of contacts in the last year              \n17 population          Total region population from US census                   \n18 fa_homicides        Number of Firearm homicides                              \n19 nfa_homicides       Number of non-firearm homicides                          \n20 homicides           number of homidides                                      \n21 fa_homicide_rate    firearm homicides per 100,000                            \n22 nfa_homicide_rate   non-firearm homicides per 100,000                        \n23 homicide_rate       homicides per 100,000                                    \n```\n\n\n:::\n:::\n\nThis chart normalizes racial proportions by multiplying by 100 to plot alongside homicide rates on the same scale, then multiplies firearm homicide rate by 10 to get a better understanding of what the trend looks like.\n\n::: {.cell}\n\n```{.r .cell-code}\ndata <- read_rds(here::here(\"dataset\", \"combined_regional_data.rds\"))\nggplot(data, aes(x = year)) +\n  geom_line(aes(y = fa_homicide_rate * 10, color = \"Firearm Homicide Rate * 10\"), size = 1) +\n  geom_line(aes(y = WHITE_NH_PROP * 100, color = \"White NH Prop\"), linetype = \"dashed\", size = 0.8) +\n  geom_line(aes(y = B_NH_PROP * 100, color = \"Black NH Prop\"), linetype = \"dashed\", size = 0.8) +\n  facet_wrap(~ region) +\n  labs(\n    title = \"Trends in Firearm Homicide Rate and Racial Proportions Over Time\",\n    x = \"Year\",\n    y = \"Rate (%)\",\n    color = \"Variable\"\n  ) +\n  theme_minimal()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](analysis_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\n\nThis interactive scatter plot shows the Firearm Homicide Rate relative to racial proportions of the 4 regions.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Define server logic ----\nserver <- function(input, output) {\n  # Load the data\n  # Replace this URL with the actual URL of your .rds file on GitHub Pages\n  data <- read_rds(here::here(\"dataset\", \"combined_regional_data.rds\"))\n    # read_rds(\"https://sussmanbu.github.io/ma-4615-fa24-final-project-group-7/dataset/combined_regional_data.rds\")\n    # Have to configure to get this to link to the dataset\n  \n    # Render the interactive plot\n  output$scatterPlot <- renderPlot({\n    ggplot(data, aes(x = fa_homicide_rate, y = .data[[input$selected_race]])) +\n      geom_point(aes(color = as.factor(year)), size = 2, alpha = 0.7) + # Use color to differentiate years\n      geom_text(aes(label = year), vjust = -1, size = 3, alpha = 0.8) + # Add year annotations above points\n      facet_wrap(~ region) + # Optional: Facet by region\n      labs(\n        title = paste(\"Firearm Homicide Rate vs\", input$selected_race),\n        x = \"Firearm Homicide Rate\",\n        y = \"Proportion\",\n        color = \"Year\"\n      ) +\n      theme_minimal() +\n      theme(\n        legend.position = \"bottom\",       # Place legend at the bottom\n        legend.box = \"horizontal\",        # Align legend items horizontally\n        legend.text = element_text(size = 10), # Adjust legend text size\n        legend.title = element_text(size = 12), # Adjust legend title size\n        legend.key.width = unit(1, \"cm\"), # Add space between legend items\n        legend.spacing.x = unit(0.5, \"cm\") # Increase horizontal spacing\n      ) +\n      guides(\n        color = guide_legend(nrow = 1, byrow = TRUE) # Force a single-row legend\n      )\n  })\n}\n\n\n\n\n# Define UI for the application ----\nui <- fluidPage(\n  # Application title\n  titlePanel(\"Interactive Scatter Plot: Firearm Homicide Rate and Racial Proportions\"),\n  \n  # Sidebar layout with input and output\n  sidebarLayout(\n    # Sidebar panel for inputs\n    sidebarPanel(\n      # Dropdown menu for selecting racial group\n      selectInput(\n        inputId = \"selected_race\",\n        label = \"Select a Racial Group:\",\n        choices = c(\n          \"White NH\" = \"WHITE_NH_PROP\",\n          \"Black NH\" = \"B_NH_PROP\",\n          \"Hispanic\" = \"HISPANIC_PROP\",\n          \"Other/Multi NH\" = \"OTHER_MULTI_NH_PROP\"\n        ),\n        selected = \"WHITE_NH_PROP\"\n      )\n    ),\n    \n    # Main panel for displaying outputs\n    mainPanel(\n      plotOutput(outputId = \"scatterPlot\")\n    )\n  )\n)\n\n# Run the application ----\nshinyApp(ui = ui, server = server)\n```\n\n::: {.cell-output-display}\n`<div style=\"width: 100% ; height: 400px ; text-align: center; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box;\" class=\"muted well\">Shiny applications not supported in static R Markdown documents</div>`{=html}\n:::\n:::\n\n\n```{shinylive-r}\n# Define the server logic\n#| eval: false\n#| standalone: false\nserver <- function(input, output) {\n  # Simulated example dataset (replace with actual dataset)\n  data <- data.frame(\n    year = rep(2000:2020, each = 4),\n    region = rep(c(\"North\", \"South\", \"East\", \"West\"), times = 21),\n    fa_homicide_rate = runif(84, 0, 10),\n    WHITE_NH_PROP = runif(84, 0, 1),\n    B_NH_PROP = runif(84, 0, 1),\n    HISPANIC_PROP = runif(84, 0, 1),\n    OTHER_MULTI_NH_PROP = runif(84, 0, 1)\n  )\n  \n  # Reactive filtered data\n  filtered_data <- reactive({\n    data %>%\n      filter(year >= 2000 & year <= 2020) %>%\n      filter(region %in% c(\"North\", \"South\", \"East\", \"West\"))\n  })\n  \n  # Render the summary table\n  output$summaryTable <- DT::renderDataTable({\n    filtered_data() %>%\n      summarise(\n        `Mean Homicide Rate` = mean(fa_homicide_rate, na.rm = TRUE),\n        `Median Homicide Rate` = median(fa_homicide_rate, na.rm = TRUE),\n        `Std Dev Homicide Rate` = sd(fa_homicide_rate, na.rm = TRUE),\n        `Mean Proportion` = mean(.data[[input$selected_race]], na.rm = TRUE),\n        `Median Proportion` = median(.data[[input$selected_race]], na.rm = TRUE),\n        `Std Dev Proportion` = sd(.data[[input$selected_race]], na.rm = TRUE)\n      )\n  })\n}\n\n# Define the UI\nui <- fluidPage(\n  titlePanel(\"Shiny App with Summary Table\"),\n  sidebarLayout(\n    sidebarPanel(\n      selectInput(\n        inputId = \"selected_race\",\n        label = \"Select a Racial Group:\",\n        choices = c(\n          \"White NH\" = \"WHITE_NH_PROP\",\n          \"Black NH\" = \"B_NH_PROP\",\n          \"Hispanic\" = \"HISPANIC_PROP\",\n          \"Other/Multi NH\" = \"OTHER_MULTI_NH_PROP\"\n        ),\n        selected = \"WHITE_NH_PROP\"\n      )\n    ),\n    mainPanel(\n      tabsetPanel(\n        tabPanel(\"Summary Table\", DT::dataTableOutput(outputId = \"summaryTable\"))\n      )\n    )\n  )\n)\n\n# Run the application\nshinyApp(ui = ui, server = server)\n```\n\n<!-- This comes from the file `analysis.qmd`. -->\n\n<!-- We describe here our detailed data analysis. This page will provide an overview of what questions you addressed, illustrations of relevant aspects of the data with tables and figures, and a statistical model that attempts to answer part of the question. You'll also reflect on next steps and further analysis. -->\n\n<!-- The audience for this page is someone like your class mates, so you can expect that they have some level of statistical and quantitative sophistication and understand ideas like linear and logistic regression, coefficients, confidence intervals, overfitting, etc.  -->\n\n<!-- While the exact number of figures and tables will vary and depend on your analysis, you should target around 5 to 6. An overly long analysis could lead to losing points. -->\n<!-- If you want you can link back to your blog posts or create separate pages with more details. -->\n\n<!-- The style of this paper should aim to be that of an academic paper.  -->\n<!-- I don't expect this to be of publication quality but you should keep that aim in mind. -->\n<!-- Avoid using \"we\" too frequently, for example \"We also found that ...\". Describe your methodology and your findings but don't describe your whole process. -->\n\n<!-- ### Example of loading data -->\n\n<!-- The code below shows an example of loading the loan refusal data set (which you should delete at some point). -->\n\n<!-- ## Note on Attribution -->\n\n<!-- In general, you should try to provide links to relevant resources, especially those that helped you. You don't have to link to every StackOverflow post you used but if there are explainers on aspects of the data or specific models that you found helpful, try to link to those. Also, try to link to other sources that might support (or refute) your analysis. These can just be regular hyperlinks. You don't need a formal citation. -->\n\n<!-- If you are directly quoting from a source, please make that clear. You can show quotes using `>` like this -->\n\n<!-- ```          -->\n<!-- > To be or not to be. -->\n<!-- ``` -->\n\n<!-- > To be or not to be. -->\n\n<!-- ------------------------------------------------------------------------ -->\n\n<!-- ## Rubric: On this page -->\n\n<!-- You will -->\n\n<!-- -   Introduce what motivates your Data Analysis (DA) -->\n<!--     -   Which variables and relationships are you most interested in? -->\n<!--     -   What questions are you interested in answering? -->\n<!--     -   Provide context for the rest of the page. This will include figures/tables that illustrate aspects of the data of your question. -->\n<!-- -   Modeling and Inference -->\n<!--     -   The page will include some kind of formal statistical model. This could be a linear regression, logistic regression, or another modeling framework. -->\n<!--     -   Explain the ideas and techniques you used to choose the predictors for your model. (Think about including interaction terms and other transformations of your variables.) -->\n<!--     -   Describe the results of your modelling and make sure to give a sense of the uncertainty in your estimates and conclusions. -->\n<!-- -   Explain the flaws and limitations of your analysis -->\n<!--     -   Are there some assumptions that you needed to make that might not hold? Is there other data that would help to answer your questions? -->\n<!-- -   Clarity Figures -->\n<!--     -   Are your figures/tables/results easy to read, informative, without problems like overplotting, hard-to-read labels, etc? -->\n<!--     -   Each figure should provide a key insight. Too many figures or other data summaries can detract from this. (While not a hard limit, around 5 total figures is probably a good target.) -->\n<!--     -   Default `lm` output and plots are typically not acceptable. -->\n<!-- -   Clarity of Explanations -->\n<!--     -   How well do you explain each figure/result? -->\n<!--     -   Do you provide interpretations that suggest further analysis or explanations for observed phenomenon? -->\n<!-- -   Organization and cleanliness. -->\n<!--     -   Make sure to remove excessive warnings, hide most or all code, organize with sections or multiple pages, use bullets, etc. -->\n<!--     -   This page should be self-contained, i.e. provide a description of the relevant data. -->\n",
    "supporting": [
      "analysis_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}